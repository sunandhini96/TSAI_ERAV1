{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY2-2EvbSgtn",
        "outputId": "53dbdf90-d807-455f-8fa1-c50e2e8487d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/capstone-MultimodalGPT/Part1')"
      ],
      "metadata": {
        "id": "5TKTCDOTSmqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Requirements"
      ],
      "metadata": {
        "id": "Bv4uhwG0rFMq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVDB1ZA6SXU5",
        "outputId": "8616a7dd-f771-429a-8160-342a2c0cde82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.7/208.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.6/630.6 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.1/778.1 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for whisperx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for faster-whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execute training code for projection layer"
      ],
      "metadata": {
        "id": "IbmpRwGvrMXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IFN4x94Sf4m",
        "outputId": "df9c2cbc-8d36-454d-d1f1-5f85f1809a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-02 12:32:08.945551: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-02 12:32:08.945603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-02 12:32:08.947220: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-02 12:32:09.989507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmadhurprakashgarg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/capstone-MultimodalGPT/Part1/wandb/run-20240202_123213-h6xerces\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstep1_projector_pretrain\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/madhurprakashgarg/tsai_multimodal_gpt_project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/madhurprakashgarg/tsai_multimodal_gpt_project/runs/h6xerces\u001b[0m\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.34it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "Train size 532577 and validation size 59176\n",
            "Train size 532577 and validation size 59176\n",
            "Training started.\n",
            "Step 0/50000: Avg Running Loss = 7.004720211029053\n",
            "Saving Checkpoint for step :  100\n",
            "Saving Checkpoint for step :  200\n",
            "Saving Checkpoint for step :  300\n",
            "Saving Checkpoint for step :  400\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  500\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  600\n",
            "Saving Checkpoint for step :  700\n",
            "Saving Checkpoint for step :  800\n",
            "Saving Checkpoint for step :  900\n",
            "Saving Checkpoint for step :  1000\n",
            "0 - Target captions:\n",
            " An open white refrigerator sits on a patch of grass between the street and the sidewalk.  \n",
            "0 - predicted_captions:\n",
            " A man is standing on a sidewalk with a dog. a dog. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A group of girls sitting at a long table eating.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man eating a sandwich on a table. a table a table a man eating a sandwich on<|endoftext|> \n",
            "Step 1000/50000: Avg Running Loss = 5.5486464440822605\n",
            "Saving Checkpoint for step :  1100\n",
            "Saving Checkpoint for step :  1200\n",
            "Saving Checkpoint for step :  1300\n",
            "Saving Checkpoint for step :  1400\n",
            "Saving Checkpoint for step :  1500\n",
            "Saving Checkpoint for step :  1600\n",
            "Saving Checkpoint for step :  1700\n",
            "Saving Checkpoint for step :  1800\n",
            "Saving Checkpoint for step :  1900\n",
            "Saving Checkpoint for step :  2000\n",
            "0 - Target captions:\n",
            " A wall hanging of a portrait on the wall with a chair in the room  \n",
            "0 - predicted_captions:\n",
            " A man is sitting on a a couch a with a a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A white building with a watch tower atop of it.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man is walking down the street with a dog. a. a. a. a.<|endoftext|> \n",
            "Step 2000/50000: Avg Running Loss = 4.962592858076095\n",
            "Saving Checkpoint for step :  2100\n",
            "Saving Checkpoint for step :  2200\n",
            "Saving Checkpoint for step :  2300\n",
            "Saving Checkpoint for step :  2400\n",
            "Saving Checkpoint for step :  2500\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  2600\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  2700\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  2800\n",
            "Saving Checkpoint for step :  2900\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  3000\n",
            "0 - Target captions:\n",
            " A man with a hat holding a cellphone.  \n",
            "0 - predicted_captions:\n",
            " A man is holding a phone and a. a. a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A large clock with letters instead of numbers.  \n",
            "1 - predicted_captions:\n",
            " A large red brick building with a large red and white a. a. a. a.<|endoftext|> \n",
            "Step 3000/50000: Avg Running Loss = 4.801064279079437\n",
            "Saving Checkpoint for step :  3100\n",
            "Saving Checkpoint for step :  3200\n",
            "Saving Checkpoint for step :  3300\n",
            "Saving Checkpoint for step :  3400\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  3500\n",
            "Saving Checkpoint for step :  3600\n",
            "Saving Checkpoint for step :  3700\n",
            "Saving Checkpoint for step :  3800\n",
            "Saving Checkpoint for step :  3900\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  4000\n",
            "0 - Target captions:\n",
            " A laptop on a table in a room.<|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A computer on a desk with a computer on it a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A man bending over an oven with two pumpkin pies.  \n",
            "1 - predicted_captions:\n",
            " A man is holding a frying pan with a frying pan on it. a. on a stove<|endoftext|> \n",
            "Step 4000/50000: Avg Running Loss = 4.705120328903198\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  4100\n",
            "Saving Checkpoint for step :  4200\n",
            "Saving Checkpoint for step :  4300\n",
            "Saving Checkpoint for step :  4400\n",
            "Saving Checkpoint for step :  4500\n",
            "Saving Checkpoint for step :  4600\n",
            "Saving Checkpoint for step :  4700\n",
            "Saving Checkpoint for step :  4800\n",
            "Saving Checkpoint for step :  4900\n",
            "Saving Checkpoint for step :  5000\n",
            "0 - Target captions:\n",
            " An oven mitt and scissors on a white surface.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A red and white plastic cup with a a a a a a a a a a a a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A group of men are having drinks and dinner at a resturant.  \n",
            "1 - predicted_captions:\n",
            " A man is sitting at a table with a a. a. a. a. a.<|endoftext|> \n",
            "Step 5000/50000: Avg Running Loss = 4.643053737163544\n",
            "Saving Checkpoint for step :  5100\n",
            "Saving Checkpoint for step :  5200\n",
            "Saving Checkpoint for step :  5300\n",
            "Saving Checkpoint for step :  5400\n",
            "Saving Checkpoint for step :  5500\n",
            "Saving Checkpoint for step :  5600\n",
            "Saving Checkpoint for step :  5700\n",
            "Saving Checkpoint for step :  5800\n",
            "Saving Checkpoint for step :  5900\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  6000\n",
            "0 - Target captions:\n",
            " black and white picture of old men one with a bat waiting for a pitch\n",
            "  \n",
            "0 - predicted_captions:\n",
            " A man wearing a white shirt and a black hat a........<|endoftext|> \n",
            "1 - Target captions:\n",
            " A desk with just a laptop lamp and picture on it.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A computer on a desk with a laptop and a a. a. a. a. a<|endoftext|> \n",
            "Step 6000/50000: Avg Running Loss = 4.600190189599991\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  6100\n",
            "Saving Checkpoint for step :  6200\n",
            "Saving Checkpoint for step :  6300\n",
            "Saving Checkpoint for step :  6400\n",
            "Saving Checkpoint for step :  6500\n",
            "Saving Checkpoint for step :  6600\n",
            "Saving Checkpoint for step :  6700\n",
            "Saving Checkpoint for step :  6800\n",
            "Saving Checkpoint for step :  6900\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  7000\n",
            "0 - Target captions:\n",
            " A child plays in the background as another child plays with a teddy bear  \n",
            "0 - predicted_captions:\n",
            " A little girl is playing with a teddy bear. a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A persons hand cutting into a plate with some scissors.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man holding a dog in his hand a........ a.<|endoftext|> \n",
            "Step 7000/50000: Avg Running Loss = 4.534742266893387\n",
            "Saving Checkpoint for step :  7100\n",
            "Saving Checkpoint for step :  7200\n",
            "Saving Checkpoint for step :  7300\n",
            "Saving Checkpoint for step :  7400\n",
            "Saving Checkpoint for step :  7500\n",
            "Saving Checkpoint for step :  7600\n",
            "Saving Checkpoint for step :  7700\n",
            "Saving Checkpoint for step :  7800\n",
            "Saving Checkpoint for step :  7900\n",
            "Saving Checkpoint for step :  8000\n",
            "0 - Target captions:\n",
            " A family is enjoying a deep dish pizza together.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A woman eating a pizza with a plate of. a........<|endoftext|> \n",
            "1 - Target captions:\n",
            " A tennis player moving on one foot holding a tennis raquet.  \n",
            "1 - predicted_captions:\n",
            " A man playing tennis with a racket in his hand. tennis court. court. tennis. tennis<|endoftext|> \n",
            "Step 8000/50000: Avg Running Loss = 4.528361698389054\n",
            "Saving Checkpoint for step :  8100\n",
            "Saving Checkpoint for step :  8200\n",
            "Saving Checkpoint for step :  8300\n",
            "Saving Checkpoint for step :  8400\n",
            "Saving Checkpoint for step :  8500\n",
            "Saving Checkpoint for step :  8600\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  8700\n",
            "Saving Checkpoint for step :  8800\n",
            "Saving Checkpoint for step :  8900\n",
            "Saving Checkpoint for step :  9000\n",
            "0 - Target captions:\n",
            " A man standing on a white star that is on the floor and he has his hands crossed.  \n",
            "0 - predicted_captions:\n",
            " A man is holding a tennis racket and a ball.........<|endoftext|> \n",
            "1 - Target captions:\n",
            " People are getting food and beverages at a buffet.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A woman is preparing a meal in the kitchen..........<|endoftext|> \n",
            "Step 9000/50000: Avg Running Loss = 4.471714339733124\n",
            "Saving Checkpoint for step :  9100\n",
            "Saving Checkpoint for step :  9200\n",
            "Saving Checkpoint for step :  9300\n",
            "Saving Checkpoint for step :  9400\n",
            "Saving Checkpoint for step :  9500\n",
            "Saving Checkpoint for step :  9600\n",
            "Saving Checkpoint for step :  9700\n",
            "Saving Checkpoint for step :  9800\n",
            "Saving Checkpoint for step :  9900\n",
            "Saving Checkpoint for step :  10000\n",
            "0 - Target captions:\n",
            " A black and silver glass top stove turned on.<|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A kitchen with a stove, oven, and a microwave. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A man is flying a blue kite in the sky.  \n",
            "1 - predicted_captions:\n",
            " A man is flying a kite in the sky... a. a. a.<|endoftext|> \n",
            "Step 10000/50000: Avg Running Loss = 4.490902932405472\n",
            "Saving Checkpoint for step :  10100\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/capstone-MultimodalGPT/Part1/main.py\", line 49, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/capstone-MultimodalGPT/Part1/main.py\", line 41, in main\n",
            "    train_model(MModalGPT, train_dataloader, val_dataloader, optimizer, device, max_steps,model_save_step,model_val_step,log_step,max_token_filter,tokenizer)\n",
            "  File \"/content/drive/MyDrive/capstone-MultimodalGPT/Part1/network.py\", line 209, in train_model\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▇▇▄█▂▅▇▄▇▃▁▄▅▃▆▃▃▅▅▃▅▃▂▅▃▃▃▃▂▂▄▆▂▅▃▄▂▂▃▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       step 10141\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 4.26865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstep1_projector_pretrain\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/madhurprakashgarg/tsai_multimodal_gpt_project/runs/h6xerces\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240202_123213-h6xerces/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y8BnMC_xS7hk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}