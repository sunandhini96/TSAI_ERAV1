{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1ZFdeJmmmcvBdBirTykn5e-A31cKs00aw",
      "authorship_tag": "ABX9TyNLDCEyl7I0JV957ecd65nQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db3292d17bf24c5a967dffbe32b1c022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8363fd4ab00c4ced8b478f6d9056795f",
              "IPY_MODEL_a773d6ad558647c49cbc02ad85255523",
              "IPY_MODEL_c111a827092942b4a406f7078e006fa8"
            ],
            "layout": "IPY_MODEL_6638cf6d665a4dd689d3e16e5e754c45"
          }
        },
        "8363fd4ab00c4ced8b478f6d9056795f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8935ae08d294383a4a3f43d252fe1c5",
            "placeholder": "​",
            "style": "IPY_MODEL_7619a9866f1146e9951929fa8456dcb3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a773d6ad558647c49cbc02ad85255523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c06c308f2aa34f26a253e4dfa6c4a34a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91e77ddfebfa4fad9d51cebb94280f3a",
            "value": 2
          }
        },
        "c111a827092942b4a406f7078e006fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95d630cb541c44e29bff89fd928f9de9",
            "placeholder": "​",
            "style": "IPY_MODEL_dace632953b44e0f8089b701bbdf67f8",
            "value": " 2/2 [00:01&lt;00:00,  1.37it/s]"
          }
        },
        "6638cf6d665a4dd689d3e16e5e754c45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8935ae08d294383a4a3f43d252fe1c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7619a9866f1146e9951929fa8456dcb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c06c308f2aa34f26a253e4dfa6c4a34a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e77ddfebfa4fad9d51cebb94280f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95d630cb541c44e29bff89fd928f9de9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dace632953b44e0f8089b701bbdf67f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunandhini96/TSAI_ERAV1/blob/main/Capstone/Stage_1/stage_1_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQUH-m2jsp7Z",
        "outputId": "c261d0ce-c3a3-4373-9487-e25840b8ddc7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sJb1AX-jCWd",
        "outputId": "edda9b22-7ba6-4ade-8a74-338903d96a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/capstone\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/capstone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sunandhini96/TSAI_ERAV1.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEGKzxC-sngU",
        "outputId": "5d42eb93-9b56-4490-8d08-f7d636e24e70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TSAI_ERAV1' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/capstone/TSAI_ERAV1/Capstone/Stage_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mCYH5mhvE-Y",
        "outputId": "ced8f4e1-b96c-4dde-8ccf-8726423816a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/capstone/TSAI_ERAV1/Capstone/Stage_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUlIsKxyvIh0",
        "outputId": "cfaab06f-a231-4a83-efd9-c80aec95114f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/m-bain/whisperx.git (from -r requirements.txt (line 6))\n",
            "  Cloning https://github.com/m-bain/whisperx.git to /tmp/pip-req-build-1pwyohqx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperx.git /tmp/pip-req-build-1pwyohqx\n",
            "  Resolved https://github.com/m-bain/whisperx.git to commit 8227807fa9e076901ea4b4fbbf79c9777a6f5e03\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu121)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.8.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.27.0)\n",
            "Requirement already satisfied: transformers==4.37 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.37.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.42.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.16.3)\n",
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.25.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37->-r requirements.txt (line 4)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37->-r requirements.txt (line 4)) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37->-r requirements.txt (line 4)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37->-r requirements.txt (line 4)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37->-r requirements.txt (line 4)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37->-r requirements.txt (line 4)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37->-r requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37->-r requirements.txt (line 4)) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37->-r requirements.txt (line 4)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37->-r requirements.txt (line 4)) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft->-r requirements.txt (line 2)) (5.9.5)\n",
            "Collecting faster-whisper@ git+https://github.com/SYSTRAN/faster-whisper.git@0.10.0 (from whisperx==3.1.1->-r requirements.txt (line 6))\n",
            "  Cloning https://github.com/SYSTRAN/faster-whisper.git (to revision 0.10.0) to /tmp/pip-install-pna9d_73/faster-whisper_652aa4424ab940e4a0794fee10c50626\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SYSTRAN/faster-whisper.git /tmp/pip-install-pna9d_73/faster-whisper_652aa4424ab940e4a0794fee10c50626\n",
            "  Running command git checkout -q e1a218fab1ab02d637b79565995bf1a9c4c83a09\n",
            "  Resolved https://github.com/SYSTRAN/faster-whisper.git to commit e1a218fab1ab02d637b79565995bf1a9c4c83a09\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchaudio>=2 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1->-r requirements.txt (line 6)) (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1->-r requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: setuptools>=65 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1->-r requirements.txt (line 6)) (67.7.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1->-r requirements.txt (line 6)) (3.8.1)\n",
            "Requirement already satisfied: pyannote.audio==3.1.1 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: asteroid-filterbanks>=0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: lightning>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (2.2.0)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: pyannote.core>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (5.0.0)\n",
            "Requirement already satisfied: pyannote.database>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (5.0.1)\n",
            "Requirement already satisfied: pyannote.metrics>=3.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: pyannote.pipeline>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (3.0.1)\n",
            "Requirement already satisfied: pytorch-metric-learning>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (2.4.1)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (13.7.0)\n",
            "Requirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: speechbrain>=0.5.14 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.5.16)\n",
            "Requirement already satisfied: tensorboardX>=2.6 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (2.6.2.2)\n",
            "Requirement already satisfied: torch-audiomentations>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.11.1)\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (1.3.0.post0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes->-r requirements.txt (line 7)) (1.11.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 8)) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 8)) (3.1.41)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 8)) (1.40.3)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 8)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 8)) (3.20.3)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (0.109.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.10.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (0.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (0.26.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (6.1.1)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (3.7.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (3.9.13)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (2.6.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (0.0.9)\n",
            "Requirement already satisfied: ruff>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (0.2.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (0.9.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 11)) (0.27.1)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.10.0->gradio->-r requirements.txt (line 11)) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 11)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 11)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 8)) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 8)) (4.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 11)) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 11)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 11)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 11)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1->-r requirements.txt (line 6)) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio->-r requirements.txt (line 11)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio->-r requirements.txt (line 11)) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37->-r requirements.txt (line 4)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37->-r requirements.txt (line 4)) (2024.2.2)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio->-r requirements.txt (line 11)) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio->-r requirements.txt (line 11)) (1.5.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio->-r requirements.txt (line 11)) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio->-r requirements.txt (line 11)) (0.36.3)\n",
            "Requirement already satisfied: av==10.* in /usr/local/lib/python3.10/dist-packages (from faster-whisper@ git+https://github.com/SYSTRAN/faster-whisper.git@0.10.0->whisperx==3.1.1->-r requirements.txt (line 6)) (10.0.0)\n",
            "Requirement already satisfied: ctranslate2<4,>=3.22 in /usr/local/lib/python3.10/dist-packages (from faster-whisper@ git+https://github.com/SYSTRAN/faster-whisper.git@0.10.0->whisperx==3.1.1->-r requirements.txt (line 6)) (3.24.0)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.10/dist-packages (from faster-whisper@ git+https://github.com/SYSTRAN/faster-whisper.git@0.10.0->whisperx==3.1.1->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r requirements.txt (line 11)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r requirements.txt (line 11)) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 8)) (5.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 11)) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 11)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 11)) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 11)) (0.17.1)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.10.1)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (2.2.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (4.9.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper@ git+https://github.com/SYSTRAN/faster-whisper.git@0.10.0->whisperx==3.1.1->-r requirements.txt (line 6)) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper@ git+https://github.com/SYSTRAN/faster-whisper.git@0.10.0->whisperx==3.1.1->-r requirements.txt (line 6)) (23.5.26)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (2.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.6.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.9.0)\n",
            "Requirement already satisfied: optuna>=3.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (3.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (2.16.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.10/dist-packages (from speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.1.99)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio->-r requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: julius<0.3,>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.2.7)\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.10.1)\n",
            "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (1.2.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (2.21)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec->torch->-r requirements.txt (line 1)) (3.9.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (3.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.58.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (1.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.1.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (6.8.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (2.0.25)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: primePy>=1.3 in /usr/local/lib/python3.10/dist-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (1.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper@ git+https://github.com/SYSTRAN/faster-whisper.git@0.10.0->whisperx==3.1.1->-r requirements.txt (line 6)) (10.0)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.10/dist-packages (from hyperpyyaml->speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.18.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (4.2.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (0.2.8)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1->-r requirements.txt (line 6)) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install glob2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbKquMfVvRzB",
        "outputId": "2fe80f50-207c-4861-bcab-fdbc3615b523"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.10/dist-packages (0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob2\n",
        "import torch\n",
        "from network import CLIPPhi2Model, train_model, frange_cycle_linear\n",
        "from dataset import collate_fn, llavadataset\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer\n",
        "import pickle\n",
        "#import bitsandbytes as bnb\n",
        "import gc\n",
        "\n",
        "\n",
        "def main():\n",
        "    with open(\"/content/drive/MyDrive/capstone/coco_captions.pickle\", \"rb\") as fp:   # Unpickling\n",
        "        coco_unpickle = pickle.load(fp)\n",
        "\n",
        "    clip_model_name = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
        "    phi_model_name  = \"microsoft/phi-2\"\n",
        "    train_batch_size = 2\n",
        "    val_batch_size   = 4\n",
        "    device     = 'cuda'\n",
        "    tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
        "\n",
        "    # model\n",
        "    MModalGPT        = CLIPPhi2Model().to(device)\n",
        "    max_steps        = 20000\n",
        "    model_save_step  = 1000\n",
        "    model_val_step   = 1000\n",
        "    log_step         = 1000\n",
        "    max_token_filter = 35\n",
        "\n",
        "    # data loaders\n",
        "    train_dataloader = DataLoader(llavadataset(coco_unpickle, phi_model_name,clip_model_name,'train',tokenizer),\n",
        "                      collate_fn=collate_fn, batch_size=train_batch_size, num_workers = 10, shuffle=True, pin_memory=True)\n",
        "\n",
        "    val_dataloader   = DataLoader(llavadataset(coco_unpickle, phi_model_name,clip_model_name,'val',tokenizer),\n",
        "                      collate_fn=collate_fn, batch_size=val_batch_size, num_workers = 10, shuffle=True, pin_memory=True)\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, MModalGPT.parameters()), lr=1e-4)\n",
        "    train_model(MModalGPT, train_dataloader, val_dataloader, optimizer, device, max_steps,model_save_step,model_val_step,log_step,max_token_filter,tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "OxS3LC12vR3F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    wandb.init(project=\"capstone_multimodal_gpt_project\", name=\"step1_projector_pretrain\") #,log_model='all')\n",
        "    torch.cuda.amp.autocast(enabled=True)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.set_float32_matmul_precision('medium')\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "db3292d17bf24c5a967dffbe32b1c022",
            "8363fd4ab00c4ced8b478f6d9056795f",
            "a773d6ad558647c49cbc02ad85255523",
            "c111a827092942b4a406f7078e006fa8",
            "6638cf6d665a4dd689d3e16e5e754c45",
            "a8935ae08d294383a4a3f43d252fe1c5",
            "7619a9866f1146e9951929fa8456dcb3",
            "c06c308f2aa34f26a253e4dfa6c4a34a",
            "91e77ddfebfa4fad9d51cebb94280f3a",
            "95d630cb541c44e29bff89fd928f9de9",
            "dace632953b44e0f8089b701bbdf67f8"
          ]
        },
        "id": "uk_9IjIUvR6g",
        "outputId": "4c8cf528-870d-4c4b-ff5b-1a2040051108"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgsunandhini\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/capstone/TSAI_ERAV1/Capstone/Stage_1/wandb/run-20240212_104111-1r75ku94</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gsunandhini/capstone_multimodal_gpt_project/runs/1r75ku94' target=\"_blank\">step1_projector_pretrain</a></strong> to <a href='https://wandb.ai/gsunandhini/capstone_multimodal_gpt_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gsunandhini/capstone_multimodal_gpt_project' target=\"_blank\">https://wandb.ai/gsunandhini/capstone_multimodal_gpt_project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gsunandhini/capstone_multimodal_gpt_project/runs/1r75ku94' target=\"_blank\">https://wandb.ai/gsunandhini/capstone_multimodal_gpt_project/runs/1r75ku94</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db3292d17bf24c5a967dffbe32b1c022",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size 532577 and validation size 59176\n",
            "Train size 532577 and validation size 59176\n",
            "Training started.\n",
            "Step 0/20000: Avg Running Loss = 9.10235595703125\n",
            "Saving Checkpoint for step :  1000\n",
            "0 - Target captions:\n",
            " A tan stuffed animal wearing a green bandana.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A woman is a man in a suit and tie sitting on a couch in a living room with<|endoftext|> \n",
            "1 - Target captions:\n",
            " A woman sitting at a table with a pair of scissors cutting paper.<|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A woman is a man in a suit and a man in a suit and a woman in a<|endoftext|> \n",
            "2 - Target captions:\n",
            " Young woman demonstrating a refrigerator in an appliance store.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A woman is a part of a group of people standing in front of a building. The building<|endoftext|> \n",
            "3 - Target captions:\n",
            " A male tennis player on a court with a tennis racket in the foreground.  \n",
            "3 - predicted_captions:\n",
            " A tennis player is playing a game of tennis on a court. The court is surrounded by a<|endoftext|> \n",
            "Step 1000/20000: Avg Running Loss = 5.513734761714935\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  2000\n",
            "0 - Target captions:\n",
            " A woman eating a large cheese pizza on a plate.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man is a man holding a plate of food. on a table. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A cat lying half asleep on a chair with a teddy bear.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A cat is sitting on a couch with a blanket. on it. a. a. a<|endoftext|> \n",
            "2 - Target captions:\n",
            " A tastefully decorated living room with pillows on a blue and white couch with a small ottoman in the foreground.  \n",
            "2 - predicted_captions:\n",
            " A man is sitting on a couch in a living room. a television is on a stand in<|endoftext|> \n",
            "3 - Target captions:\n",
            " A lady tennis player returning a serve back to her opponent. <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A woman is playing tennis on a court.\n",
            "A woman is playing tennis on a court.<|endoftext|> \n",
            "Step 2000/20000: Avg Running Loss = 5.049588554382324\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  3000\n",
            "0 - Target captions:\n",
            " Tennis player retrieving a ball from the court's surface.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man playing tennis on a court with a racket. a. a. of. the.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A woman rides a bike while looking at a phone.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man riding a bike with a woman on the back of it. of the.\n",
            "of<|endoftext|> \n",
            "2 - Target captions:\n",
            " Stuffed animals hung on a barbed wire fence lend this place a creepy air.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A small, green, and white bird is sitting on a branch of a tree. the.<|endoftext|> \n",
            "3 - Target captions:\n",
            " Two little bunnie figurines are seated next to a bear in front of a coca-cola bottle with daisies inside of it.  \n",
            "3 - predicted_captions:\n",
            " A small, fluffy cat is sitting on a a a a a a a a a a a<|endoftext|> \n",
            "Step 3000/20000: Avg Running Loss = 4.894830216884613\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  4000\n",
            "0 - Target captions:\n",
            " A black and white clock next to a brick wall.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A large, a small and a medium sized a of a of a of a of a of<|endoftext|> \n",
            "1 - Target captions:\n",
            " A large hallway with a clock hanging above it.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man is a man walking down a street at night. a street lamp in the. a<|endoftext|> \n",
            "2 - Target captions:\n",
            " A baseball player about to make a run for it.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A baseball player is a pitcher on the field. a. a. a. a. a<|endoftext|> \n",
            "3 - Target captions:\n",
            " A young girl is in her bedroom looking at her phone with food in her mouth.  \n",
            "3 - predicted_captions:\n",
            " A woman is a woman sitting on a couch with a a a a a a a a a<|endoftext|> \n",
            "Step 4000/20000: Avg Running Loss = 4.73854870724678\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  5000\n",
            "0 - Target captions:\n",
            " a round table has a glass vase with a single flower   \n",
            "0 - predicted_captions:\n",
            " A table with a plate of food on it a. a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A man is walking away from a tennis court.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A woman playing tennis on a court. a racket a tennis. a. a. a.<|endoftext|> \n",
            "2 - Target captions:\n",
            " A toddler is holding a ball and tennis racquet.<|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A boy holding a ball and a racket on a tennis court. a. a. a.<|endoftext|> \n",
            "3 - Target captions:\n",
            " A tall building with a movie poster in the middle.<|endoftext|><|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A large sign with a picture of a man holding a sign. a. a. a.<|endoftext|> \n",
            "Step 5000/20000: Avg Running Loss = 4.741553103208542\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  6000\n",
            "0 - Target captions:\n",
            " A clock tower that is surrounded by buildings.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A clock tower in the middle of a city. a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " These electronic devices are laying next to each other.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A row of a number of televisions on a. a. a. of. a.<|endoftext|> \n",
            "2 - Target captions:\n",
            " A plastic bottle of water is sitting on a granite counter top.  \n",
            "2 - predicted_captions:\n",
            " A bathroom with a large mirror and a sink. a. a. a. a. a<|endoftext|> \n",
            "3 - Target captions:\n",
            " a clock tower on top of a long building with many windows<|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A large clock tower in the center of a city. a. a. a. a.<|endoftext|> \n",
            "Step 6000/20000: Avg Running Loss = 4.68357896733284\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  7000\n",
            "0 - Target captions:\n",
            " Laptop picture of three men in suits talking to each other. <|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man is a man sitting in front of a computer. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " Two people on one side of a grass tennis court playing a tennis game together.  \n",
            "1 - predicted_captions:\n",
            " A woman is playing tennis with a man. a. a. a. a. a.<|endoftext|> \n",
            "2 - Target captions:\n",
            " a counter top with a sink in it<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A kitchen with a large refrigerator and a. a microwave. a. a. a. a<|endoftext|> \n",
            "3 - Target captions:\n",
            " Woman in uniform is showing an object to two small girls.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A girl is a holding a a a a a a a a a a a a a a<|endoftext|> \n",
            "Step 7000/20000: Avg Running Loss = 4.625743489265442\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  8000\n",
            "0 - Target captions:\n",
            " Many stuffed bears wearing scarves and hats on a shelf.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A bunch of cute animals on a bed of pillows.\n",
            "\n",
            "The above pictures are of<|endoftext|> \n",
            "1 - Target captions:\n",
            " Three men are sitting on the couch, one is on the laptop.   \n",
            "1 - predicted_captions:\n",
            " A group of people sitting on a couch in a room. a laptop. a. a.<|endoftext|> \n",
            "2 - Target captions:\n",
            " Several people gathered around a table with electronic equipment.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A group of people sitting in front of a computer... laptops. of. a.<|endoftext|> \n",
            "3 - Target captions:\n",
            " A shirtless tennis player prepares to serve the ball.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A man is a tennis player on a court. a tennis racket. a. a. a<|endoftext|> \n",
            "Step 8000/20000: Avg Running Loss = 4.604630175590515\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  9000\n",
            "0 - Target captions:\n",
            " A bowl of various healthy vegetables sits next to a beverage.<|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A plate of a bowl of fruit and a. on it a. a. on the.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A table topped with a bowl of rice and a chicken dish.  \n",
            "1 - predicted_captions:\n",
            " A girl is a little girl eating a bowl of food on a. her. her. her<|endoftext|> \n",
            "2 - Target captions:\n",
            " A brown teddy bear is sitting on a green chair.<|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A teddy bear sitting on a bed with a pillow on the. bed. bed. the<|endoftext|> \n",
            "3 - Target captions:\n",
            " A baseball player holding a bat on a field.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A baseball player is a pitcher on the field a. a. a. a. a.<|endoftext|> \n",
            "Step 9000/20000: Avg Running Loss = 4.58531127333641\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  10000\n",
            "0 - Target captions:\n",
            " two people one with a cell phone <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A woman holding a phone to her mouth. a. a...... a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A man holding a cell phone and taking a photo of a tree.<|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A woman holding a camera in a tree. a. a. of. trees. a.<|endoftext|> \n",
            "2 - Target captions:\n",
            " a group of people stand on a hill as they fly a kite   \n",
            "2 - predicted_captions:\n",
            " A man flying a kite with his wife. in the. a. a. sky.<|endoftext|> \n",
            "3 - Target captions:\n",
            " Two uncooked pizzas are on a large cutting board.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A pizza is a pizza with a pepperoni and cheese. on. a. oven. oven<|endoftext|> \n",
            "Step 10000/20000: Avg Running Loss = 4.534350307941437\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  11000\n",
            "0 - Target captions:\n",
            " A table topped with a jar filled with flowers.<|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A table with a vase of flowers on it a. a. a. on. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " a desk with a computer and a old telephone. <|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A woman is a phone on a desk with a. a. a.... a<|endoftext|> \n",
            "2 - Target captions:\n",
            " a close up of a person talking on a cell phone<|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A man holding a phone to his mouth. his. his. his. his. his.<|endoftext|> \n",
            "3 - Target captions:\n",
            " Breakfast of coffee and pastries sits on the table.  \n",
            "3 - predicted_captions:\n",
            " A plate of a chocolate chip cookie and a a a a a a a a a a a<|endoftext|> \n",
            "Step 11000/20000: Avg Running Loss = 4.522801763534546\n",
            "Saving Checkpoint for step :  12000\n",
            "0 - Target captions:\n",
            " a young boy swinging a baseball bat at a ball <|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A boy is playing with a baseball in a field a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " a couple of people that are shopping for some food<|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man is carrying a large amount of items in a. a. a. a. a<|endoftext|> \n",
            "2 - Target captions:\n",
            " A couple of glass vases near a window sill.<|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A table with a vase of flowers on it a the. a. a. the.<|endoftext|> \n",
            "3 - Target captions:\n",
            " The young child is sitting at the table to eat.   \n",
            "3 - predicted_captions:\n",
            " A table with a plate of food on it a a a a a a a a a a<|endoftext|> \n",
            "Step 12000/20000: Avg Running Loss = 4.518374887943268\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  13000\n",
            "0 - Target captions:\n",
            " A man standing next to two boxes filled with pizzas.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man holding a pizza a a a a a. a. a. a pizza a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A wall with two doors and various wall hanging on it, including a clock a mirror and numbered coat hangers.  \n",
            "1 - predicted_captions:\n",
            " A clock and a a a a a a a. a. a. a. a.<|endoftext|> \n",
            "2 - Target captions:\n",
            " a smiling woman holding a phone to her ear<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A girl with a a a a a a a. her. her.....<|endoftext|> \n",
            "3 - Target captions:\n",
            " baseball player holding baseball bat as man takes photos<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A group of baseball players on a field a. a........<|endoftext|> \n",
            "Step 13000/20000: Avg Running Loss = 5.547182035207748\n",
            "Saving Checkpoint for step :  14000\n",
            "0 - Target captions:\n",
            " A metallic refrigerator freezer sitting inside of a room.<|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A row of a of a of a of a.........<|endoftext|> \n",
            "1 - Target captions:\n",
            " there is a baseball player that is playing a game   \n",
            "1 - predicted_captions:\n",
            " A baseball player is a a a a a a. a. a. a. a.<|endoftext|> \n",
            "2 - Target captions:\n",
            " some baseball players are playing baseball on a field<|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A woman is riding a a a a a a a. a. a. a. a<|endoftext|> \n",
            "3 - Target captions:\n",
            " A photo of a kitchen focused on the window.<|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A room with a window and a a. a. a. a. a. a.<|endoftext|> \n",
            "Step 14000/20000: Avg Running Loss = 5.454179081201553\n",
            "Saving Checkpoint for step :  15000\n",
            "0 - Target captions:\n",
            " THERE ARE FOOD THAT IS ON THE TABLE AND CUPS OF BEVERAGE<|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A plate of a a a a a a a a a a a a a a a a<|endoftext|> \n",
            "1 - Target captions:\n",
            " White teddy bears cavort about an ice cream filled refrigerator in a department store window.  \n",
            "1 - predicted_captions:\n",
            " A group of a a a a a a a. a. a. a. a.<|endoftext|> \n",
            "2 - Target captions:\n",
            " A crocheted striped pouch containing a cell phone.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A doll with a a a a a a a. a. a. a a. a<|endoftext|> \n",
            "3 - Target captions:\n",
            " A plate of boiled eggs, avocados and tomatoes accompany cups of coffee and water.  \n",
            "3 - predicted_captions:\n",
            " A plate of a a a a a a a a a. a. a. a.<|endoftext|> \n",
            "Step 15000/20000: Avg Running Loss = 5.437658541440964\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  16000\n",
            "0 - Target captions:\n",
            " A kid opening a toy teddy bear sitting in a tiny wagon wrapped in plastic.  \n",
            "0 - predicted_captions:\n",
            " A girl is a a a a a a a a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " The building is shaped like a tall, old cathedral.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A tall tower of a a a a. a. a. a. a. a.<|endoftext|> \n",
            "2 - Target captions:\n",
            " The three men are together in a room observing something<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A group of a playing a a a a a. a. a. a. a.<|endoftext|> \n",
            "3 - Target captions:\n",
            " A group of people celebrates a first birthday.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A group of women a a a cake a cake. a cake. a cake. a cake<|endoftext|> \n",
            "Step 16000/20000: Avg Running Loss = 5.408080296993256\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  17000\n",
            "0 - Target captions:\n",
            " A woman sitting on a couch beside a laptop computer.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man sitting on a a a a a a a a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " The baseball player tries to beat the throw.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A baseball player a a a a a a. a. a. a. a. a<|endoftext|> \n",
            "2 - Target captions:\n",
            " a group of men eating some food at a table<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A group of people eating a a a a a. a. a. a. a.<|endoftext|> \n",
            "3 - Target captions:\n",
            " Two young men are sitting down and looking at a cell phone.   \n",
            "3 - predicted_captions:\n",
            " A girl sitting a a a a a a a a. a. a. a. a<|endoftext|> \n",
            "Step 17000/20000: Avg Running Loss = 5.384097515821457\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  18000\n",
            "0 - Target captions:\n",
            " The base ball player is sliding in to home base. <|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A group of a a a a a a a a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A crowd of people are flying kites in a field.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A group of people flying a k flying a k a. a. a. a. a<|endoftext|> \n",
            "2 - Target captions:\n",
            " A woman laughing while sitting in a chair next to an over turned desk.  \n",
            "2 - predicted_captions:\n",
            " A girl sitting a a a a a a a. a. a. a. a.<|endoftext|> \n",
            "3 - Target captions:\n",
            " A couple of little kids holding tennis racquets.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A girl girl holding a a a a a a a. a. a. a. a<|endoftext|> \n",
            "Step 18000/20000: Avg Running Loss = 5.40046708393097\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  19000\n",
            "0 - Target captions:\n",
            " A beautiful old clock tower in a city's historic district<|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A clock tower clock tower in a a. a..... a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A young girl and two young men are sitting together in a restaurant.  \n",
            "1 - predicted_captions:\n",
            " A group of of a a a a pizza. a. a. pizza. pizza. a<|endoftext|> \n",
            "2 - Target captions:\n",
            " an image of a cat that is smelling flowers<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A cat cat a a a a a flowers......... flowers<|endoftext|> \n",
            "3 - Target captions:\n",
            " Two city buses are driving down the street.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "3 - predicted_captions:\n",
            " A clock clock clock clock clock clock clock clock... city street.....<|endoftext|> \n",
            "Step 19000/20000: Avg Running Loss = 5.361190677881241\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  20000\n",
            "0 - Target captions:\n",
            " colored woman in a corner talking on the phone.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A woman woman a a a a a a a. a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " a tall clock tower with a sky in the background<|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A clock tower of a clock tower the clock. the. the. the. the. the<|endoftext|> \n",
            "2 - Target captions:\n",
            " A woman standing with a cell phone in her hand.<|endoftext|><|endoftext|>  \n",
            "2 - predicted_captions:\n",
            " A woman holding a phone a a a phone. a... a. phone. phone<|endoftext|> \n",
            "3 - Target captions:\n",
            " a lady talking to a smiling man while another man takes a picture  \n",
            "3 - predicted_captions:\n",
            " A woman woman a a a a a a a.... a. a. a<|endoftext|> \n",
            "Reached the max steps. Training stopped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDvfZTqLvR_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XHDz3eszvSCm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}