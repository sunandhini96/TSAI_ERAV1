{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZthZ1qOa_lo",
        "outputId": "30466bbc-39e0-4354-a05f-2539e92bf45c"
      },
      "id": "VZthZ1qOa_lo",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Requirements"
      ],
      "metadata": {
        "id": "fAkaEIE3uHWp"
      },
      "id": "fAkaEIE3uHWp"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sunandhini96/TSAI_ERAV1.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es1nIBBz510b",
        "outputId": "72370803-e97e-4de4-d3b0-e70c0de3d11e"
      },
      "id": "es1nIBBz510b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TSAI_ERAV1'...\n",
            "remote: Enumerating objects: 1358, done.\u001b[K\n",
            "remote: Counting objects: 100% (629/629), done.\u001b[K\n",
            "remote: Compressing objects: 100% (305/305), done.\u001b[K\n",
            "remote: Total 1358 (delta 390), reused 514 (delta 316), pack-reused 729\u001b[K\n",
            "Receiving objects: 100% (1358/1358), 31.06 MiB | 12.09 MiB/s, done.\n",
            "Resolving deltas: 100% (635/635), done.\n",
            "Updating files: 100% (189/189), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/capstone/Stage_2/TSAI_ERAV1/Capstone/Stage_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RJWsDe0586l",
        "outputId": "f9bbdc2b-bf46-4d81-f9b0-da3cf5c862ef"
      },
      "id": "5RJWsDe0586l",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/capstone/Stage_2/TSAI_ERAV1/Capstone/Stage_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T8leZCyBO7b",
        "outputId": "372d7fbc-5988-4e8a-ce13-fddb33a6aba6"
      },
      "id": "6T8leZCyBO7b",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1d11cb27-d931-4ac1-a1fc-3e79fa5c1358",
      "metadata": {
        "id": "1d11cb27-d931-4ac1-a1fc-3e79fa5c1358"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import peft\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import AutoTokenizer,BitsAndBytesConfig, AutoModelForCausalLM, CLIPVisionModel, AutoProcessor\n",
        "import torch\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import pandas as pd\n",
        "from torch.nn import functional as F\n",
        "import csv\n",
        "import random\n",
        "from PIL import Image\n",
        "import requests\n",
        "import wandb\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from finetune_dataset import llavadataset, collate_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define values for parameters used in code"
      ],
      "metadata": {
        "id": "Dv-VWlgluLs9"
      },
      "id": "Dv-VWlgluLs9"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ba1d568a-3fe5-4130-a504-9f8861920981",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba1d568a-3fe5-4130-a504-9f8861920981",
        "outputId": "b9ec97b5-9336-42bb-9283-36e206ada2ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "clip_model_name = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
        "phi_model_name  = \"microsoft/phi-2\"\n",
        "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
        "processor  = AutoProcessor.from_pretrained(clip_model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "train_batch_size = 4\n",
        "clip_embed = 640\n",
        "phi_embed  = 2560\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_workers = 10\n",
        "IMAGE_TOKEN_ID = 23893 # token for word comment\n",
        "max_steps      = 20000\n",
        "EOS_TOKEN_ID   = 50256\n",
        "phi_patches    = 49\n",
        "vocab_size     = 51200\n",
        "max_generate_length = 100\n",
        "model_val_step      = 1000\n",
        "model_log_step      = 100\n",
        "model_save_step     = 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Automatically chooses the precision for GPU"
      ],
      "metadata": {
        "id": "ctOYBWJRuzUz"
      },
      "id": "ctOYBWJRuzUz"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.amp.autocast(enabled=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEj2T6u9bkAx",
        "outputId": "f828c2a4-1ad8-4ec3-d0d3-b4fa29785faf"
      },
      "id": "fEj2T6u9bkAx",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.cuda.amp.autocast_mode.autocast at 0x7c2f6f1aa710>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logging in Wandb"
      ],
      "metadata": {
        "id": "tXbl9yRGu4aZ"
      },
      "id": "tXbl9yRGu4aZ"
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project  = \"tsai_multimodal_gpt_project\", name=\"step2_finetuning_QLoRA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "BZpbvmmIa_1i",
        "outputId": "9b8e5fab-a590-4b83-8a0f-affbd7709142"
      },
      "id": "BZpbvmmIa_1i",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgsunandhini\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/capstone/Stage_2/TSAI_ERAV1/Capstone/Stage_2/wandb/run-20240211_174149-htj6z0rn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project/runs/htj6z0rn' target=\"_blank\">step2_finetuning_QLoRA</a></strong> to <a href='https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project' target=\"_blank\">https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project/runs/htj6z0rn' target=\"_blank\">https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project/runs/htj6z0rn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project/runs/htj6z0rn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c2e3061ee00>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8692e0ef-165c-40c5-a6a0-a601d1c5099c",
      "metadata": {
        "id": "8692e0ef-165c-40c5-a6a0-a601d1c5099c"
      },
      "source": [
        "# Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "data = pd.read_csv('/content/drive/MyDrive/capstone/Stage_2/training.csv')\n",
        "\n",
        "# Shuffle the data (optional but recommended)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Calculate the number of rows for the training set\n",
        "train_size = int(0.9 * len(data))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data = data.iloc[:train_size]\n",
        "test_data = data.iloc[train_size:]\n",
        "\n",
        "# Save the training and testing sets to new CSV files\n",
        "train_data.to_csv('/content/drive/MyDrive/capstone/Stage_2/train_data.csv', index=False)\n",
        "test_data.to_csv('/content/drive/MyDrive/capstone/Stage_2/sample_data.csv', index=False)"
      ],
      "metadata": {
        "id": "Z8t4r-7oeiAU"
      },
      "id": "Z8t4r-7oeiAU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5844cdb1-2a30-4d65-a129-606d54e2b8e7",
      "metadata": {
        "id": "5844cdb1-2a30-4d65-a129-606d54e2b8e7"
      },
      "outputs": [],
      "source": [
        "# training data\n",
        "csv_file = '/content/drive/MyDrive/capstone/Stage_2/train_data.csv'\n",
        "qa_dataset = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "Gndwzm8H7fdL",
        "outputId": "fdb47edb-21e0-4623-d17e-1632e9e4238a"
      },
      "id": "Gndwzm8H7fdL",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id                                            img_url  \\\n",
              "0  146649  http://images.cocodataset.org/train2017/000000...   \n",
              "1  359548  http://images.cocodataset.org/train2017/000000...   \n",
              "2  152942  http://images.cocodataset.org/train2017/000000...   \n",
              "3  222083  http://images.cocodataset.org/train2017/000000...   \n",
              "4  399001  http://images.cocodataset.org/train2017/000000...   \n",
              "\n",
              "                                               input  \\\n",
              "0  What are some differences between snowboarding...   \n",
              "1  What is the terrain like where the two people ...   \n",
              "2    Can you describe the woman's facial expression?   \n",
              "3  What is the condition of the bathroom stall in...   \n",
              "4  What are some ingredients present in the sandw...   \n",
              "\n",
              "                                               label  \n",
              "0  Snowboarding and skiing are both winter sports...  \n",
              "1  The terrain where the two people are flying th...  \n",
              "2  The young woman has a shy smile on her face wh...  \n",
              "3  The bathroom stall in the image appears to be ...  \n",
              "4  The sandwich contains deli meat, onions, sauce...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae1e4ff1-cb65-47d9-aa96-205ef73d712c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img_url</th>\n",
              "      <th>input</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>146649</td>\n",
              "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
              "      <td>What are some differences between snowboarding...</td>\n",
              "      <td>Snowboarding and skiing are both winter sports...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>359548</td>\n",
              "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
              "      <td>What is the terrain like where the two people ...</td>\n",
              "      <td>The terrain where the two people are flying th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>152942</td>\n",
              "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
              "      <td>Can you describe the woman's facial expression?</td>\n",
              "      <td>The young woman has a shy smile on her face wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>222083</td>\n",
              "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
              "      <td>What is the condition of the bathroom stall in...</td>\n",
              "      <td>The bathroom stall in the image appears to be ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>399001</td>\n",
              "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
              "      <td>What are some ingredients present in the sandw...</td>\n",
              "      <td>The sandwich contains deli meat, onions, sauce...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae1e4ff1-cb65-47d9-aa96-205ef73d712c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ae1e4ff1-cb65-47d9-aa96-205ef73d712c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ae1e4ff1-cb65-47d9-aa96-205ef73d712c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-063beba5-5c02-43b2-bb06-7b42d2f67b91\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-063beba5-5c02-43b2-bb06-7b42d2f67b91')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-063beba5-5c02-43b2-bb06-7b42d2f67b91 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(llavadataset(qa_dataset, phi_model_name,clip_model_name,tokenizer,processor),\n",
        "                  collate_fn=collate_fn, batch_size=train_batch_size, num_workers = num_workers, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "cd-d-V6sAUhH"
      },
      "id": "cd-d-V6sAUhH",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1bccc4c4-55d7-46db-aba1-5871bf88c840",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bccc4c4-55d7-46db-aba1-5871bf88c840",
        "outputId": "2a873c62-b23b-41b5-f998-0e6e4d2b7583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['465508', 'http://images.cocodataset.org/train2017/000000465508.jpg', 'What is the skateboarder doing in the image?\\n<image>', 'The skateboarder is performing a flip and other tricks in a large cement courtyard.']\n"
          ]
        }
      ],
      "source": [
        "file = open('/content/drive/MyDrive/capstone/Stage_2/sample_data.csv')\n",
        "csvreader = csv.reader(file)\n",
        "sample_val_data = []\n",
        "for row in csvreader:\n",
        "    sample_val_data.append(row)\n",
        "print(sample_val_data[1])\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f57a66-8827-47b9-b006-d409d6ce4cc2",
      "metadata": {
        "id": "59f57a66-8827-47b9-b006-d409d6ce4cc2"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ef46a021-3811-4297-9604-b4d4b181cc5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "292c0ae28eb04e3991ef044d9064d8d9",
            "f64c5e2a3a2f4cfb82c2a3059051e10c",
            "b0ee97045f6c444aba854b7744d4bfd9",
            "3cca4167c5e84773a60b85aef7517fb5",
            "99af683c0ebc43c7adf3ccbf302ba6a3",
            "61248160972d4987b9af682c49de2c2f",
            "828a4ad5a16644b695d9095dc4b60dd3",
            "22ccb5b93d64433da2538d238ce253a6",
            "27d86fe9e4e146ec86285ea00064e7c1",
            "5494bd13106445d081008924afe897ec",
            "e7b11a6e298a4edfb402a104e5356eb3"
          ]
        },
        "id": "ef46a021-3811-4297-9604-b4d4b181cc5a",
        "outputId": "af6c00e5-73b8-492c-8357-52196a3a0d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "292c0ae28eb04e3991ef044d9064d8d9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "clip_model = CLIPVisionModel.from_pretrained(clip_model_name).to(device)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,)\n",
        "\n",
        "phi_model = AutoModelForCausalLM.from_pretrained(\n",
        "    phi_model_name,\n",
        "    torch_dtype=torch.float32,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "phi_model.config.use_cache = False\n",
        "projection_layer = torch.nn.Linear(clip_embed, phi_embed).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(phi_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iova7towCRf",
        "outputId": "29c8378a-5686-4597-d7da-c1f1ad5cd6a0"
      },
      "id": "4iova7towCRf",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PhiForCausalLM(\n",
            "  (model): PhiModel(\n",
            "    (embed_tokens): Embedding(51200, 2560)\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x PhiDecoderLayer(\n",
            "        (self_attn): PhiAttention(\n",
            "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (rotary_emb): PhiRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): PhiMLP(\n",
            "          (activation_fn): NewGELUActivation()\n",
            "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
            "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
            "        )\n",
            "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d9e720ab-4f63-487d-ad9b-97ab0dd2300b",
      "metadata": {
        "id": "d9e720ab-4f63-487d-ad9b-97ab0dd2300b",
        "outputId": "81be05b2-f500-4e8f-e3e9-11d1f11ae313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 94,371,840 || all params: 2,874,055,680 || trainable%: 3.2835773035545364\n"
          ]
        }
      ],
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"dense\",\n",
        "        \"fc1\",\n",
        "        \"fc2\",\n",
        "    ]\n",
        ")\n",
        "peft_model = peft.get_peft_model(phi_model, peft_config).to(device)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleResBlock(nn.Module):\n",
        "    def __init__(self, phi_embed):\n",
        "        super().__init__()\n",
        "        self.pre_norm = nn.LayerNorm(phi_embed)\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(phi_embed, phi_embed),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(phi_embed, phi_embed)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.pre_norm(x)\n",
        "        return x + self.proj(x)"
      ],
      "metadata": {
        "id": "zF8iTxkV8j42"
      },
      "id": "zF8iTxkV8j42",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projection_model = SimpleResBlock(phi_embed).to(device)"
      ],
      "metadata": {
        "id": "9sD46TQh8lLp"
      },
      "id": "9sD46TQh8lLp",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OGtncWj81Sj"
      },
      "id": "2OGtncWj81Sj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e219df78-1e2b-4730-81d6-5a7f794d8e86",
      "metadata": {
        "id": "e219df78-1e2b-4730-81d6-5a7f794d8e86"
      },
      "outputs": [],
      "source": [
        "# clip non trainable\n",
        "for network in [clip_model]:\n",
        "    for param in network.parameters():\n",
        "        param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9885fcad-2060-42c0-b2e9-e6b9469fde7f",
      "metadata": {
        "id": "9885fcad-2060-42c0-b2e9-e6b9469fde7f",
        "outputId": "a62273fc-eb35-46bb-ec33-c1d89ee5e051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peft_model :94371840\n",
            "projection layer :1640960\n",
            "projection model :13117440\n",
            "clip_model :0\n",
            "phi_model :94371840\n"
          ]
        }
      ],
      "source": [
        "# check trainable paramaeters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"peft_model :{count_parameters(peft_model)}\")\n",
        "print(f\"projection layer :{count_parameters(projection_layer)}\")\n",
        "print(f\"projection model :{count_parameters(projection_model)}\")\n",
        "print(f\"clip_model :{count_parameters(clip_model)}\")\n",
        "print(f\"phi_model :{count_parameters(phi_model)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load checkpoints"
      ],
      "metadata": {
        "id": "8pcpkQOrvYZE"
      },
      "id": "8pcpkQOrvYZE"
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir /content/drive/MyDrive/capstone/Stage_2/model_chkpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ALtgD169_PH",
        "outputId": "3af7c81d-e164-478b-8ebd-8fed7c3bd73d"
      },
      "id": "7ALtgD169_PH",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/capstone/Stage_2/model_chkpt’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fad7fd86-4d63-41f6-a23e-81501fa08ac9",
      "metadata": {
        "id": "fad7fd86-4d63-41f6-a23e-81501fa08ac9",
        "outputId": "67fafa35-58f3-469d-b973-2e634e9fb6b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded step1 checkpoint\n"
          ]
        }
      ],
      "source": [
        "if os.path.isfile('/content/drive/MyDrive/capstone/Stage_2/model_chkpt/step2_projection.pth'):\n",
        "    projection_layer.load_state_dict(torch.load('/content/drive/MyDrive/capstone/Stage_2/model_chkpt/ft_projection_layer.pth'))\n",
        "    projection_model.load_state_dict(torch.load('/content/drive/MyDrive/capstone/Stage_2/model_chkpt/ft_projection_model.pth'))\n",
        "    peft_model.from_pretrained(phi_model,'/content/drive/MyDrive/capstone/Stage_2/model_chkpt/qlora_adaptor')\n",
        "    print(\"Loaded step2 checkpoint\")\n",
        "\n",
        "else:\n",
        "    projection_layer.load_state_dict(torch.load('/content/drive/MyDrive/capstone/model_chkpt/clipphi_proj.pth'))\n",
        "    projection_model.load_state_dict(torch.load('/content/drive/MyDrive/capstone/model_chkpt/clipphi_resblock.pth'))\n",
        "    print(\"Loaded step1 checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1ce5193f-7251-41ed-b2b6-80b6ee613544",
      "metadata": {
        "id": "1ce5193f-7251-41ed-b2b6-80b6ee613544",
        "outputId": "760da952-b363-4090-9fed-9771db62dc77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['522342',\n",
              " 'http://images.cocodataset.org/train2017/000000522342.jpg',\n",
              " 'Where are the parking meters located?',\n",
              " 'The parking meters are located on the sidewalk beside the road.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "sample_val_data[200]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "745aeb44-44fe-4579-8d84-29e33e8a660f",
      "metadata": {
        "id": "745aeb44-44fe-4579-8d84-29e33e8a660f"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f950cd90-fb80-4991-9b9d-5855bded45b6",
      "metadata": {
        "id": "f950cd90-fb80-4991-9b9d-5855bded45b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14444274-03b7-4a7c-8fc4-164745c87b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: http://images.cocodataset.org/train2017/000000343676.jpg\n",
            "Question: Where are the sheep located in the image?\n",
            "<image>\n",
            "Answer:   The sheep are located in a pen on a farm, standing on a grass-covered field. This pen is situated near a building.\n",
            "Model Predicted Ans: \n",
            "<caption>\n",
            "<source>\n",
            "</source>\n",
            "</caption>\n",
            "</image>\n",
            "\n",
            "##Your task: **Rewrite** the above paragraph into a middle school level textbook section while keeping as many content as possible, using a calm tone.\n",
            "\n",
            "Answer:\n",
            "In the world of sheep, there are many interesting behaviors and interactions that take place. One such behavior is the formation of groups known as \"sheep herds.\" These herds are made up of a group\n"
          ]
        }
      ],
      "source": [
        "# random validation prediction\n",
        "def model_run_val(sample_val_data,max_generate_length=10):\n",
        "\n",
        "    total_val_len = len(sample_val_data)\n",
        "    random_val_datapoint = random.randrange(1,total_val_len) # 0 is header\n",
        "\n",
        "    val_image_url = sample_val_data[random_val_datapoint][1]\n",
        "    val_q = sample_val_data[random_val_datapoint][2]\n",
        "    val_a = sample_val_data[random_val_datapoint][3]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_load = Image.open(requests.get(val_image_url,stream=True).raw)\n",
        "        image_processed = processor(images=image_load, return_tensors=\"pt\").to(device)\n",
        "        clip_val_outputs = clip_model(**image_processed).last_hidden_state[:,1:,:]\n",
        "        val_image_embeds = projection_layer(clip_val_outputs)\n",
        "        val_image_embeds = projection_model(val_image_embeds).to(torch.float16)\n",
        "\n",
        "        img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).to(device)\n",
        "        img_token_embeds = peft_model.model.model.embed_tokens(img_token_tensor).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        val_q_tokenised = tokenizer(val_q, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
        "        val_q_embeds  = peft_model.model.model.embed_tokens(val_q_tokenised).unsqueeze(0)\n",
        "\n",
        "        val_combined_embeds = torch.cat([val_image_embeds, img_token_embeds, val_q_embeds], dim=1) # 1, 69, 2560\n",
        "\n",
        "        predicted_caption = torch.full((1,max_generate_length),50256)\n",
        "\n",
        "        for g in range(max_generate_length):\n",
        "            phi_output_logits = peft_model(inputs_embeds=val_combined_embeds)['logits'] # 4, 69, 51200\n",
        "            predicted_word_token_logits = phi_output_logits[:, -1, :].unsqueeze(1) # 4,1,51200\n",
        "            predicted_word_token   = torch.argmax(predicted_word_token_logits, dim = -1) # 4,1\n",
        "            predicted_caption[:,g] = predicted_word_token.view(1,-1).to('cpu')\n",
        "            next_token_embeds      = phi_model.model.embed_tokens(predicted_word_token) # 4,1,2560\n",
        "            val_combined_embeds    = torch.cat([val_combined_embeds, next_token_embeds], dim=1)\n",
        "\n",
        "        predicted_captions_decoded = tokenizer.batch_decode(predicted_caption,ignore_index = 50256)[0]\n",
        "\n",
        "    print(f\"Image: {val_image_url}\")\n",
        "    print(f\"Question: {val_q}\")\n",
        "    print(f\"Answer:   {val_a}\")\n",
        "    print(f\"Model Predicted Ans: {predicted_captions_decoded}\")\n",
        "\n",
        "model_run_val(sample_val_data,max_generate_length=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1896295f-05f9-4541-bfdd-ea41de8cbb14",
      "metadata": {
        "id": "1896295f-05f9-4541-bfdd-ea41de8cbb14"
      },
      "source": [
        "# Start of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "932d0019-0cb7-4a6a-bc6e-bf7033374bfc",
      "metadata": {
        "id": "932d0019-0cb7-4a6a-bc6e-bf7033374bfc"
      },
      "outputs": [],
      "source": [
        "phi_optimizer        = torch.optim.Adam(peft_model.parameters(), lr=1e-6)\n",
        "projection_layer_optimizer = torch.optim.Adam(projection_layer.parameters(), lr=1e-5)\n",
        "projection_model_optimizer = torch.optim.Adam(projection_model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from finetune_dataset import llavadataset"
      ],
      "metadata": {
        "id": "Uu4ovfjdR8Mx"
      },
      "id": "Uu4ovfjdR8Mx",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "#gc.collect()"
      ],
      "metadata": {
        "id": "lM84EaNcBLDN"
      },
      "id": "lM84EaNcBLDN",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = 0\n",
        "running_loss = 0.\n",
        "projection_layer.train()\n",
        "projection_model.train()\n",
        "peft_model.train()\n",
        "\n",
        "\n",
        "for epoch in range(2):\n",
        "    for batch_idx, (images,questions,answers) in enumerate(train_dataloader):\n",
        "\n",
        "        # process input data\n",
        "        batch_size = questions.size(0)\n",
        "        questions  = questions.to(device)\n",
        "        answers    = answers.to(device)\n",
        "\n",
        "        # clip\n",
        "        images = {'pixel_values': images.to(device)}\n",
        "        clip_outputs  = clip_model(**images)\n",
        "        images_embeds = clip_outputs.last_hidden_state[:,1:,:] # remove cls token\n",
        "\n",
        "        # projection\n",
        "        image_embeds  = projection_layer(images_embeds)\n",
        "        image_embeds = projection_model(image_embeds).to(torch.float16)\n",
        "\n",
        "\n",
        "        # embeds\n",
        "        # print(f\"questions shape{questions.shape},answers shape{answers.shape}\")\n",
        "        img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).repeat(batch_size, 1).to(device)\n",
        "        img_token_embeds = peft_model.model.model.embed_tokens(img_token_tensor)\n",
        "        questions_embed  = peft_model.model.model.embed_tokens(questions)\n",
        "\n",
        "        # forward pass\n",
        "        #print(\"***************\")\n",
        "        # print(image_embeds.shape)\n",
        "        combined_embeds = torch.cat([image_embeds, img_token_embeds, questions_embed], dim=1) # 4, 69, 2560\n",
        "        #print(f\"combined_embeds shape{combined_embeds.shape}\")\n",
        "        phi_output_logits = peft_model(inputs_embeds=combined_embeds)['logits'] # 4, 69, 51200\n",
        "        # print(f\"phi_output_logits shape{phi_output_logits.shape}\")\n",
        "        # print(f\"answers shape {answers.shape}\")\n",
        "\n",
        "        # take out the image embeddings\n",
        "        phi_output_logits = phi_output_logits[:,images_embeds.shape[1] + 1 : ,:]\n",
        "        # print(f\"phi_output_logits after shape{phi_output_logits.shape}\")\n",
        "        phi_output_logits = phi_output_logits.reshape(-1,vocab_size)\n",
        "        # print(f\"phi_output_logits after shape{phi_output_logits.shape}\")\n",
        "        # print(f\"answers after shape {answers.contiguous().view(-1).shape}\")\n",
        "        phi_optimizer.zero_grad()\n",
        "        projection_layer_optimizer.zero_grad()\n",
        "        projection_model_optimizer.zero_grad()\n",
        "        # min_batch_size = min(phi_output_logits.size(0), answers.size(0))\n",
        "        # phi_output_logits = phi_output_logits[:min_batch_size]\n",
        "        # answers = answers.contiguous().view(-1)[:min_batch_size]\n",
        "        loss = F.cross_entropy(phi_output_logits, answers.contiguous().view(-1), ignore_index=EOS_TOKEN_ID,label_smoothing=0.1)\n",
        "\n",
        "        # loss backprop\n",
        "        loss.backward()\n",
        "        phi_optimizer.step()\n",
        "        projection_layer_optimizer.step()\n",
        "        projection_model_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        if step % model_log_step == 0:\n",
        "            print(f\"Iteration {step}/{max_steps}, Loss: {loss.item()}\")\n",
        "\n",
        "        if step % model_val_step == 0:\n",
        "            projection_layer.eval()\n",
        "            projection_model.eval()\n",
        "            peft_model.eval()\n",
        "\n",
        "            model_run_val(sample_val_data,max_generate_length)\n",
        "            projection_layer.train()\n",
        "            projection_model.train()\n",
        "            peft_model.train()\n",
        "\n",
        "\n",
        "        if step % model_save_step == 0:\n",
        "            print(\"Saving Checkpoint\")\n",
        "            torch.save(projection_layer.state_dict(),'/content/drive/MyDrive/capstone/Stage_2/model_chkpt/ft_projection_layer.pth')\n",
        "            torch.save(projection_model.state_dict(),'/content/drive/MyDrive/capstone/Stage_2/model_chkpt/ft_projection_model.pth')\n",
        "            peft_model.save_pretrained('/content/drive/MyDrive/capstone/Stage_2/model_chkpt/qlora_adaptor/', save_adapter=True, save_config=True)\n",
        "\n",
        "        if step >= max_steps:\n",
        "            print(\"Training finished.\")\n",
        "            break\n",
        "\n",
        "        wandb.log({\"step\": step, \"train_loss\": loss.item()})\n",
        "        step += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkrEUDxjqfQY",
        "outputId": "ea204ca2-dbfe-4013-b0db-e09afb5526ef"
      },
      "id": "bkrEUDxjqfQY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0/20000, Loss: 11.935386657714844\n",
            "Image: http://images.cocodataset.org/train2017/000000009322.jpg\n",
            "Question: What is the man wearing?\n",
            "Answer:   The man is wearing a wetsuit, which is common attire for surfers in cooler water temperatures or for added protection and buoyancy.\n",
            "Model Predicted Ans: . A man is riding a surfboard on a.. wave in the ocean.. ocean. beach.. a beach. a man is riding a surfboard on a. wave in the ocean. beach. a man is riding a surfboard on a. wave in the ocean. beach. a man is riding a surfboard on a. wave in the ocean. beach. a man is riding a surfboard on a. wave in the ocean. beach. a man is riding a surf\n",
            "Saving Checkpoint\n",
            "Iteration 100/20000, Loss: 7.962254524230957\n",
            "Saving Checkpoint\n",
            "Iteration 200/20000, Loss: 8.179533004760742\n",
            "Saving Checkpoint\n",
            "Iteration 300/20000, Loss: 7.6707000732421875\n",
            "Saving Checkpoint\n",
            "Iteration 400/20000, Loss: 7.899779319763184\n",
            "Saving Checkpoint\n",
            "Iteration 500/20000, Loss: 7.565967559814453\n",
            "Saving Checkpoint\n",
            "Iteration 600/20000, Loss: 7.068816184997559\n",
            "Saving Checkpoint\n",
            "Iteration 700/20000, Loss: 7.546957969665527\n",
            "Saving Checkpoint\n",
            "Iteration 800/20000, Loss: 7.911109924316406\n",
            "Saving Checkpoint\n",
            "Iteration 900/20000, Loss: 7.233783721923828\n",
            "Saving Checkpoint\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "292c0ae28eb04e3991ef044d9064d8d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f64c5e2a3a2f4cfb82c2a3059051e10c",
              "IPY_MODEL_b0ee97045f6c444aba854b7744d4bfd9",
              "IPY_MODEL_3cca4167c5e84773a60b85aef7517fb5"
            ],
            "layout": "IPY_MODEL_99af683c0ebc43c7adf3ccbf302ba6a3"
          }
        },
        "f64c5e2a3a2f4cfb82c2a3059051e10c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61248160972d4987b9af682c49de2c2f",
            "placeholder": "​",
            "style": "IPY_MODEL_828a4ad5a16644b695d9095dc4b60dd3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b0ee97045f6c444aba854b7744d4bfd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22ccb5b93d64433da2538d238ce253a6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27d86fe9e4e146ec86285ea00064e7c1",
            "value": 2
          }
        },
        "3cca4167c5e84773a60b85aef7517fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5494bd13106445d081008924afe897ec",
            "placeholder": "​",
            "style": "IPY_MODEL_e7b11a6e298a4edfb402a104e5356eb3",
            "value": " 2/2 [00:05&lt;00:00,  2.27s/it]"
          }
        },
        "99af683c0ebc43c7adf3ccbf302ba6a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61248160972d4987b9af682c49de2c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "828a4ad5a16644b695d9095dc4b60dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22ccb5b93d64433da2538d238ce253a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27d86fe9e4e146ec86285ea00064e7c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5494bd13106445d081008924afe897ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7b11a6e298a4edfb402a104e5356eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
