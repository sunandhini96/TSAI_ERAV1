{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-02-12T10:50:54.010232Z","iopub.execute_input":"2024-02-12T10:50:54.011061Z","iopub.status.idle":"2024-02-12T10:50:54.017016Z","shell.execute_reply.started":"2024-02-12T10:50:54.011018Z","shell.execute_reply":"2024-02-12T10:50:54.015901Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Downloading LLAVA Instruct 150 k dataset ","metadata":{}},{"cell_type":"code","source":"! wget https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json","metadata":{"execution":{"iopub.status.busy":"2024-02-12T10:51:31.151000Z","iopub.execute_input":"2024-02-12T10:51:31.151714Z","iopub.status.idle":"2024-02-12T10:51:39.503243Z","shell.execute_reply.started":"2024-02-12T10:51:31.151684Z","shell.execute_reply":"2024-02-12T10:51:39.502100Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2024-02-12 10:51:32--  https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\nResolving huggingface.co (huggingface.co)... 18.239.50.16, 18.239.50.80, 18.239.50.103, ...\nConnecting to huggingface.co (huggingface.co)|18.239.50.16|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.huggingface.co/repos/4d/41/4d41ea1e2709f0e68e9e361e4218192b9620c5a3f2cb8055bc625942b6cd3039/6b68bc5ca2bfd8a71119af0e8454929668ccda6a334955ccc95d114fc8d082fa?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llava_instruct_150k.json%3B+filename%3D%22llava_instruct_150k.json%22%3B&response-content-type=application%2Fjson&Expires=1707994292&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNzk5NDI5Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80ZC80MS80ZDQxZWExZTI3MDlmMGU2OGU5ZTM2MWU0MjE4MTkyYjk2MjBjNWEzZjJjYjgwNTViYzYyNTk0MmI2Y2QzMDM5LzZiNjhiYzVjYTJiZmQ4YTcxMTE5YWYwZTg0NTQ5Mjk2NjhjY2RhNmEzMzQ5NTVjY2M5NWQxMTRmYzhkMDgyZmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=N2VF2jhK9kNYpD-kcVfdIF1-GRfjFeRgkv25HDBTiWca03FSmiQXdoqErg8by71zlLFxn6O-dj8k81uGT6pNq4cnAX4vK6RpkjjrXinQGAB7xyqlG5KIp9saNYWbOWAfKP9hwGvOknfOc42hw2IG1KMv%7Ev6FfFoy4gD8SokaM0vBMXj293od%7E%7EWU8GAkDe20mzi0kBXHBOF375QDqqK1Mzi0hxRTXbpN1CoeTm5aLlJ%7ExiDerRddMyeTKdjB35PmrQ-Og-88Mbon7QzYQCLHNhEOB7NgAZRY5b0VFpqJaFlgvMBVINu7V7pU36Y2sYrrSNROTEH6U-OKNhYf3pGvHA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n--2024-02-12 10:51:32--  https://cdn-lfs.huggingface.co/repos/4d/41/4d41ea1e2709f0e68e9e361e4218192b9620c5a3f2cb8055bc625942b6cd3039/6b68bc5ca2bfd8a71119af0e8454929668ccda6a334955ccc95d114fc8d082fa?response-content-disposition=attachment%3B+filename*%3DUTF-8''llava_instruct_150k.json%3B+filename%3D%22llava_instruct_150k.json%22%3B&response-content-type=application%2Fjson&Expires=1707994292&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNzk5NDI5Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80ZC80MS80ZDQxZWExZTI3MDlmMGU2OGU5ZTM2MWU0MjE4MTkyYjk2MjBjNWEzZjJjYjgwNTViYzYyNTk0MmI2Y2QzMDM5LzZiNjhiYzVjYTJiZmQ4YTcxMTE5YWYwZTg0NTQ5Mjk2NjhjY2RhNmEzMzQ5NTVjY2M5NWQxMTRmYzhkMDgyZmE~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=N2VF2jhK9kNYpD-kcVfdIF1-GRfjFeRgkv25HDBTiWca03FSmiQXdoqErg8by71zlLFxn6O-dj8k81uGT6pNq4cnAX4vK6RpkjjrXinQGAB7xyqlG5KIp9saNYWbOWAfKP9hwGvOknfOc42hw2IG1KMv~v6FfFoy4gD8SokaM0vBMXj293od~~WU8GAkDe20mzi0kBXHBOF375QDqqK1Mzi0hxRTXbpN1CoeTm5aLlJ~xiDerRddMyeTKdjB35PmrQ-Og-88Mbon7QzYQCLHNhEOB7NgAZRY5b0VFpqJaFlgvMBVINu7V7pU36Y2sYrrSNROTEH6U-OKNhYf3pGvHA__&Key-Pair-Id=KVTP0A1DKRTAX\nResolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.239.18.94, 18.239.18.68, 18.239.18.84, ...\nConnecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.239.18.94|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 228941895 (218M) [application/json]\nSaving to: 'llava_instruct_150k.json'\n\nllava_instruct_150k 100%[===================>] 218.34M  33.7MB/s    in 6.7s    \n\n2024-02-12 10:51:39 (32.7 MB/s) - 'llava_instruct_150k.json' saved [228941895/228941895]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n# Opening JSON file - instruct150k\nf = open('llava_instruct_150k.json')\n\n# returns JSON object as\n# a dictionary\ndata = json.load(f)\ndata[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-12T10:52:04.257291Z","iopub.execute_input":"2024-02-12T10:52:04.258253Z","iopub.status.idle":"2024-02-12T10:52:06.722206Z","shell.execute_reply.started":"2024-02-12T10:52:04.258218Z","shell.execute_reply":"2024-02-12T10:52:06.721309Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'id': '000000033471',\n 'image': '000000033471.jpg',\n 'conversations': [{'from': 'human',\n   'value': '<image>\\nWhat are the colors of the bus in the image?'},\n  {'from': 'gpt', 'value': 'The bus in the image is white and red.'},\n  {'from': 'human',\n   'value': 'What feature can be seen on the back of the bus?'},\n  {'from': 'gpt', 'value': 'The back of the bus features an advertisement.'},\n  {'from': 'human',\n   'value': 'Is the bus driving down the street or pulled off to the side?'},\n  {'from': 'gpt',\n   'value': 'The bus is driving down the street, which is crowded with people and other vehicles.'}]}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preparing data ","metadata":{}},{"cell_type":"code","source":"# Data Preparation\ndata_instruct150_flatten = []  # Initialize an empty list to store flattened data\nr = 0  # Counter for printing progress\n\n# Iterate over each item in the 'data' list\nfor a_idx, d in enumerate(data):\n    # Extract image information\n    image = d['image']\n    image_url = 'http://images.cocodataset.org/train2017/' + image\n    \n    # Iterate over conversations\n    conv_iter = iter(d['conversations'])\n    for i in conv_iter:\n        # Get next conversation item (assumed to be GPT response)\n        gpt_ans = next(conv_iter)\n        \n        # Filter out long answers\n        if len(gpt_ans['value']) > 200:\n            continue\n        \n        # Add to flattened list if it's a human-to-GPT conversation\n        if i['from'] == 'human' and gpt_ans['from'] == 'gpt':\n            # Append image URL, human message, and GPT response\n            data_instruct150_flatten.append((image_url, i['value'].replace('<image>\\n', '').replace('\\n<image>', ''), gpt_ans['value']))\n    \n    # Print progress every 10,000 items\n    if a_idx % 10000 == 0:\n        print(f\"{10000 * r} processed\")\n        r += 1\n","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:09:06.374359Z","iopub.execute_input":"2024-02-12T11:09:06.374810Z","iopub.status.idle":"2024-02-12T11:09:06.920873Z","shell.execute_reply.started":"2024-02-12T11:09:06.374775Z","shell.execute_reply":"2024-02-12T11:09:06.919917Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"0 processed\n10000 processed\n20000 processed\n30000 processed\n40000 processed\n50000 processed\n60000 processed\n70000 processed\n80000 processed\n90000 processed\n100000 processed\n110000 processed\n120000 processed\n130000 processed\n140000 processed\n150000 processed\n","output_type":"stream"}]},{"cell_type":"code","source":"data_instruct150_flatten[4]","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:09:18.623049Z","iopub.execute_input":"2024-02-12T11:09:18.623414Z","iopub.status.idle":"2024-02-12T11:09:18.629632Z","shell.execute_reply.started":"2024-02-12T11:09:18.623384Z","shell.execute_reply":"2024-02-12T11:09:18.628683Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('http://images.cocodataset.org/train2017/000000052846.jpg',\n 'What is the cat doing in the image?',\n 'The cat is coming out from some curtains onto the couch and is sitting or standing on top of it.')"},"metadata":{}}]},{"cell_type":"code","source":"# add tokens\nphi_model_name  = \"microsoft/phi-2\"\ntokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\ntokenizer.add_tokens('[QA]')\ntokenizer.add_special_tokens({'pad_token':'[PAD]'}) \ntokenizer.pad_token, tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:09:21.975968Z","iopub.execute_input":"2024-02-12T11:09:21.976346Z","iopub.status.idle":"2024-02-12T11:09:22.215385Z","shell.execute_reply.started":"2024-02-12T11:09:21.976317Z","shell.execute_reply":"2024-02-12T11:09:22.214428Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('[PAD]', '<|endoftext|>')"},"metadata":{}}]},{"cell_type":"code","source":"import csv\n# gpt like training dataset\nwith open('train_token.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerows([['img_url','input','label']])\n    \ntrain_data_temp = []\nr = 1\nfor df in data_instruct150_flatten:\n  image_url = df[0]\n  image_q   = df[1] + ' [QA]'\n  image_a   = df[2] +  tokenizer.eos_token\n  #print(image_q)\n  #print(image_a)\n  \n  # tokenise \n  ques_token = tokenizer(image_q, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n  ans_token  = tokenizer(image_a, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n\n  #print(ques_token)\n  #print(ans_token)\n  #break\n\n  context_length = len(ques_token)\n  combo_q_a = torch.cat( [ques_token,ans_token])\n    \n  for al in range(len(ans_token)):   \n    input = combo_q_a[al : al + context_length].numpy()\n    label = combo_q_a[al + 1 : al + context_length + 1].numpy()\n    train_data_temp.append([image_url,input,label])\n    if len(train_data_temp) >= 100000: # write to the file\n       print(f\"Writing to disk after {r * 100000} rows\")\n       r += 1\n       with open('train_token.csv', 'a', newline='') as file:\n          writer = csv.writer(file)\n          writer.writerows(train_data_temp)\n       train_data_temp = []","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:25:08.709900Z","iopub.execute_input":"2024-02-12T11:25:08.710543Z","iopub.status.idle":"2024-02-12T11:37:55.376736Z","shell.execute_reply.started":"2024-02-12T11:25:08.710493Z","shell.execute_reply":"2024-02-12T11:37:55.375924Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Writing to disk after 100000 rows\nWriting to disk after 200000 rows\nWriting to disk after 300000 rows\nWriting to disk after 400000 rows\nWriting to disk after 500000 rows\nWriting to disk after 600000 rows\nWriting to disk after 700000 rows\nWriting to disk after 800000 rows\nWriting to disk after 900000 rows\nWriting to disk after 1000000 rows\nWriting to disk after 1100000 rows\nWriting to disk after 1200000 rows\nWriting to disk after 1300000 rows\nWriting to disk after 1400000 rows\nWriting to disk after 1500000 rows\nWriting to disk after 1600000 rows\nWriting to disk after 1700000 rows\nWriting to disk after 1800000 rows\nWriting to disk after 1900000 rows\nWriting to disk after 2000000 rows\nWriting to disk after 2100000 rows\nWriting to disk after 2200000 rows\nWriting to disk after 2300000 rows\nWriting to disk after 2400000 rows\nWriting to disk after 2500000 rows\nWriting to disk after 2600000 rows\nWriting to disk after 2700000 rows\nWriting to disk after 2800000 rows\nWriting to disk after 2900000 rows\nWriting to disk after 3000000 rows\nWriting to disk after 3100000 rows\nWriting to disk after 3200000 rows\nWriting to disk after 3300000 rows\nWriting to disk after 3400000 rows\nWriting to disk after 3500000 rows\nWriting to disk after 3600000 rows\nWriting to disk after 3700000 rows\nWriting to disk after 3800000 rows\nWriting to disk after 3900000 rows\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data_temp[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:38:16.333745Z","iopub.execute_input":"2024-02-12T11:38:16.334115Z","iopub.status.idle":"2024-02-12T11:38:16.340114Z","shell.execute_reply.started":"2024-02-12T11:38:16.334087Z","shell.execute_reply":"2024-02-12T11:38:16.339436Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['http://images.cocodataset.org/train2017/000000431378.jpg',\n array([19916,   448,    11,   810,   339,   318,  9361,   284,  5529,\n          465,  5236,   290]),\n array([ 448,   11,  810,  339,  318, 9361,  284, 5529,  465, 5236,  290,\n        6594])]"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom torch.utils.data import random_split, DataLoader\nfrom transformers import AutoProcessor, AutoTokenizer\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:38:33.720253Z","iopub.execute_input":"2024-02-12T11:38:33.720943Z","iopub.status.idle":"2024-02-12T11:38:33.725682Z","shell.execute_reply.started":"2024-02-12T11:38:33.720909Z","shell.execute_reply":"2024-02-12T11:38:33.724787Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Training Data:","metadata":{}},{"cell_type":"code","source":"df_data = pd.read_csv('train_token.csv')\ndf_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:38:38.593876Z","iopub.execute_input":"2024-02-12T11:38:38.594800Z","iopub.status.idle":"2024-02-12T11:38:51.055544Z","shell.execute_reply.started":"2024-02-12T11:38:38.594767Z","shell.execute_reply":"2024-02-12T11:38:51.054650Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                             img_url  \\\n0  http://images.cocodataset.org/train2017/000000...   \n1  http://images.cocodataset.org/train2017/000000...   \n2  http://images.cocodataset.org/train2017/000000...   \n3  http://images.cocodataset.org/train2017/000000...   \n4  http://images.cocodataset.org/train2017/000000...   \n5  http://images.cocodataset.org/train2017/000000...   \n6  http://images.cocodataset.org/train2017/000000...   \n7  http://images.cocodataset.org/train2017/000000...   \n8  http://images.cocodataset.org/train2017/000000...   \n9  http://images.cocodataset.org/train2017/000000...   \n\n                                               input  \\\n0  [ 2061   389   262  7577   286   262  1323   2...   \n1  [  389   262  7577   286   262  1323   287   2...   \n2  [  262  7577   286   262  1323   287   262  29...   \n3  [ 7577   286   262  1323   287   262  2939    ...   \n4  [  286   262  1323   287   262  2939    30   2...   \n5  [  262  1323   287   262  2939    30   220 502...   \n6  [ 1323   287   262  2939    30   220 50295   4...   \n7  [  287   262  2939    30   220 50295   464  13...   \n8  [  262  2939    30   220 50295   464  1323   2...   \n9  [ 2939    30   220 50295   464  1323   287   2...   \n\n                                               label  \n0  [  389   262  7577   286   262  1323   287   2...  \n1  [  262  7577   286   262  1323   287   262  29...  \n2  [ 7577   286   262  1323   287   262  2939    ...  \n3  [  286   262  1323   287   262  2939    30   2...  \n4  [  262  1323   287   262  2939    30   220 502...  \n5  [ 1323   287   262  2939    30   220 50295   4...  \n6  [  287   262  2939    30   220 50295   464  13...  \n7  [  262  2939    30   220 50295   464  1323   2...  \n8  [ 2939    30   220 50295   464  1323   287   2...  \n9  [   30   220 50295   464  1323   287   262  29...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img_url</th>\n      <th>input</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>http://images.cocodataset.org/train2017/000000...</td>\n      <td>[ 2061   389   262  7577   286   262  1323   2...</td>\n      <td>[  389   262  7577   286   262  1323   287   2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://images.cocodataset.org/train2017/000000...</td>\n      <td>[  389   262  7577   286   262  1323   287   2...</td>\n      <td>[  262  7577   286   262  1323   287   262  29...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>http://images.cocodataset.org/train2017/000000...</td>\n      <td>[  262  7577   286   262  1323   287   262  29...</td>\n      <td>[ 7577   286   262  1323   287   262  2939    ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>http://images.cocodataset.org/train2017/000000...</td>\n      <td>[ 7577   286   262  1323   287   262  2939    ...</td>\n      <td>[  286   262  1323   287   262  2939    30   2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>http://images.cocodataset.org/train2017/000000...</td>\n      <td>[  286   262  1323   287   262  2939    30   2...</td>\n      <td>[  262  1323   287   262  2939    30   220 502...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>http://images.cocodataset.org/train2017/000000...</td>\n      <td>[  262  1323   287   262  2939    30   220 502...</td>\n      <td>[ 1323   287   262  2939    30   220 50295   4...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>http://images.cocodataset.org/train2017/000000...</td>\n      <td>[ 1323   287   262  2939    30   220 50295   4...</td>\n      <td>[  287   262  2939    30   220 50295   464  13...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>http://images.cocodataset.org/train2017/000000...</td>\n      <td>[  287   262  2939    30   220 50295   464  13...</td>\n      <td>[  262  2939    30   220 50295   464  1323   2...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>http://images.cocodataset.org/train2017/000000...</td>\n      <td>[  262  2939    30   220 50295   464  1323   2...</td>\n      <td>[ 2939    30   220 50295   464  1323   287   2...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>http://images.cocodataset.org/train2017/000000...</td>\n      <td>[ 2939    30   220 50295   464  1323   287   2...</td>\n      <td>[   30   220 50295   464  1323   287   262  29...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"for i in (df_data[0:1]['input']):\n    print(i)\nfor i in (df_data[0:1]['label']):\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:39:52.406981Z","iopub.execute_input":"2024-02-12T11:39:52.408063Z","iopub.status.idle":"2024-02-12T11:39:52.415846Z","shell.execute_reply.started":"2024-02-12T11:39:52.408018Z","shell.execute_reply":"2024-02-12T11:39:52.414829Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[ 2061   389   262  7577   286   262  1323   287   262  2939    30   220\n 50295]\n[  389   262  7577   286   262  1323   287   262  2939    30   220 50295\n   464]\n","output_type":"stream"}]},{"cell_type":"code","source":"class llavadataset(Dataset):\n  def __init__(self, qa_dataset, phi_model_name, clip_model_name, tokenizer):\n    self.processor  = AutoProcessor.from_pretrained(clip_model_name)\n    self.qa_dataset = qa_dataset\n\n  def __len__(self):\n    return self.qa_dataset.shape[0]\n\n  def __getitem__(self, idx):\n    # from image perspective\n    img_url = self.qa_dataset.img_url[idx]\n    ques    = torch.tensor(np.array(np.matrix(self.qa_dataset.input[idx]))[0])  \n    ans     = torch.tensor(np.array(np.matrix(self.qa_dataset.label[idx]))[0])\n    \n    # image load\n    image_load = Image.open(requests.get(img_url,stream=True).raw)\n    image_processed = self.processor(images=image_load, return_tensors=\"pt\") ['pixel_values']\n    image_processed = image_processed.squeeze(0)\n    # q = self.tokenizer(ques, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n    # a = self.tokenizer(ans, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n    return(image_processed , ques, ans)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:40:02.149610Z","iopub.execute_input":"2024-02-12T11:40:02.149976Z","iopub.status.idle":"2024-02-12T11:40:02.158392Z","shell.execute_reply.started":"2024-02-12T11:40:02.149948Z","shell.execute_reply":"2024-02-12T11:40:02.157459Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"clip_model_name = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n#phi_model_name  = \"microsoft/phi-2\"\n#tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\ncsv_file = 'train_token.csv'\nqa_dataset = pd.read_csv(csv_file)\nstep2_dataset = llavadataset(qa_dataset, phi_model_name, clip_model_name, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:42:47.677196Z","iopub.execute_input":"2024-02-12T11:42:47.678177Z","iopub.status.idle":"2024-02-12T11:43:02.831680Z","shell.execute_reply.started":"2024-02-12T11:42:47.678141Z","shell.execute_reply":"2024-02-12T11:43:02.830473Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea6451856d5140b78b36263abcace3f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ddec22ac08b40728af30757deb14b26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c30bead0a1dd41a095ac255835bfa763"}},"metadata":{}}]},{"cell_type":"code","source":"def collate_fn(batch):\n    image_embeddings, ques, ans = zip(*batch)\n    image_embeddings_stacked = torch.stack(image_embeddings, dim=0)\n    ques_padded = torch.nn.utils.rnn.pad_sequence(ques, batch_first=True, padding_value=tokenizer.pad_token_id)\n    ans_padded = torch.nn.utils.rnn.pad_sequence(ans, batch_first=True, padding_value=tokenizer.pad_token_id)\n    return (image_embeddings_stacked, ques_padded,ans_padded)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:43:04.898409Z","iopub.execute_input":"2024-02-12T11:43:04.898775Z","iopub.status.idle":"2024-02-12T11:43:04.904781Z","shell.execute_reply.started":"2024-02-12T11:43:04.898748Z","shell.execute_reply":"2024-02-12T11:43:04.903699Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"tokenizer.pad_token_id,tokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:43:10.378994Z","iopub.execute_input":"2024-02-12T11:43:10.379646Z","iopub.status.idle":"2024-02-12T11:43:10.386083Z","shell.execute_reply.started":"2024-02-12T11:43:10.379610Z","shell.execute_reply":"2024-02-12T11:43:10.385065Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(50296, 50256)"},"metadata":{}}]},{"cell_type":"code","source":"# Get the token value for token ID 50295\ntoken_value = tokenizer.convert_ids_to_tokens(50295)\ntoken_value","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:46:39.920881Z","iopub.execute_input":"2024-02-12T11:46:39.921728Z","iopub.status.idle":"2024-02-12T11:46:39.927430Z","shell.execute_reply.started":"2024-02-12T11:46:39.921695Z","shell.execute_reply":"2024-02-12T11:46:39.926551Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'[QA]'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Validation data loader","metadata":{}},{"cell_type":"code","source":"val_dataloader   = DataLoader(llavadataset(qa_dataset, phi_model_name,clip_model_name,tokenizer),\n                      collate_fn=collate_fn, batch_size=2, num_workers = 10, shuffle=True, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:48:40.945002Z","iopub.execute_input":"2024-02-12T11:48:40.945397Z","iopub.status.idle":"2024-02-12T11:48:42.459915Z","shell.execute_reply.started":"2024-02-12T11:48:40.945369Z","shell.execute_reply":"2024-02-12T11:48:42.458752Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"import random","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:49:04.352799Z","iopub.execute_input":"2024-02-12T11:49:04.353468Z","iopub.status.idle":"2024-02-12T11:49:04.357665Z","shell.execute_reply.started":"2024-02-12T11:49:04.353438Z","shell.execute_reply":"2024-02-12T11:49:04.356607Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#! wget https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\n# Opening JSON file - instruct150k\nf = open('llava_instruct_150k.json')\n\n# returns JSON object as\n# a dictionary\ndata = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:49:13.162478Z","iopub.execute_input":"2024-02-12T11:49:13.163348Z","iopub.status.idle":"2024-02-12T11:49:15.501807Z","shell.execute_reply.started":"2024-02-12T11:49:13.163319Z","shell.execute_reply":"2024-02-12T11:49:15.501019Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# create input pickle file by flattening the data\ndata_instruct150_sample_val_flatten = []\nr = 0\n\nfor a_idx,d in enumerate(data):\n    image = d['image']\n    image_url = 'http://images.cocodataset.org/train2017/' + image\n    conv_iter = iter( d['conversations'])\n    for i in conv_iter:\n      gpt_ans = next(conv_iter)\n      if len(gpt_ans['value']) > 200: # filter out too long answers\n          continue\n      if i['from'] == 'human' and gpt_ans['from'] == 'gpt':\n        image_q   = i['value'].replace('<image>\\n','').replace('\\n<image>','') + ' [QA]'\n        image_a   = gpt_ans['value'] +  tokenizer.eos_token\n        data_instruct150_sample_val_flatten.append([image_url, image_q, image_a ])\n\n    if a_idx % 10000 == 0:\n      print(f\"{10000 * r} processed\")\n      r += 1\n      if r >= 2:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:50:23.780660Z","iopub.execute_input":"2024-02-12T11:50:23.781069Z","iopub.status.idle":"2024-02-12T11:50:23.906789Z","shell.execute_reply.started":"2024-02-12T11:50:23.781039Z","shell.execute_reply":"2024-02-12T11:50:23.905870Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"0 processed\n10000 processed\n","output_type":"stream"}]},{"cell_type":"code","source":"data_instruct150_sample_val_flatten[3]","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:50:27.603787Z","iopub.execute_input":"2024-02-12T11:50:27.604441Z","iopub.status.idle":"2024-02-12T11:50:27.610374Z","shell.execute_reply.started":"2024-02-12T11:50:27.604408Z","shell.execute_reply":"2024-02-12T11:50:27.609454Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"['http://images.cocodataset.org/train2017/000000052846.jpg',\n 'Where is the cat positioned in the image? [QA]',\n 'The cat is positioned on top of the back of the couch in the living room.<|endoftext|>']"},"metadata":{}}]},{"cell_type":"code","source":"# header\nwith open('sample_val_data.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerows([['img_url','q','a']])\n\n# data\nwith open('sample_val_data.csv', 'a', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerows(data_instruct150_sample_val_flatten)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T11:50:34.518946Z","iopub.execute_input":"2024-02-12T11:50:34.519951Z","iopub.status.idle":"2024-02-12T11:50:34.833364Z","shell.execute_reply.started":"2024-02-12T11:50:34.519904Z","shell.execute_reply":"2024-02-12T11:50:34.832356Z"},"trusted":true},"execution_count":37,"outputs":[]}]}