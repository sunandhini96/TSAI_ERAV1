{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunandhini96/TSAI_ERAV1/blob/main/Capstone/Stage_2/stage2_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZthZ1qOa_lo",
        "outputId": "3ddc9d60-a368-48e0-c5bd-5317e11eba86"
      },
      "id": "VZthZ1qOa_lo",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Requirements"
      ],
      "metadata": {
        "id": "fAkaEIE3uHWp"
      },
      "id": "fAkaEIE3uHWp"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sunandhini96/TSAI_ERAV1.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es1nIBBz510b",
        "outputId": "280acf38-e74a-42a6-9b7d-a80ae215d098"
      },
      "id": "es1nIBBz510b",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TSAI_ERAV1'...\n",
            "remote: Enumerating objects: 1442, done.\u001b[K\n",
            "remote: Counting objects: 100% (713/713), done.\u001b[K\n",
            "remote: Compressing objects: 100% (389/389), done.\u001b[K\n",
            "remote: Total 1442 (delta 432), reused 516 (delta 316), pack-reused 729\u001b[K\n",
            "Receiving objects: 100% (1442/1442), 31.11 MiB | 14.98 MiB/s, done.\n",
            "Resolving deltas: 100% (677/677), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/capstone/Stage_2/TSAI_ERAV1/Capstone/Stage_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RJWsDe0586l",
        "outputId": "0a9699bc-1b4c-410c-eef5-cced8dfc833d"
      },
      "id": "5RJWsDe0586l",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/capstone/Stage_2/TSAI_ERAV1/Capstone/Stage_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T8leZCyBO7b",
        "outputId": "5782ceed-c7b8-4402-d04b-5bd0308c48e5"
      },
      "id": "6T8leZCyBO7b",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1d11cb27-d931-4ac1-a1fc-3e79fa5c1358",
      "metadata": {
        "id": "1d11cb27-d931-4ac1-a1fc-3e79fa5c1358"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import peft\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import AutoTokenizer,BitsAndBytesConfig, AutoModelForCausalLM, CLIPVisionModel, AutoProcessor\n",
        "import torch\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import pandas as pd\n",
        "from torch.nn import functional as F\n",
        "import csv\n",
        "import random\n",
        "from PIL import Image\n",
        "import requests\n",
        "import wandb\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from finetune_dataset import llavadataset, collate_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define values for parameters used in code"
      ],
      "metadata": {
        "id": "Dv-VWlgluLs9"
      },
      "id": "Dv-VWlgluLs9"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ba1d568a-3fe5-4130-a504-9f8861920981",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba1d568a-3fe5-4130-a504-9f8861920981",
        "outputId": "a1bfe78e-7b87-4281-902a-660e8cfb0bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "clip_model_name = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
        "phi_model_name  = \"microsoft/phi-2\"\n",
        "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
        "processor  = AutoProcessor.from_pretrained(clip_model_name)\n",
        "tokenizer.add_tokens('[QA]')\n",
        "tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n",
        "\n",
        "train_batch_size = 4\n",
        "clip_embed = 640\n",
        "phi_embed  = 2560\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_workers = 10\n",
        "IMAGE_TOKEN_ID = 23893 # token for word comment\n",
        "max_steps      = 20000\n",
        "EOS_TOKEN_ID   = 50256\n",
        "phi_patches    = 49\n",
        "vocab_size     = 51200\n",
        "max_generate_length = 100\n",
        "model_val_step      = 1000\n",
        "model_log_step      = 100\n",
        "model_save_step     = 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Automatically chooses the precision for GPU"
      ],
      "metadata": {
        "id": "ctOYBWJRuzUz"
      },
      "id": "ctOYBWJRuzUz"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.amp.autocast(enabled=True)\n",
        "torch.set_float32_matmul_precision('medium')"
      ],
      "metadata": {
        "id": "fEj2T6u9bkAx"
      },
      "id": "fEj2T6u9bkAx",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logging in Wandb"
      ],
      "metadata": {
        "id": "tXbl9yRGu4aZ"
      },
      "id": "tXbl9yRGu4aZ"
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project  = \"tsai_multimodal_gpt_project\", name=\"step2_finetuning_QLoRA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229,
          "referenced_widgets": [
            "009e96c3c4764873b9883395677e25f9",
            "92bd9d8f3dd242f6ac18eba196143a5a",
            "65ced50cdf94444f9477a623ca19e144",
            "b60ae49d2f7949c6a4b3fa1fafac5b4b",
            "50f4cd110b1249419057f090f2490337",
            "8ff3a030473e40009be749f4928a5782",
            "946a219a834a434195efb56606714b7b",
            "a2270bd0004d4697be6415289db6559a"
          ]
        },
        "id": "BZpbvmmIa_1i",
        "outputId": "624fb334-3530-44fc-937f-2694bea74b9f"
      },
      "id": "BZpbvmmIa_1i",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:yvuw6ikn) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "009e96c3c4764873b9883395677e25f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">step2_finetuning_QLoRA</strong> at: <a href='https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project/runs/yvuw6ikn' target=\"_blank\">https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project/runs/yvuw6ikn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240212_182107-yvuw6ikn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:yvuw6ikn). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/capstone/Stage_2/TSAI_ERAV1/Capstone/Stage_2/wandb/run-20240212_182442-gh9xkj9m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project/runs/gh9xkj9m' target=\"_blank\">step2_finetuning_QLoRA</a></strong> to <a href='https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project' target=\"_blank\">https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project/runs/gh9xkj9m' target=\"_blank\">https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project/runs/gh9xkj9m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/gsunandhini/tsai_multimodal_gpt_project/runs/gh9xkj9m?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78cac2a33c70>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8692e0ef-165c-40c5-a6a0-a601d1c5099c",
      "metadata": {
        "id": "8692e0ef-165c-40c5-a6a0-a601d1c5099c"
      },
      "source": [
        "# Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5844cdb1-2a30-4d65-a129-606d54e2b8e7",
      "metadata": {
        "id": "5844cdb1-2a30-4d65-a129-606d54e2b8e7"
      },
      "outputs": [],
      "source": [
        "# training data\n",
        "csv_file = '/content/drive/MyDrive/capstone/Stage_2/train_token.csv'\n",
        "qa_dataset = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Gndwzm8H7fdL",
        "outputId": "893086a2-7f2e-46c2-b1ec-a92bc4092524"
      },
      "id": "Gndwzm8H7fdL",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             img_url  \\\n",
              "0  http://images.cocodataset.org/train2017/000000...   \n",
              "1  http://images.cocodataset.org/train2017/000000...   \n",
              "2  http://images.cocodataset.org/train2017/000000...   \n",
              "3  http://images.cocodataset.org/train2017/000000...   \n",
              "4  http://images.cocodataset.org/train2017/000000...   \n",
              "\n",
              "                                               input  \\\n",
              "0  [ 2061   389   262  7577   286   262  1323   2...   \n",
              "1  [  389   262  7577   286   262  1323   287   2...   \n",
              "2  [  262  7577   286   262  1323   287   262  29...   \n",
              "3  [ 7577   286   262  1323   287   262  2939    ...   \n",
              "4  [  286   262  1323   287   262  2939    30   2...   \n",
              "\n",
              "                                               label  \n",
              "0  [  389   262  7577   286   262  1323   287   2...  \n",
              "1  [  262  7577   286   262  1323   287   262  29...  \n",
              "2  [ 7577   286   262  1323   287   262  2939    ...  \n",
              "3  [  286   262  1323   287   262  2939    30   2...  \n",
              "4  [  262  1323   287   262  2939    30   220 502...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6d415c1-4021-44a9-980c-9604e701007c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_url</th>\n",
              "      <th>input</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
              "      <td>[ 2061   389   262  7577   286   262  1323   2...</td>\n",
              "      <td>[  389   262  7577   286   262  1323   287   2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
              "      <td>[  389   262  7577   286   262  1323   287   2...</td>\n",
              "      <td>[  262  7577   286   262  1323   287   262  29...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
              "      <td>[  262  7577   286   262  1323   287   262  29...</td>\n",
              "      <td>[ 7577   286   262  1323   287   262  2939    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
              "      <td>[ 7577   286   262  1323   287   262  2939    ...</td>\n",
              "      <td>[  286   262  1323   287   262  2939    30   2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
              "      <td>[  286   262  1323   287   262  2939    30   2...</td>\n",
              "      <td>[  262  1323   287   262  2939    30   220 502...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6d415c1-4021-44a9-980c-9604e701007c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d6d415c1-4021-44a9-980c-9604e701007c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d6d415c1-4021-44a9-980c-9604e701007c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7ca42dac-a932-4bfc-9194-5b1c42c62bde\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ca42dac-a932-4bfc-9194-5b1c42c62bde')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7ca42dac-a932-4bfc-9194-5b1c42c62bde button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "class llavadataset(Dataset):\n",
        "  def __init__(self, qa_dataset, phi_model_name, clip_model_name, tokenizer,processor):\n",
        "    self.processor  = processor\n",
        "    self.qa_dataset = qa_dataset\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.qa_dataset.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # from image perspective\n",
        "    img_url = self.qa_dataset.img_url[idx]\n",
        "    ques    = torch.tensor(np.array(np.matrix(self.qa_dataset.input[idx]))[0])\n",
        "    ans     = torch.tensor(np.array(np.matrix(self.qa_dataset.label[idx]))[0])\n",
        "\n",
        "    # image load\n",
        "    image_load = Image.open(requests.get(img_url,stream=True).raw)\n",
        "    image_processed = self.processor(images=image_load, return_tensors=\"pt\") ['pixel_values']\n",
        "    image_processed = image_processed.squeeze(0)\n",
        "    # q = self.tokenizer(ques, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
        "    # a = self.tokenizer(ans, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
        "    return(image_processed , ques, ans)\n"
      ],
      "metadata": {
        "id": "3-eDHXXokk7g"
      },
      "id": "3-eDHXXokk7g",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(llavadataset(qa_dataset, phi_model_name,clip_model_name,tokenizer,processor),\n",
        "                  collate_fn=collate_fn, batch_size=train_batch_size, num_workers = num_workers, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "cd-d-V6sAUhH"
      },
      "id": "cd-d-V6sAUhH",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6DUOcPLkkAR"
      },
      "id": "Y6DUOcPLkkAR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1bccc4c4-55d7-46db-aba1-5871bf88c840",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bccc4c4-55d7-46db-aba1-5871bf88c840",
        "outputId": "f27533ae-68b3-4dea-ab4a-34b515be289b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['http://images.cocodataset.org/train2017/000000033471.jpg', 'What are the colors of the bus in the image? [QA]', 'The bus in the image is white and red.<|endoftext|>']\n"
          ]
        }
      ],
      "source": [
        "file = open('/content/drive/MyDrive/capstone/Stage_2/sample_val_data.csv')\n",
        "csvreader = csv.reader(file)\n",
        "sample_val_data = []\n",
        "for row in csvreader:\n",
        "    sample_val_data.append(row)\n",
        "print(sample_val_data[1])\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f57a66-8827-47b9-b006-d409d6ce4cc2",
      "metadata": {
        "id": "59f57a66-8827-47b9-b006-d409d6ce4cc2"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ef46a021-3811-4297-9604-b4d4b181cc5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "c9d48a4a699840f18e5239afb90b3a07",
            "ce49153395884f6f9965ca6dfd39c94e",
            "c7d57cd0a3634f0cbf3eb24f814ce268",
            "210a6f3dccbc499282185cc5d973d198",
            "85cea7da6332451ab13a717987423804",
            "dcbc56d683af473896f43c03cfc0e788",
            "0b5400504c09420ba5e2d1d10f0b89e5",
            "a850b0dad02c490286e76b28789c1093",
            "885486af1a14450db900ccb33dd8b178",
            "3b4f98fe0bb545a2bb623a1b1aa0ca04",
            "347df8ea6c20482ba3c095c640dd52b3"
          ]
        },
        "id": "ef46a021-3811-4297-9604-b4d4b181cc5a",
        "outputId": "4b60a874-91a1-4f2e-a3b7-2732193ca90e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9d48a4a699840f18e5239afb90b3a07"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "clip_model = CLIPVisionModel.from_pretrained(clip_model_name).to(device)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,)\n",
        "\n",
        "phi_model = AutoModelForCausalLM.from_pretrained(\n",
        "    phi_model_name,\n",
        "    torch_dtype=torch.float32,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "phi_model.config.use_cache = False\n",
        "projection_layer = torch.nn.Linear(clip_embed, phi_embed).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(phi_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iova7towCRf",
        "outputId": "0a4cdcb3-b6ea-418d-eeaa-8af3ab3b0dfc"
      },
      "id": "4iova7towCRf",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PhiForCausalLM(\n",
            "  (model): PhiModel(\n",
            "    (embed_tokens): Embedding(51200, 2560)\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x PhiDecoderLayer(\n",
            "        (self_attn): PhiAttention(\n",
            "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (rotary_emb): PhiRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): PhiMLP(\n",
            "          (activation_fn): NewGELUActivation()\n",
            "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
            "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
            "        )\n",
            "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d9e720ab-4f63-487d-ad9b-97ab0dd2300b",
      "metadata": {
        "id": "d9e720ab-4f63-487d-ad9b-97ab0dd2300b",
        "outputId": "2476b386-9e84-49be-aabd-03cfdeef01bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 94,371,840 || all params: 2,874,055,680 || trainable%: 3.2835773035545364\n"
          ]
        }
      ],
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"dense\",\n",
        "        \"fc1\",\n",
        "        \"fc2\",\n",
        "    ]\n",
        ")\n",
        "peft_model = peft.get_peft_model(phi_model, peft_config).to(device)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleResBlock(nn.Module):\n",
        "    def __init__(self, phi_embed):\n",
        "        super().__init__()\n",
        "        self.pre_norm = nn.LayerNorm(phi_embed)\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(phi_embed, phi_embed),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(phi_embed, phi_embed)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.pre_norm(x)\n",
        "        return x + self.proj(x)"
      ],
      "metadata": {
        "id": "zF8iTxkV8j42"
      },
      "id": "zF8iTxkV8j42",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projection_model = SimpleResBlock(phi_embed).to(device)"
      ],
      "metadata": {
        "id": "9sD46TQh8lLp"
      },
      "id": "9sD46TQh8lLp",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OGtncWj81Sj"
      },
      "id": "2OGtncWj81Sj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "e219df78-1e2b-4730-81d6-5a7f794d8e86",
      "metadata": {
        "id": "e219df78-1e2b-4730-81d6-5a7f794d8e86"
      },
      "outputs": [],
      "source": [
        "# clip non trainable\n",
        "for network in [clip_model]:\n",
        "    for param in network.parameters():\n",
        "        param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "9885fcad-2060-42c0-b2e9-e6b9469fde7f",
      "metadata": {
        "id": "9885fcad-2060-42c0-b2e9-e6b9469fde7f",
        "outputId": "54b30216-abed-4699-970f-b50125f85ae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peft_model :94371840\n",
            "projection layer :1640960\n",
            "projection model :13117440\n",
            "clip_model :0\n",
            "phi_model :94371840\n"
          ]
        }
      ],
      "source": [
        "# check trainable paramaeters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"peft_model :{count_parameters(peft_model)}\")\n",
        "print(f\"projection layer :{count_parameters(projection_layer)}\")\n",
        "print(f\"projection model :{count_parameters(projection_model)}\")\n",
        "print(f\"clip_model :{count_parameters(clip_model)}\")\n",
        "print(f\"phi_model :{count_parameters(phi_model)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load checkpoints"
      ],
      "metadata": {
        "id": "8pcpkQOrvYZE"
      },
      "id": "8pcpkQOrvYZE"
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir /content/drive/MyDrive/capstone/Stage_2/model_chkpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ALtgD169_PH",
        "outputId": "3af7c81d-e164-478b-8ebd-8fed7c3bd73d"
      },
      "id": "7ALtgD169_PH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/capstone/Stage_2/model_chkpt’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "fad7fd86-4d63-41f6-a23e-81501fa08ac9",
      "metadata": {
        "id": "fad7fd86-4d63-41f6-a23e-81501fa08ac9",
        "outputId": "16e965ec-a074-4d89-eca7-eccfb19144b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded step1 checkpoint\n"
          ]
        }
      ],
      "source": [
        "if os.path.isfile('/content/drive/MyDrive/capstone/Stage_2/model_chkpt/step2_projection.pth'):\n",
        "    projection_layer.load_state_dict(torch.load('/content/drive/MyDrive/capstone/Stage_2/model_chkpt/ft_projection_layer.pth'))\n",
        "    projection_model.load_state_dict(torch.load('/content/drive/MyDrive/capstone/Stage_2/model_chkpt/ft_projection_model.pth'))\n",
        "    peft_model.from_pretrained(phi_model,'/content/drive/MyDrive/capstone/Stage_2/model_chkpt/qlora_adaptor')\n",
        "    print(\"Loaded step2 checkpoint\")\n",
        "\n",
        "else:\n",
        "    projection_layer.load_state_dict(torch.load('/content/drive/MyDrive/capstone/model_chkpt/clipphi_proj.pth'))\n",
        "    projection_model.load_state_dict(torch.load('/content/drive/MyDrive/capstone/model_chkpt/clipphi_resblock.pth'))\n",
        "    print(\"Loaded step1 checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "1ce5193f-7251-41ed-b2b6-80b6ee613544",
      "metadata": {
        "id": "1ce5193f-7251-41ed-b2b6-80b6ee613544",
        "outputId": "cb18000a-ef7e-4003-a5e7-dee47f5ad44c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['http://images.cocodataset.org/train2017/000000039446.jpg',\n",
              " 'Where is the man sitting while eating pizza? [QA]',\n",
              " 'The man is sitting at a computer desk while eating pizza.<|endoftext|>']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "sample_val_data[200]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "745aeb44-44fe-4579-8d84-29e33e8a660f",
      "metadata": {
        "id": "745aeb44-44fe-4579-8d84-29e33e8a660f"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "f950cd90-fb80-4991-9b9d-5855bded45b6",
      "metadata": {
        "id": "f950cd90-fb80-4991-9b9d-5855bded45b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d39a9cbf-b614-404b-ad28-e3b3275a4d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: http://images.cocodataset.org/train2017/000000579807.jpg\n",
            "Question: What color are the teddy bears? [QA]\n",
            "Answer:   The teddy bears are brown in color.<|endoftext|>\n",
            "Model Predicted Ans:  a. a........................ a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a\n"
          ]
        }
      ],
      "source": [
        "# random validation prediction\n",
        "def model_run_val(sample_val_data,max_generate_length=10):\n",
        "\n",
        "    total_val_len = len(sample_val_data)\n",
        "    random_val_datapoint = random.randrange(1,total_val_len) # 0 is header\n",
        "\n",
        "    val_image_url = sample_val_data[random_val_datapoint][0]\n",
        "    val_q = sample_val_data[random_val_datapoint][1]\n",
        "    val_a = sample_val_data[random_val_datapoint][2]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_load = Image.open(requests.get(val_image_url,stream=True).raw)\n",
        "        image_processed = processor(images=image_load, return_tensors=\"pt\").to(device)\n",
        "        clip_val_outputs = clip_model(**image_processed).last_hidden_state[:,1:,:]\n",
        "        val_image_embeds = projection_layer(clip_val_outputs)\n",
        "        val_image_embeds = projection_model(val_image_embeds).to(torch.float16)\n",
        "\n",
        "        img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).to(device)\n",
        "        img_token_embeds = peft_model.model.model.embed_tokens(img_token_tensor).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        val_q_tokenised = tokenizer(val_q, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
        "        val_q_embeds  = peft_model.model.model.embed_tokens(val_q_tokenised).unsqueeze(0)\n",
        "\n",
        "        val_combined_embeds = torch.cat([val_image_embeds, img_token_embeds, val_q_embeds], dim=1) # 1, 69, 2560\n",
        "\n",
        "        predicted_caption = torch.full((1,max_generate_length),50256)\n",
        "\n",
        "        for g in range(max_generate_length):\n",
        "            phi_output_logits = peft_model(inputs_embeds=val_combined_embeds)['logits'] # 4, 69, 51200\n",
        "            predicted_word_token_logits = phi_output_logits[:, -1, :].unsqueeze(1) # 4,1,51200\n",
        "            predicted_word_token   = torch.argmax(predicted_word_token_logits, dim = -1) # 4,1\n",
        "            predicted_caption[:,g] = predicted_word_token.view(1,-1).to('cpu')\n",
        "            next_token_embeds      = phi_model.model.embed_tokens(predicted_word_token) # 4,1,2560\n",
        "            val_combined_embeds    = torch.cat([val_combined_embeds, next_token_embeds], dim=1)\n",
        "\n",
        "        predicted_captions_decoded = tokenizer.batch_decode(predicted_caption,ignore_index = 50256)[0]\n",
        "\n",
        "    print(f\"Image: {val_image_url}\")\n",
        "    print(f\"Question: {val_q}\")\n",
        "    print(f\"Answer:   {val_a}\")\n",
        "    print(f\"Model Predicted Ans: {predicted_captions_decoded}\")\n",
        "\n",
        "model_run_val(sample_val_data,max_generate_length=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1896295f-05f9-4541-bfdd-ea41de8cbb14",
      "metadata": {
        "id": "1896295f-05f9-4541-bfdd-ea41de8cbb14"
      },
      "source": [
        "# Start of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "932d0019-0cb7-4a6a-bc6e-bf7033374bfc",
      "metadata": {
        "id": "932d0019-0cb7-4a6a-bc6e-bf7033374bfc"
      },
      "outputs": [],
      "source": [
        "phi_optimizer        = torch.optim.Adam(peft_model.parameters(), lr=1e-6)\n",
        "projection_layer_optimizer = torch.optim.Adam(projection_layer.parameters(), lr=1e-5)\n",
        "projection_model_optimizer = torch.optim.Adam(projection_model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "#gc.collect()"
      ],
      "metadata": {
        "id": "lM84EaNcBLDN"
      },
      "id": "lM84EaNcBLDN",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = 0\n",
        "running_loss = 0.\n",
        "projection_layer.train()\n",
        "projection_model.train()\n",
        "peft_model.train()\n",
        "\n",
        "\n",
        "for epoch in range(2):\n",
        "    for batch_idx, (images,questions,answers) in enumerate(train_dataloader):\n",
        "\n",
        "        # process input data\n",
        "        batch_size = questions.size(0)\n",
        "        questions  = questions.to(device)\n",
        "        answers    = answers.to(device)\n",
        "\n",
        "        # clip\n",
        "        images = {'pixel_values': images.to(device)}\n",
        "        clip_outputs  = clip_model(**images)\n",
        "        images_embeds = clip_outputs.last_hidden_state[:,1:,:] # remove cls token\n",
        "\n",
        "        # projection\n",
        "        image_embeds  = projection_layer(images_embeds)\n",
        "        image_embeds = projection_model(image_embeds).to(torch.float16)\n",
        "\n",
        "\n",
        "        # embeds\n",
        "        # print(f\"questions shape{questions.shape},answers shape{answers.shape}\")\n",
        "        img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).repeat(batch_size, 1).to(device)\n",
        "        img_token_embeds = peft_model.model.model.embed_tokens(img_token_tensor)\n",
        "        questions_embed  = peft_model.model.model.embed_tokens(questions)\n",
        "\n",
        "        # forward pass\n",
        "        #print(\"***************\")\n",
        "        # print(image_embeds.shape)\n",
        "        combined_embeds = torch.cat([image_embeds, img_token_embeds, questions_embed], dim=1) # 4, 69, 2560\n",
        "        #print(f\"combined_embeds shape{combined_embeds.shape}\")\n",
        "        phi_output_logits = peft_model(inputs_embeds=combined_embeds)['logits'] # 4, 69, 51200\n",
        "        # print(f\"phi_output_logits shape{phi_output_logits.shape}\")\n",
        "        # print(f\"answers shape {answers.shape}\")\n",
        "\n",
        "        # take out the image embeddings\n",
        "        phi_output_logits = phi_output_logits[:,images_embeds.shape[1] + 1 : ,:]\n",
        "        # print(f\"phi_output_logits after shape{phi_output_logits.shape}\")\n",
        "        phi_output_logits = phi_output_logits.reshape(-1,vocab_size)\n",
        "\n",
        "        phi_optimizer.zero_grad()\n",
        "        projection_layer_optimizer.zero_grad()\n",
        "        projection_model_optimizer.zero_grad()\n",
        "        # min_batch_size = min(phi_output_logits.size(0), answers.size(0))\n",
        "        # phi_output_logits = phi_output_logits[:min_batch_size]\n",
        "        # answers = answers.contiguous().view(-1)[:min_batch_size]\n",
        "        loss = F.cross_entropy(phi_output_logits.float(), answers.contiguous().view(-1), ignore_index=EOS_TOKEN_ID, label_smoothing=0.1)\n",
        "\n",
        "\n",
        "        # loss backprop\n",
        "        loss.backward()\n",
        "        phi_optimizer.step()\n",
        "        projection_layer_optimizer.step()\n",
        "        projection_model_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        if step % model_log_step == 0:\n",
        "            print(f\"Iteration {step}/{max_steps}, Loss: {loss.item()}\")\n",
        "\n",
        "        if step % model_val_step == 0:\n",
        "            projection_layer.eval()\n",
        "            projection_model.eval()\n",
        "            peft_model.eval()\n",
        "\n",
        "            model_run_val(sample_val_data,max_generate_length)\n",
        "            projection_layer.train()\n",
        "            projection_model.train()\n",
        "            peft_model.train()\n",
        "\n",
        "\n",
        "        if step % model_save_step == 0:\n",
        "            print(\"Saving Checkpoint\")\n",
        "            torch.save(projection_layer.state_dict(),'/content/drive/MyDrive/capstone/Stage_2/model_chkpt/ft_projection_layer.pth')\n",
        "            torch.save(projection_model.state_dict(),'/content/drive/MyDrive/capstone/Stage_2/model_chkpt/ft_projection_model.pth')\n",
        "            peft_model.save_pretrained('/content/drive/MyDrive/capstone/Stage_2/model_chkpt/qlora_adaptor/', save_adapter=True, save_config=True)\n",
        "\n",
        "        if step >= max_steps:\n",
        "            print(\"Training finished.\")\n",
        "            break\n",
        "\n",
        "        wandb.log({\"step\": step, \"train_loss\": loss.item()})\n",
        "        step += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkrEUDxjqfQY",
        "outputId": "d32a9a24-db5d-47b5-f344-8557c213eb05"
      },
      "id": "bkrEUDxjqfQY",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0/20000, Loss: 3.7245471477508545\n",
            "Image: http://images.cocodataset.org/train2017/000000038899.jpg\n",
            "Question: What is the man attempting to catch in the image? [QA]\n",
            "Answer:   The man is attempting to catch a frisbee or a flying disc in the image.<|endoftext|>\n",
            "Model Predicted Ans: The man is attempting to catch a baseball in the image.                                                                                        \n",
            "Saving Checkpoint\n",
            "Iteration 100/20000, Loss: 4.122663497924805\n",
            "Saving Checkpoint\n",
            "Iteration 200/20000, Loss: 3.616518974304199\n",
            "Saving Checkpoint\n",
            "Iteration 300/20000, Loss: 3.5029053688049316\n",
            "Saving Checkpoint\n",
            "Iteration 400/20000, Loss: 3.7930595874786377\n",
            "Saving Checkpoint\n",
            "Iteration 500/20000, Loss: 3.990140199661255\n",
            "Saving Checkpoint\n",
            "Iteration 600/20000, Loss: 3.8376057147979736\n",
            "Saving Checkpoint\n",
            "Iteration 700/20000, Loss: 3.5134801864624023\n",
            "Saving Checkpoint\n",
            "Iteration 800/20000, Loss: 3.618428945541382\n",
            "Saving Checkpoint\n",
            "Iteration 900/20000, Loss: 3.583021640777588\n",
            "Saving Checkpoint\n",
            "Iteration 1000/20000, Loss: 3.678767681121826\n",
            "Image: http://images.cocodataset.org/train2017/000000470467.jpg\n",
            "Question: How many women are dancing in the image? [QA]\n",
            "Answer:   There are two women dancing in the image.<|endoftext|>\n",
            "Model Predicted Ans: There are two women dancing in the image. \u0000The women are wearing black and white dresses, and they are holding hands while dancing. \u0000The women are wearing black and white dresses, and they are holding hands while dancing. \u0000The women are wearing black and white dresses, and they are holding hands while dancing. \u0000The women are wearing black and white dresses, and they are holding hands while dancing. \u0000The women are wearing black and white dresses, and they are holding\n",
            "Saving Checkpoint\n",
            "Iteration 1100/20000, Loss: 3.425313711166382\n",
            "Saving Checkpoint\n",
            "Iteration 1200/20000, Loss: 3.405616283416748\n",
            "Saving Checkpoint\n",
            "Iteration 1300/20000, Loss: 4.121916770935059\n",
            "Saving Checkpoint\n",
            "Iteration 1400/20000, Loss: 4.030831813812256\n",
            "Saving Checkpoint\n",
            "Iteration 1500/20000, Loss: 3.74674654006958\n",
            "Saving Checkpoint\n",
            "Iteration 1600/20000, Loss: 3.564483642578125\n",
            "Saving Checkpoint\n",
            "Iteration 1700/20000, Loss: 3.642059326171875\n",
            "Saving Checkpoint\n",
            "Iteration 1800/20000, Loss: 3.3877644538879395\n",
            "Saving Checkpoint\n",
            "Iteration 1900/20000, Loss: 3.6678404808044434\n",
            "Saving Checkpoint\n",
            "Iteration 2000/20000, Loss: 3.6374447345733643\n",
            "Image: http://images.cocodataset.org/train2017/000000314174.jpg\n",
            "Question: What is the event taking place in the image? [QA]\n",
            "Answer:   The image displays a birthday celebration, with a cake and lit candles being presented to a girl who is getting ready to blow them out.<|endoftext|>\n",
            "Model Predicted Ans: The image depicts a birthday party, specifically a birthday celebration. \u0000The person in the image is holding a cake and a lit candle, which is a common tradition during birthday celebrations. \u0000The person is also wearing a party hat, which is another common accessory for birthday parties. \u0000The image also shows a group of people, indicating that there is a gathering or celebration happening. \u0000The event taking place in the image is a birthday party. \u0000The person is holding a cake\n",
            "Saving Checkpoint\n",
            "Iteration 2100/20000, Loss: 4.115924835205078\n",
            "Saving Checkpoint\n",
            "Iteration 2200/20000, Loss: 3.5788731575012207\n",
            "Saving Checkpoint\n",
            "Iteration 2300/20000, Loss: 3.0476608276367188\n",
            "Saving Checkpoint\n",
            "Iteration 2400/20000, Loss: 3.244206428527832\n",
            "Saving Checkpoint\n",
            "Iteration 2500/20000, Loss: 3.946014165878296\n",
            "Saving Checkpoint\n",
            "Iteration 2600/20000, Loss: 4.005293369293213\n",
            "Saving Checkpoint\n",
            "Iteration 2700/20000, Loss: 3.82928466796875\n",
            "Saving Checkpoint\n",
            "Iteration 2800/20000, Loss: 3.205383777618408\n",
            "Saving Checkpoint\n",
            "Iteration 2900/20000, Loss: 4.096004486083984\n",
            "Saving Checkpoint\n",
            "Iteration 3000/20000, Loss: 3.7149980068206787\n",
            "Image: http://images.cocodataset.org/train2017/000000132299.jpg\n",
            "Question: Where is the woman feeding the calf? [QA]\n",
            "Answer:   The woman is feeding the calf in a stall or stable.<|endoftext|>\n",
            "Model Predicted Ans: The woman is feeding the calf in a field, possibly in a pasture or open area. \u0000The woman is using a bucket to feed the calf, possibly a bucket of hay or other food. \u0000The woman is standing in a field, possibly in a grassy area or a pasture. \u0000The woman is wearing a white dress, which is a common attire for women in many cultures. \u0000The woman is holding a bucket, which is a common tool used for feeding animals or\n",
            "Saving Checkpoint\n",
            "Iteration 3100/20000, Loss: 3.6112053394317627\n",
            "Saving Checkpoint\n",
            "Iteration 3200/20000, Loss: 3.3106751441955566\n",
            "Saving Checkpoint\n",
            "Iteration 3300/20000, Loss: 3.6461129188537598\n",
            "Saving Checkpoint\n",
            "Iteration 3400/20000, Loss: 3.246680736541748\n",
            "Saving Checkpoint\n",
            "Iteration 3500/20000, Loss: 3.3956708908081055\n",
            "Saving Checkpoint\n",
            "Iteration 3600/20000, Loss: 3.400444507598877\n",
            "Saving Checkpoint\n",
            "Iteration 3700/20000, Loss: 3.3708672523498535\n",
            "Saving Checkpoint\n",
            "Iteration 3800/20000, Loss: 3.0643177032470703\n",
            "Saving Checkpoint\n",
            "Iteration 3900/20000, Loss: 3.829761028289795\n",
            "Saving Checkpoint\n",
            "Iteration 4000/20000, Loss: 3.582886219024658\n",
            "Image: http://images.cocodataset.org/train2017/000000170751.jpg\n",
            "Question: What color is the man's shirt in the image? [QA]\n",
            "Answer:   The man's shirt in the image is green with stripes.<|endoftext|>\n",
            "Model Predicted Ans: The man's shirt is blue in color in the image. \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000\n",
            "Saving Checkpoint\n",
            "Iteration 4100/20000, Loss: 3.744429111480713\n",
            "Saving Checkpoint\n",
            "Iteration 4200/20000, Loss: 3.1740856170654297\n",
            "Saving Checkpoint\n",
            "Iteration 4300/20000, Loss: 3.1184873580932617\n",
            "Saving Checkpoint\n",
            "Iteration 4400/20000, Loss: 3.76387619972229\n",
            "Saving Checkpoint\n",
            "Iteration 4500/20000, Loss: 3.514822006225586\n",
            "Saving Checkpoint\n",
            "Iteration 4600/20000, Loss: 3.803135395050049\n",
            "Saving Checkpoint\n",
            "Iteration 4700/20000, Loss: 3.2767386436462402\n",
            "Saving Checkpoint\n",
            "Iteration 4800/20000, Loss: 3.3841309547424316\n",
            "Saving Checkpoint\n",
            "Iteration 4900/20000, Loss: 3.2758684158325195\n",
            "Saving Checkpoint\n",
            "Iteration 5000/20000, Loss: 3.9130678176879883\n",
            "Image: http://images.cocodataset.org/train2017/000000254177.jpg\n",
            "Question: Does the man have any facial hair? [QA]\n",
            "Answer:   Yes, the man has a mustache.<|endoftext|>\n",
            "Model Predicted Ans: Yes, the man has a mustache. He is wearing a black suit and a white shirt, and he has a mustache. \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \n",
            "Saving Checkpoint\n",
            "Iteration 5100/20000, Loss: 3.320600986480713\n",
            "Saving Checkpoint\n",
            "Iteration 5200/20000, Loss: 3.0118255615234375\n",
            "Saving Checkpoint\n",
            "Iteration 5300/20000, Loss: 3.634404182434082\n",
            "Saving Checkpoint\n",
            "Iteration 5400/20000, Loss: 3.380472183227539\n",
            "Saving Checkpoint\n",
            "Iteration 5500/20000, Loss: 3.6033198833465576\n",
            "Saving Checkpoint\n",
            "Iteration 5600/20000, Loss: 3.4762191772460938\n",
            "Saving Checkpoint\n",
            "Iteration 5700/20000, Loss: 3.2394421100616455\n",
            "Saving Checkpoint\n",
            "Iteration 5800/20000, Loss: 3.119748115539551\n",
            "Saving Checkpoint\n",
            "Iteration 5900/20000, Loss: 3.758514881134033\n",
            "Saving Checkpoint\n",
            "Iteration 6000/20000, Loss: 3.4213309288024902\n",
            "Image: http://images.cocodataset.org/train2017/000000095509.jpg\n",
            "Question: What is the age and gender of the person on the surfboard? [QA]\n",
            "Answer:   The person on the surfboard is a young man. It is difficult to determine his exact age, but he appears to be a youth or young adult.<|endoftext|>\n",
            "Model Predicted Ans: The person on the surfboard is a young man, possibly in his teens or early twenties. He is wearing a wetsuit and a surfboard, indicating that he is engaged in surfing. \u0000The person is also wearing a hat, which is a common accessory for surfers to protect their heads from the sun. \u0000The person is also wearing a wetsuit, which is a type of clothing worn by surfers to keep them warm in cold water. \u0000Overall, the\n",
            "Saving Checkpoint\n",
            "Iteration 6100/20000, Loss: 3.3303451538085938\n",
            "Saving Checkpoint\n",
            "Iteration 6200/20000, Loss: 3.310969352722168\n",
            "Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78cb35225750>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6300/20000, Loss: 3.3181724548339844\n",
            "Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78cb35225750>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78cb35225750>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78cb35225750>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6400/20000, Loss: 3.731227397918701\n",
            "Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78cb35225750>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78cb35225750>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78cb35225750>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78cb35225750>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6500/20000, Loss: 3.356456756591797\n",
            "Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78cb35225750>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6600/20000, Loss: 3.3726680278778076\n",
            "Saving Checkpoint\n",
            "Iteration 6700/20000, Loss: 4.006077766418457\n",
            "Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78cb35225750>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6800/20000, Loss: 3.299954891204834\n",
            "Saving Checkpoint\n",
            "Iteration 6900/20000, Loss: 3.8004989624023438\n",
            "Saving Checkpoint\n",
            "Iteration 7000/20000, Loss: 3.0115110874176025\n",
            "Image: http://images.cocodataset.org/train2017/000000546408.jpg\n",
            "Question: Where is the woman standing in the image? [QA]\n",
            "Answer:   The woman is standing on an airport tarmac in the image.<|endoftext|>\n",
            "Model Predicted Ans: The woman is standing in a field, possibly in a rural or open area. She is holding an umbrella and a large red umbrella, which she is holding up in the air. \u0000The woman is standing in a field, possibly in a rural or open area. She is holding an umbrella and a large red umbrella, which she is holding up in the air. \u0000The woman is standing in a field, possibly in a rural or open area. She is holding an umbrella and a large\n",
            "Saving Checkpoint\n",
            "Iteration 7100/20000, Loss: 3.168562412261963\n",
            "Saving Checkpoint\n",
            "Iteration 7200/20000, Loss: 3.4053688049316406\n",
            "Saving Checkpoint\n",
            "Iteration 7300/20000, Loss: 3.305392026901245\n",
            "Saving Checkpoint\n",
            "Iteration 7400/20000, Loss: 3.2850751876831055\n",
            "Saving Checkpoint\n",
            "Iteration 7500/20000, Loss: 3.584024429321289\n",
            "Saving Checkpoint\n",
            "Iteration 7600/20000, Loss: 3.1722917556762695\n",
            "Saving Checkpoint\n",
            "Iteration 7700/20000, Loss: 3.422060012817383\n",
            "Saving Checkpoint\n",
            "Iteration 7800/20000, Loss: 3.4305596351623535\n",
            "Saving Checkpoint\n",
            "Iteration 7900/20000, Loss: 3.482424259185791\n",
            "Saving Checkpoint\n",
            "Iteration 8000/20000, Loss: 3.482926368713379\n",
            "Image: http://images.cocodataset.org/train2017/000000346313.jpg\n",
            "Question: Is the image of the boy and the soccer ball a photograph or a wax figurine? [QA]\n",
            "Answer:   The image shows a real boy playing soccer on a field, not a wax figurine.<|endoftext|>\n",
            "Model Predicted Ans: The image is a photograph.  or a wax figurine?  the image is a photograph.  or a wax figurine?  the image is a photograph.  or a wax figurine?  the image is a photograph.  or a wax figurine?  the image is a photograph.  or a wax figurine?  the image is a photograph.  or a wax figurine?  the image is a photograph.  or a wax figurine?  the image\n",
            "Saving Checkpoint\n",
            "Iteration 8100/20000, Loss: 4.141637325286865\n",
            "Saving Checkpoint\n",
            "Iteration 8200/20000, Loss: 3.2000796794891357\n",
            "Saving Checkpoint\n",
            "Iteration 8300/20000, Loss: 3.7126317024230957\n",
            "Saving Checkpoint\n",
            "Iteration 8400/20000, Loss: 3.433438301086426\n",
            "Saving Checkpoint\n",
            "Iteration 8500/20000, Loss: 3.429302215576172\n",
            "Saving Checkpoint\n",
            "Iteration 8600/20000, Loss: 3.1757731437683105\n",
            "Saving Checkpoint\n",
            "Iteration 8700/20000, Loss: 3.447169303894043\n",
            "Saving Checkpoint\n",
            "Iteration 8800/20000, Loss: 4.02587366104126\n",
            "Saving Checkpoint\n",
            "Iteration 8900/20000, Loss: 3.6089367866516113\n",
            "Saving Checkpoint\n",
            "Iteration 9000/20000, Loss: 3.430716037750244\n",
            "Image: http://images.cocodataset.org/train2017/000000516158.jpg\n",
            "Question: How many people are visible riding bicycles in the image? [QA]\n",
            "Answer:   There is one person visible riding a bicycle in the image.<|endoftext|>\n",
            "Model Predicted Ans: There are two people visible riding bicycles in the image. One person is riding a red bicycle, and the other person is riding a blue bicycle. \u0000The image shows two people riding bicycles, one on a red bicycle and the other on a blue bicycle. \u0000The image shows two people riding bicycles, one on a red bicycle and the other on a blue bicycle. \u0000The image shows two people riding bicycles, one on a red bicycle and the other on a blue bicycle. \u0000\n",
            "Saving Checkpoint\n",
            "Iteration 9100/20000, Loss: 3.95508074760437\n",
            "Saving Checkpoint\n",
            "Iteration 9200/20000, Loss: 3.345764636993408\n",
            "Saving Checkpoint\n",
            "Iteration 9300/20000, Loss: 3.6116104125976562\n",
            "Saving Checkpoint\n",
            "Iteration 9400/20000, Loss: 3.7098653316497803\n",
            "Saving Checkpoint\n",
            "Iteration 9500/20000, Loss: 2.7856836318969727\n",
            "Saving Checkpoint\n",
            "Iteration 9600/20000, Loss: 3.5910086631774902\n",
            "Saving Checkpoint\n",
            "Iteration 9700/20000, Loss: 3.527657985687256\n",
            "Saving Checkpoint\n",
            "Iteration 9800/20000, Loss: 3.7055952548980713\n",
            "Saving Checkpoint\n",
            "Iteration 9900/20000, Loss: 3.077603340148926\n",
            "Saving Checkpoint\n",
            "Iteration 10000/20000, Loss: 3.3403210639953613\n",
            "Image: http://images.cocodataset.org/train2017/000000228119.jpg\n",
            "Question: What is the man's facial expression while feeding the baby? [QA]\n",
            "Answer:   The man is smiling while feeding the baby her bottle.<|endoftext|>\n",
            "Model Predicted Ans: The man's facial expression while feeding the baby is relaxed and content. He is smiling and appears to be enjoying the interaction with the baby. \u0000The man's facial expression while feeding the baby is relaxed and content. He is smiling and appears to be enjoying the interaction with the baby. \u0000The man's facial expression while feeding the baby is relaxed and content. He is smiling and appears to be enjoying the interaction with the baby. \u0000The man's facial expression while feeding the baby is\n",
            "Saving Checkpoint\n",
            "Iteration 10100/20000, Loss: 3.75125789642334\n",
            "Saving Checkpoint\n",
            "Iteration 10200/20000, Loss: 3.187946319580078\n",
            "Saving Checkpoint\n",
            "Iteration 10300/20000, Loss: 3.4504189491271973\n",
            "Saving Checkpoint\n",
            "Iteration 10400/20000, Loss: 3.505545139312744\n",
            "Saving Checkpoint\n",
            "Iteration 10500/20000, Loss: 3.6212172508239746\n",
            "Saving Checkpoint\n",
            "Iteration 10600/20000, Loss: 3.6106715202331543\n",
            "Saving Checkpoint\n",
            "Iteration 10700/20000, Loss: 3.498143196105957\n",
            "Saving Checkpoint\n",
            "Iteration 10800/20000, Loss: 2.9162373542785645\n",
            "Saving Checkpoint\n",
            "Iteration 10900/20000, Loss: 3.0544018745422363\n",
            "Saving Checkpoint\n",
            "Iteration 11000/20000, Loss: 3.580660104751587\n",
            "Image: http://images.cocodataset.org/train2017/000000153589.jpg\n",
            "Question: What are the zebras doing in the image? [QA]\n",
            "Answer:   The zebras are grazing and eating grass in the field.<|endoftext|>\n",
            "Model Predicted Ans: The zebras are standing in a field, possibly grazing on grass or other vegetation. They are also standing in a group, possibly socializing or interacting with each other. \u0000The zebras are standing in a field, possibly grazing on grass or other vegetation. They are also standing in a group, possibly socializing or interacting with each other. \u0000The zebras are standing in a field, possibly grazing on grass or other vegetation. They are also standing in a group,\n",
            "Saving Checkpoint\n",
            "Iteration 11100/20000, Loss: 3.72576904296875\n",
            "Saving Checkpoint\n",
            "Iteration 11200/20000, Loss: 3.4573583602905273\n",
            "Saving Checkpoint\n",
            "Iteration 11300/20000, Loss: 3.5868067741394043\n",
            "Saving Checkpoint\n",
            "Iteration 11400/20000, Loss: 3.1723551750183105\n",
            "Saving Checkpoint\n",
            "Iteration 11500/20000, Loss: 4.008584976196289\n",
            "Saving Checkpoint\n",
            "Iteration 11600/20000, Loss: 3.22171688079834\n",
            "Saving Checkpoint\n",
            "Iteration 11700/20000, Loss: 3.021151542663574\n",
            "Saving Checkpoint\n",
            "Iteration 11800/20000, Loss: 3.6757349967956543\n",
            "Saving Checkpoint\n",
            "Iteration 11900/20000, Loss: 3.2853479385375977\n",
            "Saving Checkpoint\n",
            "Iteration 12000/20000, Loss: 3.539487361907959\n",
            "Image: http://images.cocodataset.org/train2017/000000280340.jpg\n",
            "Question: What is the man holding in his hand? [QA]\n",
            "Answer:   The man is holding a Pizza Hut salt shaker in his hand.<|endoftext|>\n",
            "Model Predicted Ans: The man is holding a plate of food in his hand. He is holding a plate of food, possibly a meal or a dish, while he is playing the video game.  Cosponsors of the man's plate of food are a plate of fries and a glass of soda.  Cosponsors of the man's plate of food are a plate of fries and a glass of soda.  The man is holding a plate of food, possibly a meal or a dish, while he is playing the video game.\n",
            "Saving Checkpoint\n",
            "Iteration 12100/20000, Loss: 3.4486498832702637\n",
            "Saving Checkpoint\n",
            "Iteration 12200/20000, Loss: 3.4881367683410645\n",
            "Saving Checkpoint\n",
            "Iteration 12300/20000, Loss: 3.347902774810791\n",
            "Saving Checkpoint\n",
            "Iteration 12400/20000, Loss: 3.4600493907928467\n",
            "Saving Checkpoint\n",
            "Iteration 12500/20000, Loss: 3.3972814083099365\n",
            "Saving Checkpoint\n",
            "Iteration 12600/20000, Loss: 3.2460994720458984\n",
            "Saving Checkpoint\n",
            "Iteration 12700/20000, Loss: 3.3169612884521484\n",
            "Saving Checkpoint\n",
            "Iteration 12800/20000, Loss: 3.248880386352539\n",
            "Saving Checkpoint\n",
            "Iteration 12900/20000, Loss: 3.471980571746826\n",
            "Saving Checkpoint\n",
            "Iteration 13000/20000, Loss: 3.541985034942627\n",
            "Image: http://images.cocodataset.org/train2017/000000454961.jpg\n",
            "Question: What is the fish fillet covered with? [QA]\n",
            "Answer:   The fish fillet is covered with gravy.<|endoftext|>\n",
            "Model Predicted Ans: The fish fillet is covered with a layer of rice.  Cosponsors of the fish fillet are also present on the plate.                                                                          \n",
            "Saving Checkpoint\n",
            "Iteration 13100/20000, Loss: 3.6377902030944824\n",
            "Saving Checkpoint\n",
            "Iteration 13200/20000, Loss: 3.5917320251464844\n",
            "Saving Checkpoint\n",
            "Iteration 13300/20000, Loss: 3.2404847145080566\n",
            "Saving Checkpoint\n",
            "Iteration 13400/20000, Loss: 3.7176079750061035\n",
            "Saving Checkpoint\n",
            "Iteration 13500/20000, Loss: 3.1609625816345215\n",
            "Saving Checkpoint\n",
            "Iteration 13600/20000, Loss: 3.612147569656372\n",
            "Saving Checkpoint\n",
            "Iteration 13700/20000, Loss: 3.9286279678344727\n",
            "Saving Checkpoint\n",
            "Iteration 13800/20000, Loss: 3.4900152683258057\n",
            "Saving Checkpoint\n",
            "Iteration 13900/20000, Loss: 3.285305976867676\n",
            "Saving Checkpoint\n",
            "Iteration 14000/20000, Loss: 3.127295970916748\n",
            "Image: http://images.cocodataset.org/train2017/000000319045.jpg\n",
            "Question: How would you describe the group size in the image? [QA]\n",
            "Answer:   The group size in the image can be described as small.<|endoftext|>\n",
            "Model Predicted Ans: The group size in the image is small, with only a few people visible. \u0000The image shows a small group of people skiing on the snowy slope. \u0000The group is not very large, as there are only a few people visible in the image. \u0000The group is engaged in skiing, which suggests that they are enjoying the activity together. \u0000The image captures a small group of people skiing on the snowy slope, enjoying the winter scenery. \u0000The group is engaged in\n",
            "Saving Checkpoint\n",
            "Iteration 14100/20000, Loss: 3.381617546081543\n",
            "Saving Checkpoint\n",
            "Iteration 14200/20000, Loss: 3.7034974098205566\n",
            "Saving Checkpoint\n",
            "Iteration 14300/20000, Loss: 3.5303094387054443\n",
            "Saving Checkpoint\n",
            "Iteration 14400/20000, Loss: 3.184659957885742\n",
            "Saving Checkpoint\n",
            "Iteration 14500/20000, Loss: 2.9250121116638184\n",
            "Saving Checkpoint\n",
            "Iteration 14600/20000, Loss: 3.0991733074188232\n",
            "Saving Checkpoint\n",
            "Iteration 14700/20000, Loss: 4.093841075897217\n",
            "Saving Checkpoint\n",
            "Iteration 14800/20000, Loss: 3.660717010498047\n",
            "Saving Checkpoint\n",
            "Iteration 14900/20000, Loss: 2.8748884201049805\n",
            "Saving Checkpoint\n",
            "Iteration 15000/20000, Loss: 3.7184481620788574\n",
            "Image: http://images.cocodataset.org/train2017/000000175951.jpg\n",
            "Question: Where is the no parking sign located? [QA]\n",
            "Answer:   The no parking sign is located on the side of a street, near an intersection in the city. It is placed on the sidewalk, mounted on a pole close to the street.<|endoftext|>\n",
            "Model Predicted Ans: The no parking sign is located on the side of the street, near the intersection of the two roads. It is positioned on the side of the street, near the intersection of the two roads. \u0000The sign is located on the side of the street, near the intersection of the two roads. It is positioned on the side of the street, near the intersection of the two roads. \u0000The sign is located on the side of the street, near the intersection of the two roads. It\n",
            "Saving Checkpoint\n",
            "Iteration 15100/20000, Loss: 3.7785725593566895\n",
            "Saving Checkpoint\n",
            "Iteration 15200/20000, Loss: 3.637467622756958\n",
            "Saving Checkpoint\n",
            "Iteration 15300/20000, Loss: 3.4103832244873047\n",
            "Saving Checkpoint\n",
            "Iteration 15400/20000, Loss: 3.9162261486053467\n",
            "Saving Checkpoint\n",
            "Iteration 15500/20000, Loss: 3.4987783432006836\n",
            "Saving Checkpoint\n",
            "Iteration 15600/20000, Loss: 3.264753818511963\n",
            "Saving Checkpoint\n",
            "Iteration 15700/20000, Loss: 3.2045490741729736\n",
            "Saving Checkpoint\n",
            "Iteration 15800/20000, Loss: 3.1966054439544678\n",
            "Saving Checkpoint\n",
            "Iteration 15900/20000, Loss: 2.870274782180786\n",
            "Saving Checkpoint\n",
            "Iteration 16000/20000, Loss: 3.1204943656921387\n",
            "Image: http://images.cocodataset.org/train2017/000000312105.jpg\n",
            "Question: Does the surfer have hair, or is he bald? [QA]\n",
            "Answer:   The surfer in the image is bald.<|endoftext|>\n",
            "Model Predicted Ans: The surfer has hair, as he is wearing a wetsuit and a hat, which are common accessories for surfers.  Cosponsors of the surfer's outfit include a wetsuit, a hat, and possibly a rash guard or a wetsuit.  The surfer is riding a wave on a surfboard, which is a longboard specifically designed for riding waves.  The surfer is riding the wave on a large body of water, which is a body of water large\n",
            "Saving Checkpoint\n",
            "Iteration 16100/20000, Loss: 3.0987675189971924\n",
            "Saving Checkpoint\n",
            "Iteration 16200/20000, Loss: 2.7325925827026367\n",
            "Saving Checkpoint\n",
            "Iteration 16300/20000, Loss: 3.295821189880371\n",
            "Saving Checkpoint\n",
            "Iteration 16400/20000, Loss: 3.0930683612823486\n",
            "Saving Checkpoint\n",
            "Iteration 16500/20000, Loss: 2.999964714050293\n",
            "Saving Checkpoint\n",
            "Iteration 16600/20000, Loss: 3.5114831924438477\n",
            "Saving Checkpoint\n",
            "Iteration 16700/20000, Loss: 3.1957364082336426\n",
            "Saving Checkpoint\n",
            "Iteration 16800/20000, Loss: 3.475999355316162\n",
            "Saving Checkpoint\n",
            "Iteration 16900/20000, Loss: 3.6336827278137207\n",
            "Saving Checkpoint\n",
            "Iteration 17000/20000, Loss: 3.0340018272399902\n",
            "Image: http://images.cocodataset.org/train2017/000000354500.jpg\n",
            "Question: What types of equipment can be seen in the image? [QA]\n",
            "Answer:   In the image, you can see a man wearing headphones, and there is a radio station microphone in front of him.<|endoftext|>\n",
            "Model Predicted Ans: In the image, there are two microphones, a podium, and a podium microphone stand. These are the equipment used by the man to deliver his speech.  or a podium microphone stand. These are the equipment used by the man to deliver his speech.  or a podium microphone stand. These are the equipment used by the man to deliver his speech.  or a podium microphone stand. These are the equipment used by the man to deliver his speech.  or a podium microphone stand. These are\n",
            "Saving Checkpoint\n",
            "Iteration 17100/20000, Loss: 3.2871456146240234\n",
            "Saving Checkpoint\n",
            "Iteration 17200/20000, Loss: 3.419023036956787\n",
            "Saving Checkpoint\n",
            "Iteration 17300/20000, Loss: 3.1143031120300293\n",
            "Saving Checkpoint\n",
            "Iteration 17400/20000, Loss: 3.4518678188323975\n",
            "Saving Checkpoint\n",
            "Iteration 17500/20000, Loss: 3.3293628692626953\n",
            "Saving Checkpoint\n",
            "Iteration 17600/20000, Loss: 3.582585573196411\n",
            "Saving Checkpoint\n",
            "Iteration 17700/20000, Loss: 3.8351190090179443\n",
            "Saving Checkpoint\n",
            "Iteration 17800/20000, Loss: 3.254544734954834\n",
            "Saving Checkpoint\n",
            "Iteration 17900/20000, Loss: 2.977221965789795\n",
            "Saving Checkpoint\n",
            "Iteration 18000/20000, Loss: 3.210886240005493\n",
            "Image: http://images.cocodataset.org/train2017/000000482332.jpg\n",
            "Question: How many people can be seen in the image? [QA]\n",
            "Answer:   Two people can be seen in the image.<|endoftext|>\n",
            "Model Predicted Ans: There are three people visible in the image: a young man, a young woman, and a young boy. They are all skateboarding on the tennis court.  or a young boy, a young woman, and a young boy are skateboarding on the tennis court. They are all wearing helmets, which is a safety measure while skateboarding.  or a young man, a young woman, and a young boy are skateboarding on the tennis court. They are all wearing helmets, which is a\n",
            "Saving Checkpoint\n",
            "Iteration 18100/20000, Loss: 3.7628960609436035\n",
            "Saving Checkpoint\n",
            "Iteration 18200/20000, Loss: 3.6369123458862305\n",
            "Saving Checkpoint\n",
            "Iteration 18300/20000, Loss: 3.5362329483032227\n",
            "Saving Checkpoint\n",
            "Iteration 18400/20000, Loss: 3.0612952709198\n",
            "Saving Checkpoint\n",
            "Iteration 18500/20000, Loss: 3.7570900917053223\n",
            "Saving Checkpoint\n",
            "Iteration 18600/20000, Loss: 3.6041688919067383\n",
            "Saving Checkpoint\n",
            "Iteration 18700/20000, Loss: 2.959138870239258\n",
            "Saving Checkpoint\n",
            "Iteration 18800/20000, Loss: 3.3585972785949707\n",
            "Saving Checkpoint\n",
            "Iteration 18900/20000, Loss: 2.8959975242614746\n",
            "Saving Checkpoint\n",
            "Iteration 19000/20000, Loss: 2.9529192447662354\n",
            "Image: http://images.cocodataset.org/train2017/000000171105.jpg\n",
            "Question: Is the boy sitting on the skateboard indoors or outdoors? [QA]\n",
            "Answer:   The image does not provide sufficient information to determine whether the boy is sitting on the skateboard indoors or outdoors.<|endoftext|>\n",
            "Model Predicted Ans: The boy is sitting on the skateboard indoors, in a living room or a similar indoor space.  Cosponsors of the boy's skateboard are a helmet and a knee pad, which are typically worn for safety while skateboarding.  The boy is wearing a helmet and a knee pad, which are typical safety gear for skateboarding.  The image does not provide information about the boy's age or the specific skateboard he is using.  However, it does show him sitting on the skateboard\n",
            "Saving Checkpoint\n",
            "Iteration 19100/20000, Loss: 3.3593673706054688\n",
            "Saving Checkpoint\n",
            "Iteration 19200/20000, Loss: 3.1470537185668945\n",
            "Saving Checkpoint\n",
            "Iteration 19300/20000, Loss: 3.0588927268981934\n",
            "Saving Checkpoint\n",
            "Iteration 19400/20000, Loss: 3.888509511947632\n",
            "Saving Checkpoint\n",
            "Iteration 19500/20000, Loss: 3.704146385192871\n",
            "Saving Checkpoint\n",
            "Iteration 19600/20000, Loss: 3.248237133026123\n",
            "Saving Checkpoint\n",
            "Iteration 19700/20000, Loss: 3.572849750518799\n",
            "Saving Checkpoint\n",
            "Iteration 19800/20000, Loss: 3.5760674476623535\n",
            "Saving Checkpoint\n",
            "Iteration 19900/20000, Loss: 3.6919779777526855\n",
            "Saving Checkpoint\n",
            "Iteration 20000/20000, Loss: 2.959592580795288\n",
            "Image: http://images.cocodataset.org/train2017/000000048282.jpg\n",
            "Question: What type of location is the skateboarder at? [QA]\n",
            "Answer:   The skateboarder is at a skate park, most likely on a half-pipe or a ramp, which allows skateboarders to gain momentum and perform tricks.<|endoftext|>\n",
            "Model Predicted Ans: The skateboarder is at a skate park, which is a designated area for skateboarding with various ramps, rails, and other features for skateboarders to perform tricks and maneuvers. \u0000The skate park is a place where skateboarders can practice and showcase their skills. \u0000The skate park is a designated area for skateboarding, providing a safe and controlled environment for skateboarders to perform tricks and maneuvers. \u0000The skate park is a place where skateboarders can practice and\n",
            "Saving Checkpoint\n",
            "Training finished.\n",
            "Iteration 20000/20000, Loss: 3.5786917209625244\n",
            "Image: http://images.cocodataset.org/train2017/000000427864.jpg\n",
            "Question: What is the setting of the image? [QA]\n",
            "Answer:   The setting of the image is in a snowy field, where the parent and child are flying the kite.<|endoftext|>\n",
            "Model Predicted Ans: The image is set in a snowy environment, with the person flying the kite on a snowy hill. The snow-covered landscape adds to the wintery atmosphere of the image. \u0000The person is flying the kite on a snowy hill, which provides a suitable environment for flying the kite. The snowy setting adds to the wintery atmosphere of the image. \u0000The person is flying the kite on a snowy hill, which provides a suitable environment for flying the kite. The\n",
            "Saving Checkpoint\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pGUOUFWC3Yxe"
      },
      "id": "pGUOUFWC3Yxe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBnxQKTI3Zwt"
      },
      "id": "IBnxQKTI3Zwt",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "009e96c3c4764873b9883395677e25f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92bd9d8f3dd242f6ac18eba196143a5a",
              "IPY_MODEL_65ced50cdf94444f9477a623ca19e144"
            ],
            "layout": "IPY_MODEL_b60ae49d2f7949c6a4b3fa1fafac5b4b"
          }
        },
        "92bd9d8f3dd242f6ac18eba196143a5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50f4cd110b1249419057f090f2490337",
            "placeholder": "​",
            "style": "IPY_MODEL_8ff3a030473e40009be749f4928a5782",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "65ced50cdf94444f9477a623ca19e144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_946a219a834a434195efb56606714b7b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2270bd0004d4697be6415289db6559a",
            "value": 1
          }
        },
        "b60ae49d2f7949c6a4b3fa1fafac5b4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50f4cd110b1249419057f090f2490337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ff3a030473e40009be749f4928a5782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "946a219a834a434195efb56606714b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2270bd0004d4697be6415289db6559a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9d48a4a699840f18e5239afb90b3a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce49153395884f6f9965ca6dfd39c94e",
              "IPY_MODEL_c7d57cd0a3634f0cbf3eb24f814ce268",
              "IPY_MODEL_210a6f3dccbc499282185cc5d973d198"
            ],
            "layout": "IPY_MODEL_85cea7da6332451ab13a717987423804"
          }
        },
        "ce49153395884f6f9965ca6dfd39c94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcbc56d683af473896f43c03cfc0e788",
            "placeholder": "​",
            "style": "IPY_MODEL_0b5400504c09420ba5e2d1d10f0b89e5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c7d57cd0a3634f0cbf3eb24f814ce268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a850b0dad02c490286e76b28789c1093",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_885486af1a14450db900ccb33dd8b178",
            "value": 2
          }
        },
        "210a6f3dccbc499282185cc5d973d198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b4f98fe0bb545a2bb623a1b1aa0ca04",
            "placeholder": "​",
            "style": "IPY_MODEL_347df8ea6c20482ba3c095c640dd52b3",
            "value": " 2/2 [00:05&lt;00:00,  2.27s/it]"
          }
        },
        "85cea7da6332451ab13a717987423804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcbc56d683af473896f43c03cfc0e788": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b5400504c09420ba5e2d1d10f0b89e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a850b0dad02c490286e76b28789c1093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "885486af1a14450db900ccb33dd8b178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b4f98fe0bb545a2bb623a1b1aa0ca04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "347df8ea6c20482ba3c095c640dd52b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}