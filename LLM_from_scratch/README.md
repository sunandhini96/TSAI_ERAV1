# Task: Training LLM from Scratch
## Problem Statement: 
- We trained the LLM model (Phi-1.5) from scratch to learn and generate text-based outputs.
## Dataset:
- We used redpajama dataset, created 100mb sample file for training.
## Training Details:
- Trained the model for 10,000 steps, observing a decrease in loss from 11.19 to 5.6.
- The training process took approximately 40 minutes to complete.
- Employed a batch size of 2 during training.
  
## Training Logs: (Starting):
<img width="439" alt="image" src="https://github.com/sunandhini96/TSAI_ERAV1/assets/63030539/5e13ba9d-5154-4e22-8084-c94f2b374e70">

## Training Logs: (at the end):
<img width="458" alt="image" src="https://github.com/sunandhini96/TSAI_ERAV1/assets/63030539/7841b026-fb62-4384-bf68-5141d123d3d8">

