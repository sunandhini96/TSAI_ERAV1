{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMu6cd7+0CY1JNuapqPaocM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunandhini96/TSAI_ERAV1/blob/main/LLM_from_scratch/llm_from_scratc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNUxBy1ZzbG3",
        "outputId": "be25e30c-807b-48d1-90bd-f6c849b03588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/llm_from_scratch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwy-22YFzwRU",
        "outputId": "08887206-e599-4e57-ba7c-62dfbb26f5f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/llm_from_scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip redpajama_100MB.zip -d redpajama_100mb"
      ],
      "metadata": {
        "id": "K7cHOddz0BAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sunandhini96/TSAI_ERAV1.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwZ6oqHMzwUs",
        "outputId": "2c815c20-e7f3-444f-ee49-cfe3c6434463"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TSAI_ERAV1'...\n",
            "remote: Enumerating objects: 1331, done.\u001b[K\n",
            "remote: Counting objects: 100% (602/602), done.\u001b[K\n",
            "remote: Compressing objects: 100% (278/278), done.\u001b[K\n",
            "remote: Total 1331 (delta 376), reused 516 (delta 316), pack-reused 729\u001b[K\n",
            "Receiving objects: 100% (1331/1331), 31.01 MiB | 14.08 MiB/s, done.\n",
            "Resolving deltas: 100% (621/621), done.\n",
            "Updating files: 100% (186/186), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/llm_from_scratch/TSAI_ERAV1/LLM_from_scratch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAnmB69MzzRg",
        "outputId": "84948c54-6b23-4c92-f63f-2664f7f1ae6f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/llm_from_scratch/TSAI_ERAV1/LLM_from_scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ap7UXjVn0uuT",
        "outputId": "9059d263-ac4a-4680-8f71-609dc5f49fa2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a (from -r requirements.txt (line 2))\n",
            "  Cloning https://github.com/Lightning-AI/lightning (to revision 6dfa5cca9de5c28548eef5582a53c483b0eda66a) to /tmp/pip-install-c8wif7ve/lightning_b0fa33dbaa044711a5231b44a44ea674\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Lightning-AI/lightning /tmp/pip-install-c8wif7ve/lightning_b0fa33dbaa044711a5231b44a44ea674\n",
            "  Running command git rev-parse -q --verify 'sha^6dfa5cca9de5c28548eef5582a53c483b0eda66a'\n",
            "  Running command git fetch -q https://github.com/Lightning-AI/lightning 6dfa5cca9de5c28548eef5582a53c483b0eda66a\n",
            "  Running command git checkout -q 6dfa5cca9de5c28548eef5582a53c483b0eda66a\n",
            "  Resolved https://github.com/Lightning-AI/lightning to commit 6dfa5cca9de5c28548eef5582a53c483b0eda66a\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Encountered 22 file(s) that should have been pointers, but weren't:\n",
            "        .notebooks/course_UvA-DL/01-introduction-to-pytorch.ipynb\n",
            "        .notebooks/course_UvA-DL/02-activation-functions.ipynb\n",
            "        .notebooks/course_UvA-DL/03-initialization-and-optimization.ipynb\n",
            "        .notebooks/course_UvA-DL/04-inception-resnet-densenet.ipynb\n",
            "        .notebooks/course_UvA-DL/05-transformers-and-MH-attention.ipynb\n",
            "        .notebooks/course_UvA-DL/06-graph-neural-networks.ipynb\n",
            "        .notebooks/course_UvA-DL/07-deep-energy-based-generative-models.ipynb\n",
            "        .notebooks/course_UvA-DL/08-deep-autoencoders.ipynb\n",
            "        .notebooks/course_UvA-DL/09-normalizing-flows.ipynb\n",
            "        .notebooks/course_UvA-DL/10-autoregressive-image-modeling.ipynb\n",
            "        .notebooks/course_UvA-DL/11-vision-transformer.ipynb\n",
            "        .notebooks/flash_tutorials/electricity_forecasting.ipynb\n",
            "        .notebooks/flash_tutorials/image_classification.ipynb\n",
            "        .notebooks/flash_tutorials/tabular_classification.ipynb\n",
            "        .notebooks/flash_tutorials/text_classification.ipynb\n",
            "        .notebooks/lightning_examples/cifar10-baseline.ipynb\n",
            "        .notebooks/lightning_examples/datamodules.ipynb\n",
            "        .notebooks/lightning_examples/finetuning-scheduler.ipynb\n",
            "        .notebooks/lightning_examples/warp-drive.ipynb\n",
            "        .notebooks/templates/img-classify.ipynb\n",
            "        .notebooks/templates/simple.ipynb\n",
            "        .notebooks/templates/titanic.ipynb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/EleutherAI/lm-evaluation-harness.git@115206dc89dad67b8beaa90051fb52db77f0a529 (from -r requirements.txt (line 15))\n",
            "  Cloning https://github.com/EleutherAI/lm-evaluation-harness.git (to revision 115206dc89dad67b8beaa90051fb52db77f0a529) to /tmp/pip-req-build-2tf4x808\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/EleutherAI/lm-evaluation-harness.git /tmp/pip-req-build-2tf4x808\n",
            "  Running command git rev-parse -q --verify 'sha^115206dc89dad67b8beaa90051fb52db77f0a529'\n",
            "  Running command git fetch -q https://github.com/EleutherAI/lm-evaluation-harness.git 115206dc89dad67b8beaa90051fb52db77f0a529\n",
            "  Running command git checkout -q 115206dc89dad67b8beaa90051fb52db77f0a529\n",
            "  Resolved https://github.com/EleutherAI/lm-evaluation-harness.git to commit 115206dc89dad67b8beaa90051fb52db77f0a529\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu121)\n",
            "Collecting jsonargparse[signatures] (from -r requirements.txt (line 3))\n",
            "  Downloading jsonargparse-4.27.4-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.41.0 (from -r requirements.txt (line 4))\n",
            "  Downloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.11.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.1.99)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.15.1)\n",
            "Collecting datasets (from -r requirements.txt (line 8))\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zstandard (from -r requirements.txt (line 9))\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.5.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (10.0.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.15.1)\n",
            "Collecting torchmetrics (from -r requirements.txt (line 13))\n",
            "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2)) (6.0.1)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2))\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2)) (4.66.1)\n",
            "Collecting pytorch-lightning (from lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2))\n",
            "  Downloading pytorch_lightning-2.2.0-py3-none-any.whl (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.3/800.3 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docstring-parser>=0.15 (from jsonargparse[signatures]->-r requirements.txt (line 3))\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]->-r requirements.txt (line 3))\n",
            "  Downloading typeshed_client-2.4.0-py3-none-any.whl (594 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->-r requirements.txt (line 7)) (0.20.3)\n",
            "Collecting pyarrow (from -r requirements.txt (line 11))\n",
            "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 8)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 8))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 8)) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 8)) (3.4.1)\n",
            "Collecting multiprocess (from datasets->-r requirements.txt (line 8))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 8)) (3.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 10)) (2023.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 12)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 12)) (1.60.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 12)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 12)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 12)) (3.5.2)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 12)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 12)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 12)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 12)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 12)) (3.0.1)\n",
            "Collecting einops (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.3.0->-r requirements.txt (line 15)) (6.1.1)\n",
            "Collecting jsonlines (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.3.0->-r requirements.txt (line 15)) (2.9.0)\n",
            "Collecting openai>=0.6.4 (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf>=2.2 (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.2.0 (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading peft-0.8.2-py3-none-any.whl (183 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11>=2.6.2 (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycountry (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading pycountry-23.12.11-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytablewriter (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score>=0.0.4 (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu==1.5.0 (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.3.0->-r requirements.txt (line 15)) (1.2.2)\n",
            "Collecting sqlitedict (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm-multiprocess (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.3.0->-r requirements.txt (line 15)) (4.35.2)\n",
            "Collecting accelerate>=0.17.1 (from lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading accelerate-0.27.0-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu==1.5.0->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting lightning-cloud (from lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2))\n",
            "  Downloading lightning_cloud-0.5.64-py3-none-any.whl (928 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m928.4/928.4 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.17.1->lm-eval==0.3.0->-r requirements.txt (line 15)) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.17.1->lm-eval==0.3.0->-r requirements.txt (line 15)) (0.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 12)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 12)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 12)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 12)) (1.3.1)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.2->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm-eval==0.3.0->-r requirements.txt (line 15)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=0.6.4->lm-eval==0.3.0->-r requirements.txt (line 15)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=0.6.4->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm-eval==0.3.0->-r requirements.txt (line 15)) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm-eval==0.3.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 8)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 8)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 8)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 8)) (2024.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval==0.3.0->-r requirements.txt (line 15)) (3.8.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval==0.3.0->-r requirements.txt (line 15)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval==0.3.0->-r requirements.txt (line 15)) (3.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1->lm-eval==0.3.0->-r requirements.txt (line 15)) (2023.12.25)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 12)) (2.1.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2)) (8.1.7)\n",
            "Collecting fastapi (from lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2))\n",
            "  Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2)) (2.3.0)\n",
            "Collecting python-multipart (from lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2)) (13.7.0)\n",
            "Collecting uvicorn (from lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2))\n",
            "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2)) (1.7.0)\n",
            "Collecting boto3 (from lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2))\n",
            "  Downloading boto3-1.34.39-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting DataProperty<2,>=1.0.1 (from pytablewriter->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\n",
            "Collecting typepy[datetime]<2,>=1.3.2 (from pytablewriter->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Collecting colorama (from tqdm-multiprocess->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=0.6.4->lm-eval==0.3.0->-r requirements.txt (line 15)) (1.2.0)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=0.6.4->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=0.6.4->lm-eval==0.3.0->-r requirements.txt (line 15))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval==0.3.0->-r requirements.txt (line 15)) (5.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 12)) (0.5.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=0.6.4->lm-eval==0.3.0->-r requirements.txt (line 15)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=0.6.4->lm-eval==0.3.0->-r requirements.txt (line 15)) (2.16.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 12)) (3.2.2)\n",
            "Collecting botocore<1.35.0,>=1.34.39 (from boto3->lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2))\n",
            "  Downloading botocore-1.34.39-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2))\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.37.0,>=0.36.3 (from fastapi->lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2))\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2)) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->lightning-cloud->lightning@ git+https://github.com/Lightning-AI/lightning@6dfa5cca9de5c28548eef5582a53c483b0eda66a->-r requirements.txt (line 2)) (0.1.2)\n",
            "Building wheels for collected packages: lightning, lm-eval, antlr4-python3-runtime, rouge-score, sqlitedict\n",
            "  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightning: filename=lightning-2.2.0.dev0-py3-none-any.whl size=2030562 sha256=e381beb42b56b71302797e00e8c7cba1de16edd373d9657a88dcd73e8ea48b44\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/1c/3e/de04349904750b0c9a77c6277b9a61123a75da06238d2e98c3\n",
            "  Building wheel for lm-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lm-eval: filename=lm_eval-0.3.0-py3-none-any.whl size=2660363 sha256=c33775c021864e0c59610e3965954883404f43d8f4900d656feff02ca8163957\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/5f/52/6150b0db7e42c8977c1a5fc7d208eb8e77dfad895424d142ef\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=fc4a1ae1e24b7b8fa21a0a01a6f96ca2373ce513dc65585d3d3ceb5702ff5f47\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=e817071ac487439f9bb29102bbac9fd19bf17f0e7777327372118d147939d147\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=2b0f9a5d6d2ac4c7f73b3f4a2c80ff3c74eda4dd168a5f36b2aa160e551331f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built lightning lm-eval antlr4-python3-runtime rouge-score sqlitedict\n",
            "Installing collected packages: sqlitedict, bitsandbytes, antlr4-python3-runtime, zstandard, typeshed-client, tcolorpy, python-multipart, pycountry, pybind11, pyarrow, portalocker, pathvalidate, omegaconf, mbstrdecoder, lightning-utilities, jsonlines, jsonargparse, jmespath, h11, einops, docstring-parser, dill, colorama, uvicorn, typepy, tqdm-multiprocess, starlette, sacrebleu, rouge-score, multiprocess, httpcore, botocore, torchmetrics, s3transfer, httpx, fastapi, accelerate, pytorch-lightning, openai, datasets, DataProperty, boto3, tabledata, peft, lightning-cloud, lightning, pytablewriter, lm-eval\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 10.0.1\n",
            "    Uninstalling pyarrow-10.0.1:\n",
            "      Successfully uninstalled pyarrow-10.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed DataProperty-1.0.1 accelerate-0.27.0 antlr4-python3-runtime-4.9.3 bitsandbytes-0.41.0 boto3-1.34.39 botocore-1.34.39 colorama-0.4.6 datasets-2.17.0 dill-0.3.8 docstring-parser-0.15 einops-0.7.0 fastapi-0.109.2 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 jmespath-1.0.1 jsonargparse-4.27.4 jsonlines-4.0.0 lightning-2.2.0.dev0 lightning-cloud-0.5.64 lightning-utilities-0.10.1 lm-eval-0.3.0 mbstrdecoder-1.1.3 multiprocess-0.70.16 omegaconf-2.3.0 openai-1.12.0 pathvalidate-3.2.0 peft-0.8.2 portalocker-2.8.2 pyarrow-15.0.0 pybind11-2.11.1 pycountry-23.12.11 pytablewriter-1.2.0 python-multipart-0.0.9 pytorch-lightning-2.2.0 rouge-score-0.1.2 s3transfer-0.10.0 sacrebleu-1.5.0 sqlitedict-2.1.0 starlette-0.36.3 tabledata-1.3.3 tcolorpy-0.1.4 torchmetrics-1.3.0.post0 tqdm-multiprocess-0.0.11 typepy-1.3.2 typeshed-client-2.4.0 uvicorn-0.27.1 zstandard-0.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOwTEjFm1iHb",
        "outputId": "3331b76b-3370-4ef8-9095-6c9bea52c358"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio-client\n",
            "  Downloading gradio_client-0.10.0-py3-none-any.whl (307 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/307.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m307.2/307.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio-client) (0.26.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio-client) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client) (23.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client) (4.9.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio-client) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio-client) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio-client) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio-client) (6.0.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio-client) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio-client) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio-client) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio-client) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->gradio-client) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio-client) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio-client) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio-client) (2.0.7)\n",
            "Installing collected packages: websockets, gradio-client\n",
            "Successfully installed gradio-client-0.10.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import glob\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Union\n",
        "import os\n",
        "print(os.getcwd())\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "print(torchaudio.__version__)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Number of CUDA devices:\", torch.cuda.device_count())\n",
        "\n",
        "import lightning as L\n",
        "from lightning.fabric.loggers import CSVLogger\n",
        "from lightning.fabric.strategies import FSDPStrategy\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.utils.flop_counter import FlopCounterMode\n",
        "from speed_monitor import SpeedMonitorBase, estimate_flops, measure_flops\n",
        "from speed_monitor import SpeedMonitorFabric as SpeedMonitor\n",
        "from model import GPT, Block, Config\n",
        "from packed_dataset import CombinedDataset, PackedDataset, PhiAPIDataset\n",
        "from utils import (\n",
        "    chunked_cross_entropy,\n",
        "    get_default_supported_precision,\n",
        "    num_parameters,load_checkpoint\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOeyG_A70mm0",
        "outputId": "32957f66-5a69-4cf1-ed22-6d635e2f0fda"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/llm_from_scratch/TSAI_ERAV1/LLM_from_scratch\n",
            "True\n",
            "2.1.0+cu121\n",
            "12.1\n",
            "2.1.0+cu121\n",
            "Number of CUDA devices: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for running the model\n",
        "model_name = \"phi-1_5\"\n",
        "name = \"redpajama\"\n",
        "out_dir = Path(\"out\") / name\n",
        "save_interval = 1000\n",
        "eval_interval = 1000\n",
        "eval_iters = 100\n",
        "log_interval = 100\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 6e-3\n",
        "batch_size = 2\n",
        "micro_batch_size = 1\n",
        "gradient_accumulation_steps = 4 #batch_size // micro_batch_size\n",
        "assert gradient_accumulation_steps > 0\n",
        "#max_iters = 600000  # num_epochs * (epoch_size // micro_batch_size) // devices\n",
        "max_iters = 10000\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "decay_lr = True\n",
        "warmup_iters = 2000\n",
        "lr_decay_iters = max_iters\n",
        "min_lr = 6e-6\n",
        "\n",
        "# Data configurations\n",
        "data_config = [\n",
        "    # (\"arxiv\", 2.5),\n",
        "    (\"book\", 40.0),\n",
        "    (\"c4\", 20.0),\n",
        "    (\"cc\", 40.0),\n",
        "    # (\"github\", 4.5),\n",
        "    # (\"stackexchange\", 2.0),\n",
        "    # (\"wikipedia\", 4.5),\n",
        "]"
      ],
      "metadata": {
        "id": "ya8ncI091c5Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}\n",
        "logger = CSVLogger(\"out\", name, flush_logs_every_n_steps=log_interval)\n",
        "\n",
        "def setup(\n",
        "    devices: int = 4,\n",
        "    train_data_dir: Path = Path(\"data/redpajama_sample\"),\n",
        "    val_data_dir: Optional[Path] = None,\n",
        "    precision: Optional[str] = None,\n",
        "    resume: Union[bool, Path] = False,\n",
        ") -> None:\n",
        "    precision = precision or get_default_supported_precision(training=True)\n",
        "\n",
        "    if devices > 1:\n",
        "        strategy = FSDPStrategy(\n",
        "            auto_wrap_policy={Block},\n",
        "            activation_checkpointing_policy={Block},\n",
        "            state_dict_type=\"full\",\n",
        "            limit_all_gathers=True,\n",
        "            cpu_offload=False,\n",
        "        )\n",
        "    else:\n",
        "        strategy = \"auto\"\n",
        "\n",
        "    print(f'strategy is {strategy}')\n",
        "    fabric = L.Fabric(devices=devices, strategy=strategy, precision=precision, loggers=logger)\n",
        "    fabric.print(hparams)\n",
        "    fabric.launch(main, train_data_dir, val_data_dir, resume)\n",
        "\n",
        "model_copy = None"
      ],
      "metadata": {
        "id": "rbBhOwDk1c74"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(fabric: L.Fabric, train_data_dir: Path, val_data_dir: Path, resume: Union[bool, Path]) -> None:\n",
        "    global model_copy\n",
        "    speed_monitor = SpeedMonitor(fabric, window_size=50, time_unit=\"seconds\")\n",
        "\n",
        "    if fabric.global_rank == 0:\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    config = Config.from_name(model_name)\n",
        "\n",
        "    train_dataloader, val_dataloader = create_dataloaders(\n",
        "        batch_size=micro_batch_size,\n",
        "        block_size=config.block_size,\n",
        "        fabric=fabric,\n",
        "        train_data_dir=train_data_dir,\n",
        "        val_data_dir=val_data_dir,\n",
        "        seed=(1337 + fabric.global_rank),\n",
        "    )\n",
        "    if val_dataloader is None:\n",
        "        train_dataloader = fabric.setup_dataloaders(train_dataloader)\n",
        "    else:\n",
        "        train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)\n",
        "\n",
        "    fabric.seed_everything(1337)  # same seed for every process to init model (FSDP)\n",
        "\n",
        "    fabric.print(f\"Loading model with {config.__dict__}\")\n",
        "    t0 = time.perf_counter()\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    def _init_weights(module: nn.Module) -> None:\n",
        "            \"\"\"Meant to be used with `gpt.apply(gpt._init_weights)`.\"\"\"\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    with fabric.init_module(empty_init=True):\n",
        "        model = GPT(config)\n",
        "        model.apply(_init_weights)\n",
        "    model.apply(_init_weights)\n",
        "\n",
        "\n",
        "    # checkpoint_path = Path(\"out/redpajama/iter-000999-ckpt.pth\")\n",
        "\n",
        "    # load_checkpoint(fabric, model, checkpoint_path)\n",
        "\n",
        "    # print(model.transformer.h[0].mlp.fc.weight)\n",
        "\n",
        "    fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\")\n",
        "    fabric.print(f\"Total parameters {num_parameters(model):,}\")\n",
        "\n",
        "    model = fabric.setup(model)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), foreach=False\n",
        "    )\n",
        "\n",
        "    # model_copy = model\n",
        "\n",
        "    optimizer = fabric.setup_optimizers(optimizer)\n",
        "\n",
        "    state = {\"model\": model, \"optimizer\": optimizer, \"hparams\": hparams, \"iter_num\": 0, \"step_count\": 0}\n",
        "\n",
        "    if resume is True:\n",
        "        resume = max(out_dir.glob(\"*.pth\"), key=lambda p: int(p.name.split(\"-\")[1]))\n",
        "        fabric.print(f\"Resume is true\")\n",
        "    if resume:\n",
        "        fabric.print(f\"Resuming training from {resume}\")\n",
        "        fabric.load(resume, state)\n",
        "\n",
        "    train_time = time.perf_counter()\n",
        "    train(fabric, state, train_dataloader, val_dataloader, speed_monitor)\n",
        "    # train_check(fabric, state, train_dataloader, val_dataloader, speed_monitor)\n",
        "    fabric.print(f\"Training time: {(time.perf_counter()-train_time):.2f}s\")\n",
        "    if fabric.device.type == \"cuda\":\n",
        "        fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")\n"
      ],
      "metadata": {
        "id": "rBiumubO1c-g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_check(\n",
        "    fabric: L.Fabric,\n",
        "    state: dict,\n",
        "    train_dataloader: DataLoader,\n",
        "    val_dataloader: DataLoader,\n",
        "    speed_monitor: SpeedMonitorBase,\n",
        ") -> None:\n",
        "    model = state[\"model\"]\n",
        "    optimizer = state[\"optimizer\"]\n",
        "\n",
        "    if val_dataloader is not None:\n",
        "        validate(fabric, model, val_dataloader)  # sanity check\n",
        "\n",
        "    with torch.device(\"meta\"):\n",
        "        meta_model = GPT(model.config)\n",
        "        # \"estimated\" is not as precise as \"measured\". Estimated is optimistic but widely used in the wild.\n",
        "        # When comparing MFU or FLOP numbers with other projects that use estimated FLOPs,\n",
        "        # consider passing `SpeedMonitor(flops_per_batch=estimated_flops)` instead\n",
        "        estimated_flops = estimate_flops(meta_model) * micro_batch_size\n",
        "        fabric.print(f\"Estimated TFLOPs: {estimated_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        x = torch.randint(0, 1, (micro_batch_size, model.max_seq_length))\n",
        "        measured_flops = measure_flops(meta_model, x)\n",
        "        fabric.print(f\"Measured TFLOPs: {measured_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        del meta_model, x\n",
        "\n",
        "    total_lengths = 0\n",
        "    total_t0 = time.perf_counter()\n",
        "    # print(f'model : {model}')\n",
        "    for state[\"iter_num\"], train_data in enumerate(train_dataloader, state[\"iter_num\"]):\n",
        "        print(f'train_data : {train_data.shape}')\n",
        "        if state[\"iter_num\"] >= 1:\n",
        "            print(f'Breaking...')\n",
        "            break"
      ],
      "metadata": {
        "id": "4XmV1eW51dCN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    fabric: L.Fabric,\n",
        "    state: dict,\n",
        "    train_dataloader: DataLoader,\n",
        "    val_dataloader: DataLoader,\n",
        "    speed_monitor: SpeedMonitorBase,\n",
        ") -> None:\n",
        "    model = state[\"model\"]\n",
        "    optimizer = state[\"optimizer\"]\n",
        "\n",
        "    if val_dataloader is not None:\n",
        "        validate(fabric, model, val_dataloader)  # sanity check\n",
        "\n",
        "    with torch.device(\"meta\"):\n",
        "        meta_model = GPT(model.config)\n",
        "        # \"estimated\" is not as precise as \"measured\". Estimated is optimistic but widely used in the wild.\n",
        "        # When comparing MFU or FLOP numbers with other projects that use estimated FLOPs,\n",
        "        # consider passing `SpeedMonitor(flops_per_batch=estimated_flops)` instead\n",
        "        estimated_flops = estimate_flops(meta_model) * micro_batch_size\n",
        "        fabric.print(f\"Estimated TFLOPs: {estimated_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        x = torch.randint(0, 1, (micro_batch_size, model.max_seq_length))\n",
        "        measured_flops = measure_flops(meta_model, x)\n",
        "        fabric.print(f\"Measured TFLOPs: {measured_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        del meta_model, x\n",
        "\n",
        "    total_lengths = 0\n",
        "    total_t0 = time.perf_counter()\n",
        "\n",
        "    for state[\"iter_num\"], train_data in enumerate(train_dataloader, state[\"iter_num\"]):\n",
        "        if state[\"iter_num\"] >= max_iters:\n",
        "            checkpoint_path = f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
        "            fabric.print(f\"Breaking at max...iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}\")\n",
        "            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n",
        "            fabric.save(checkpoint_path, state)\n",
        "            break\n",
        "\n",
        "        # determine and set the learning rate for this iteration\n",
        "        lr = get_lr(state[\"iter_num\"]) if decay_lr else learning_rate\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "        iter_t0 = time.perf_counter()\n",
        "\n",
        "        input_ids = train_data[:, 0 : model.max_seq_length].contiguous()\n",
        "        targets = train_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
        "\n",
        "        is_accumulating = (state[\"iter_num\"] + 1) % gradient_accumulation_steps != 0\n",
        "        with fabric.no_backward_sync(model, enabled=is_accumulating):\n",
        "            logits = model(input_ids)\n",
        "            loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "            fabric.backward(loss / gradient_accumulation_steps)\n",
        "\n",
        "        # return\n",
        "\n",
        "        if not is_accumulating:\n",
        "            # print(f'state[\"iter_num\"] : {state[\"iter_num\"]}, gradient_accumulation_steps : {gradient_accumulation_steps}')\n",
        "            # fabric.clip_gradients(model, optimizer, max_norm=grad_clip)#, norm_type=\"inf\")\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            state[\"step_count\"] += 1\n",
        "\n",
        "        t1 = time.perf_counter()\n",
        "        total_lengths += input_ids.size(1)\n",
        "        speed_monitor.on_train_batch_end(\n",
        "            (state[\"iter_num\"] + 1) * micro_batch_size,\n",
        "            t1 - total_t0,\n",
        "            # this assumes that device FLOPs are the same and that all devices have the same batch size\n",
        "            fabric.world_size,\n",
        "            flops_per_batch=measured_flops,\n",
        "            lengths=total_lengths,\n",
        "        )\n",
        "        if state[\"iter_num\"] % log_interval == 0:\n",
        "            fabric.print(\n",
        "                f\"iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}, LR: {lr:.6f}, iter time:\"\n",
        "                f\" {(t1 - iter_t0) * 1000:.2f}ms{' (optimizer.step)' if not is_accumulating else ''}\"\n",
        "            )\n",
        "\n",
        "        if val_dataloader is not None and not is_accumulating and state[\"step_count\"] % eval_interval == 0:\n",
        "            t0 = time.perf_counter()\n",
        "            val_loss = validate(fabric, model, val_dataloader)\n",
        "            t1 = time.perf_counter() - t0\n",
        "            speed_monitor.eval_end(t1)\n",
        "            fabric.print(f\"step {state['iter_num']}: val loss {val_loss.item():.4f}, val time: {t1 * 1000:.2f}ms\")\n",
        "            fabric.barrier()\n",
        "        if not is_accumulating and state[\"step_count\"] % save_interval == 0:\n",
        "            checkpoint_path = f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
        "            fabric.print(f\"{state['iter_num']} - Saving checkpoint to {str(checkpoint_path)!r}\")\n",
        "            fabric.save(checkpoint_path, state)\n",
        "            fabric.print(\n",
        "                f\"iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}, LR: {lr:.6f}, iter time:\"\n",
        "            )\n",
        "\n",
        "@torch.inference_mode()\n",
        "def validate(fabric: L.Fabric, model: torch.nn.Module, val_dataloader: DataLoader) -> torch.Tensor:\n",
        "    fabric.print(\"Validating ...\")\n",
        "    model.eval()\n",
        "\n",
        "    losses = torch.zeros(eval_iters, device=fabric.device)\n",
        "    for k, val_data in enumerate(val_dataloader):\n",
        "        input_ids = val_data[:, 0 : model.max_seq_length].contiguous()\n",
        "        targets = val_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
        "        logits = model(input_ids)\n",
        "        losses[k] = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "    out = losses.mean()\n",
        "\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "_B_m2Acj1fBT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(\n",
        "    batch_size: int, block_size: int, data_dir: Path, fabric: L.Fabric, shuffle: bool = True, seed: int = 12345\n",
        ") -> DataLoader:\n",
        "    datasets = []\n",
        "    for prefix, _ in data_config:\n",
        "        filenames = glob.glob(str(data_dir / f\"{prefix}*\"))\n",
        "        dataset = PackedDataset(\n",
        "            filenames,\n",
        "            n_chunks=1,\n",
        "            block_size=block_size,\n",
        "            shuffle=shuffle,\n",
        "            seed=seed,\n",
        "            num_processes=fabric.world_size,\n",
        "            process_rank=fabric.global_rank,\n",
        "        )\n",
        "        datasets.append(dataset)\n",
        "\n",
        "    if not datasets:\n",
        "        raise RuntimeError(\n",
        "            f\"No data found at {data_dir}. Make sure you ran prepare_redpajama.py to create the dataset.\"\n",
        "        )\n",
        "\n",
        "    weights = [weight for _, weight in data_config]\n",
        "    sum_weights = sum(weights)\n",
        "    weights = [el / sum_weights for el in weights]\n",
        "\n",
        "    combined_dataset = CombinedDataset(datasets=datasets, seed=seed, weights=weights)\n",
        "\n",
        "    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "def create_dataloaders(\n",
        "    batch_size: int,\n",
        "    block_size: int,\n",
        "    fabric: L.Fabric,\n",
        "    train_data_dir: Path = Path(\"data/redpajama_sample\"),\n",
        "    val_data_dir: Optional[Path] = None,\n",
        "    seed: int = 12345,\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "    # Increase by one because we need the next word as well\n",
        "    effective_block_size = block_size + 1\n",
        "    train_dataloader = create_dataloader(\n",
        "        batch_size=batch_size,\n",
        "        block_size=effective_block_size,\n",
        "        fabric=fabric,\n",
        "        data_dir=train_data_dir,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "    )\n",
        "    val_dataloader = (\n",
        "        create_dataloader(\n",
        "            batch_size=batch_size,\n",
        "            block_size=effective_block_size,\n",
        "            fabric=fabric,\n",
        "            data_dir=val_data_dir,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "        )\n",
        "        if val_data_dir\n",
        "        else None\n",
        "    )\n",
        "    return train_dataloader, val_dataloader\n"
      ],
      "metadata": {
        "id": "V55eB51J2HuF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it: int) -> float:\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "print(f'About to run...')\n",
        "print(os.getcwd())\n",
        "# setup(\n",
        "#     devices=1,\n",
        "#     train_data_dir=Path(\"C://Users//rajes//Downloads//data//data//lit-redpajama-sample\")\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sf47TVA2HwZ",
        "outputId": "fd6d9309-e9be-46ea-ef81-411c42cf8c8f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "About to run...\n",
            "/content/drive/MyDrive/llm_from_scratch/TSAI_ERAV1/LLM_from_scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.set_float32_matmul_precision(\"medium\")\n",
        "torch.cuda.amp.autocast(enabled=True)\n",
        "\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(\"/content/drive/MyDrive/llm_from_scratch/redpajama_100mb/redpajama_100MB\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncezZdjB2H0C",
        "outputId": "dccaf404-7334-4548-cca7-31916ed5d0c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "strategy is auto\n",
            "{'model_name': 'phi-1_5', 'name': 'redpajama', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 2, 'micro_batch_size': 1, 'gradient_accumulation_steps': 4, 'max_iters': 10000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 10000, 'min_lr': 6e-06}\n",
            "Loading model with {'name': 'phi-1_5', 'hf_config': {'org': 'microsoft', 'name': 'phi-1_5'}, 'block_size': 2048, 'vocab_size': 50257, 'padding_multiple': 512, 'padded_vocab_size': 51200, 'n_layer': 24, 'n_head': 32, 'n_embd': 2048, 'rotary_percentage': 0.5, 'parallel_residual': True, 'bias': True, 'lm_head_bias': True, 'n_query_groups': 32, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'tanh', 'intermediate_size': 8192, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'head_size': 64, 'rope_n_elem': 32}\n",
            "Time to instantiate model: 0.45 seconds.\n",
            "Total parameters 1,418,270,720\n",
            "Estimated TFLOPs: 21.55\n",
            "Measured TFLOPs: 18.61\n",
            "iter 0 step 0: loss 11.1941, LR: 0.000000, iter time: 1159.52ms\n",
            "iter 100 step 25: loss 7.1225, LR: 0.000300, iter time: 130.33ms\n",
            "iter 200 step 50: loss 7.7584, LR: 0.000600, iter time: 130.27ms\n",
            "iter 300 step 75: loss 6.4891, LR: 0.000900, iter time: 129.72ms\n",
            "iter 400 step 100: loss 6.0033, LR: 0.001200, iter time: 130.79ms\n",
            "iter 500 step 125: loss 7.8733, LR: 0.001500, iter time: 129.91ms\n",
            "iter 600 step 150: loss 7.8919, LR: 0.001800, iter time: 128.73ms\n",
            "iter 700 step 175: loss 6.8110, LR: 0.002100, iter time: 119.68ms\n",
            "iter 800 step 200: loss 6.0758, LR: 0.002400, iter time: 130.33ms\n",
            "iter 900 step 225: loss 6.5886, LR: 0.002700, iter time: 130.77ms\n",
            "iter 1000 step 250: loss 6.6106, LR: 0.003000, iter time: 130.13ms\n",
            "iter 1100 step 275: loss 6.5280, LR: 0.003300, iter time: 128.94ms\n",
            "iter 1200 step 300: loss 6.5763, LR: 0.003600, iter time: 130.86ms\n",
            "iter 1300 step 325: loss 6.3225, LR: 0.003900, iter time: 128.27ms\n",
            "iter 1400 step 350: loss 5.9872, LR: 0.004200, iter time: 130.25ms\n",
            "iter 1500 step 375: loss 6.4825, LR: 0.004500, iter time: 130.61ms\n",
            "iter 1600 step 400: loss 6.3632, LR: 0.004800, iter time: 127.88ms\n",
            "iter 1700 step 425: loss 5.5815, LR: 0.005100, iter time: 129.66ms\n",
            "iter 1800 step 450: loss 6.3892, LR: 0.005400, iter time: 128.13ms\n",
            "iter 1900 step 475: loss 6.2915, LR: 0.005700, iter time: 130.02ms\n",
            "iter 2000 step 500: loss 7.9368, LR: 0.006000, iter time: 130.19ms\n",
            "iter 2100 step 525: loss 6.1652, LR: 0.005998, iter time: 129.97ms\n",
            "iter 2200 step 550: loss 5.9021, LR: 0.005991, iter time: 130.47ms\n",
            "iter 2300 step 575: loss 6.4159, LR: 0.005979, iter time: 130.39ms\n",
            "iter 2400 step 600: loss 6.0582, LR: 0.005963, iter time: 128.79ms\n",
            "iter 2500 step 625: loss 6.1934, LR: 0.005942, iter time: 130.04ms\n",
            "iter 2600 step 650: loss 6.3137, LR: 0.005917, iter time: 129.70ms\n",
            "iter 2700 step 675: loss 6.3184, LR: 0.005887, iter time: 127.80ms\n",
            "iter 2800 step 700: loss 6.3320, LR: 0.005853, iter time: 130.32ms\n",
            "iter 2900 step 725: loss 5.9625, LR: 0.005815, iter time: 130.46ms\n",
            "iter 3000 step 750: loss 6.4806, LR: 0.005772, iter time: 130.99ms\n",
            "iter 3100 step 775: loss 6.5186, LR: 0.005725, iter time: 130.73ms\n",
            "iter 3200 step 800: loss 5.7111, LR: 0.005673, iter time: 129.46ms\n",
            "iter 3300 step 825: loss 6.2438, LR: 0.005618, iter time: 129.29ms\n",
            "iter 3400 step 850: loss 6.7928, LR: 0.005558, iter time: 126.13ms\n",
            "iter 3500 step 875: loss 6.7136, LR: 0.005495, iter time: 130.23ms\n",
            "iter 3600 step 900: loss 6.8546, LR: 0.005428, iter time: 129.74ms\n",
            "iter 3700 step 925: loss 6.1706, LR: 0.005357, iter time: 129.86ms\n",
            "iter 3800 step 950: loss 7.9361, LR: 0.005282, iter time: 130.43ms\n",
            "iter 3900 step 975: loss 6.3881, LR: 0.005204, iter time: 129.50ms\n",
            "3999 - Saving checkpoint to 'iter-003999-ckpt.pth'\n",
            "iter 3999 step 1000: loss 6.1358, LR: 0.005123, iter time:\n",
            "iter 4000 step 1000: loss 6.8866, LR: 0.005122, iter time: 123.48ms\n",
            "iter 4100 step 1025: loss 6.9882, LR: 0.005037, iter time: 129.46ms\n",
            "iter 4200 step 1050: loss 7.6402, LR: 0.004949, iter time: 130.84ms\n",
            "iter 4300 step 1075: loss 6.5119, LR: 0.004858, iter time: 130.52ms\n",
            "iter 4400 step 1100: loss 6.2994, LR: 0.004765, iter time: 130.85ms\n",
            "iter 4500 step 1125: loss 5.9925, LR: 0.004668, iter time: 128.26ms\n",
            "iter 4600 step 1150: loss 6.5583, LR: 0.004569, iter time: 131.05ms\n",
            "iter 4700 step 1175: loss 6.2698, LR: 0.004467, iter time: 131.30ms\n",
            "iter 4800 step 1200: loss 6.3866, LR: 0.004364, iter time: 130.86ms\n",
            "iter 4900 step 1225: loss 6.4844, LR: 0.004258, iter time: 130.19ms\n",
            "iter 5000 step 1250: loss 6.8060, LR: 0.004150, iter time: 129.75ms\n",
            "iter 5100 step 1275: loss 6.5024, LR: 0.004040, iter time: 130.35ms\n",
            "iter 5200 step 1300: loss 5.8475, LR: 0.003929, iter time: 128.84ms\n",
            "iter 5300 step 1325: loss 6.5620, LR: 0.003817, iter time: 130.69ms\n",
            "iter 5400 step 1350: loss 5.9729, LR: 0.003703, iter time: 130.57ms\n",
            "iter 5500 step 1375: loss 5.7559, LR: 0.003588, iter time: 129.04ms\n",
            "iter 5600 step 1400: loss 6.4525, LR: 0.003472, iter time: 129.94ms\n",
            "iter 5700 step 1425: loss 6.8721, LR: 0.003355, iter time: 129.46ms\n",
            "iter 5800 step 1450: loss 5.9980, LR: 0.003238, iter time: 129.48ms\n",
            "iter 5900 step 1475: loss 5.8110, LR: 0.003121, iter time: 130.43ms\n",
            "iter 6000 step 1500: loss 5.9355, LR: 0.003003, iter time: 130.02ms\n",
            "iter 6100 step 1525: loss 6.3023, LR: 0.002885, iter time: 130.69ms\n",
            "iter 6200 step 1550: loss 6.1744, LR: 0.002768, iter time: 130.09ms\n",
            "iter 6300 step 1575: loss 6.2123, LR: 0.002651, iter time: 131.17ms\n",
            "iter 6400 step 1600: loss 6.1417, LR: 0.002534, iter time: 129.44ms\n",
            "iter 6500 step 1625: loss 7.0939, LR: 0.002418, iter time: 129.56ms\n",
            "iter 6600 step 1650: loss 3.9350, LR: 0.002303, iter time: 130.56ms\n",
            "iter 6700 step 1675: loss 5.6432, LR: 0.002189, iter time: 130.41ms\n",
            "iter 6800 step 1700: loss 5.9345, LR: 0.002077, iter time: 131.15ms\n",
            "iter 6900 step 1725: loss 6.5545, LR: 0.001966, iter time: 128.30ms\n",
            "iter 7000 step 1750: loss 7.0648, LR: 0.001856, iter time: 129.95ms\n",
            "iter 7100 step 1775: loss 6.6696, LR: 0.001748, iter time: 130.06ms\n",
            "iter 7200 step 1800: loss 5.9525, LR: 0.001642, iter time: 130.99ms\n",
            "iter 7300 step 1825: loss 5.4868, LR: 0.001539, iter time: 129.87ms\n",
            "iter 7400 step 1850: loss 6.2612, LR: 0.001437, iter time: 130.74ms\n",
            "iter 7500 step 1875: loss 6.4085, LR: 0.001338, iter time: 128.75ms\n",
            "iter 7600 step 1900: loss 6.0138, LR: 0.001241, iter time: 130.94ms\n",
            "iter 7700 step 1925: loss 5.7046, LR: 0.001148, iter time: 129.16ms\n",
            "iter 7800 step 1950: loss 5.7653, LR: 0.001057, iter time: 130.90ms\n",
            "iter 7900 step 1975: loss 3.8608, LR: 0.000969, iter time: 130.78ms\n",
            "7999 - Saving checkpoint to 'iter-007999-ckpt.pth'\n",
            "iter 7999 step 2000: loss 6.3998, LR: 0.000885, iter time:\n",
            "iter 8000 step 2000: loss 6.3129, LR: 0.000884, iter time: 123.79ms\n",
            "iter 8100 step 2025: loss 5.9907, LR: 0.000802, iter time: 128.56ms\n",
            "iter 8200 step 2050: loss 6.2861, LR: 0.000724, iter time: 129.30ms\n",
            "iter 8300 step 2075: loss 6.2327, LR: 0.000649, iter time: 131.44ms\n",
            "iter 8400 step 2100: loss 4.3776, LR: 0.000578, iter time: 129.98ms\n",
            "iter 8500 step 2125: loss 4.1531, LR: 0.000511, iter time: 130.73ms\n",
            "iter 8600 step 2150: loss 5.3956, LR: 0.000448, iter time: 130.26ms\n",
            "iter 8700 step 2175: loss 6.5889, LR: 0.000388, iter time: 122.51ms\n",
            "iter 8800 step 2200: loss 6.1074, LR: 0.000333, iter time: 129.44ms\n",
            "iter 8900 step 2225: loss 6.3341, LR: 0.000281, iter time: 131.42ms\n",
            "iter 9000 step 2250: loss 5.7643, LR: 0.000234, iter time: 129.43ms\n",
            "iter 9100 step 2275: loss 5.8672, LR: 0.000191, iter time: 125.16ms\n",
            "iter 9200 step 2300: loss 6.2402, LR: 0.000153, iter time: 129.65ms\n",
            "iter 9300 step 2325: loss 4.0863, LR: 0.000119, iter time: 127.48ms\n",
            "iter 9400 step 2350: loss 6.0944, LR: 0.000089, iter time: 130.83ms\n",
            "iter 9500 step 2375: loss 5.3757, LR: 0.000064, iter time: 129.60ms\n",
            "iter 9600 step 2400: loss 6.2444, LR: 0.000043, iter time: 128.03ms\n",
            "iter 9700 step 2425: loss 6.3824, LR: 0.000027, iter time: 129.62ms\n",
            "iter 9800 step 2450: loss 5.1854, LR: 0.000015, iter time: 125.20ms\n",
            "iter 9900 step 2475: loss 5.3764, LR: 0.000008, iter time: 130.54ms\n",
            "Breaking at max...iter 10000 step 2500: loss 5.6327\n",
            "Saving checkpoint to 'iter-010000-ckpt.pth'\n",
            "Training time: 2069.68s\n",
            "Memory used: 30.87 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qznXE10r8p5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8YOcTHq0mtD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}