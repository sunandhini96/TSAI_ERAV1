{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60e06fa0e11d4134afb68ca55d6fd2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28f61acb3ed1417aa2116601fd6cd2f8",
              "IPY_MODEL_6ac03709d82443e899cd1109bd503622",
              "IPY_MODEL_b4ae299330334f9ab0350d8f2bdeb74e"
            ],
            "layout": "IPY_MODEL_4a63943801324a4393e47ddb4e71610c"
          }
        },
        "28f61acb3ed1417aa2116601fd6cd2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cafc5bc93e541f0aad51062cd7db359",
            "placeholder": "​",
            "style": "IPY_MODEL_81cf928589b345628fc78fd74769dbe4",
            "value": "Epoch 15:  82%"
          }
        },
        "6ac03709d82443e899cd1109bd503622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d06f8441af8e4b88995e0aea17b194b3",
            "max": 98,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cac432f3bd14eb7811fa863ab55bcc6",
            "value": 80
          }
        },
        "b4ae299330334f9ab0350d8f2bdeb74e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b24500526a5b453b9117e9f12fa659be",
            "placeholder": "​",
            "style": "IPY_MODEL_deb31457fd01480e840f91cfaa0f0bc9",
            "value": " 80/98 [01:12&lt;00:16,  1.10it/s, loss=2.8e+03, v_num=0]"
          }
        },
        "4a63943801324a4393e47ddb4e71610c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "5cafc5bc93e541f0aad51062cd7db359": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81cf928589b345628fc78fd74769dbe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d06f8441af8e4b88995e0aea17b194b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cac432f3bd14eb7811fa863ab55bcc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b24500526a5b453b9117e9f12fa659be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deb31457fd01480e840f91cfaa0f0bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "TGE4J-Uj6-18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning-bolts"
      ],
      "metadata": {
        "id": "njoiWTCs7RlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from pl_bolts.models.autoencoders.components import (\n",
        "    resnet18_decoder,\n",
        "    resnet18_encoder,\n",
        ")\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.callbacks import ModelSummary\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets\n"
      ],
      "metadata": {
        "id": "f02mT-i22fEn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Cifar10Dataset(torchvision.datasets.CIFAR10):\n",
        "    def __init__(self, root=\"./data\", train=True, download=True, transform=None):\n",
        "      super().__init__(root=root, train=train, download=download, transform=transform)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image, label = self.data[index], self.targets[index]\n",
        "\n",
        "      if self.transform is not None:\n",
        "        transformed = self.transform(image=image)\n",
        "        image = transformed[\"image\"]\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "9bQLna0q2fB4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "mu_cifar = (0.49139968, 0.48215827, 0.44653124)\n",
        "sigma_cifar = (0.24703233, 0.24348505, 0.26158768)\n",
        "\n",
        "def augmentation(data, mu=mu_cifar, sigma=sigma_cifar, pad=4):\n",
        "    if data == 'Train':\n",
        "        transform = A.Compose([A.HorizontalFlip(p=0.5),\n",
        "                            A.Normalize(mean=mu, std=sigma),\n",
        "                            ToTensorV2()])\n",
        "    elif data == 'Test':\n",
        "        transform = A.Compose([A.Normalize(mean=mu_cifar, std=sigma_cifar),\n",
        "                           ToTensorV2()])\n",
        "    else:\n",
        "        transform = A.Compose([ToTensorV2()])\n",
        "\n",
        "    return transform"
      ],
      "metadata": {
        "id": "aKNYKSdx7-74"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_to_categorical(y, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "    ohe=np.eye(num_classes, dtype='uint8')[y]\n",
        "    return ohe\n",
        "\n",
        "def categorical_to_num(y):\n",
        "  \"\"\" Decodes a 1-hot encoding \"\"\"\n",
        "  return np.argmax(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OddIWw58EvK",
        "outputId": "85238c15-554d-4370-843c-a4b488fb2dd1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset is there to be able to interact with DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class cifar10_data(Dataset):\n",
        "  '''\n",
        "  cifar10_data class to create an iteratable on our custom cifar10 dataset:\n",
        "  where data = (image and image label as one-hot-encoding) and labels = (image)\n",
        "  '''\n",
        "\n",
        "  def __init__(self, d):\n",
        "    self.cifar = d                                                              # Initialize with cifar data object\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    image, label = self.cifar[index]\n",
        "    number_encoding = num_to_categorical(label, 10)                             # 1-hot encoding of number\n",
        "    number_encoding = torch.tensor(number_encoding)\n",
        "\n",
        "    return image, number_encoding                                               # (input), (output) to network\n",
        "\n",
        "  def __len__(self):                                                            # Return the length of the dataset\n",
        "    return len(self.cifar)"
      ],
      "metadata": {
        "id": "ciFs8Pit8J0o"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "classes_dict = {c:idx for idx, c in enumerate(classes)}"
      ],
      "metadata": {
        "id": "TeIfdZQS8MpM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "\n",
        "class VAE(pl.LightningModule):\n",
        "    def __init__(self, data_dir, n_labels=10, enc_out_dim=512, latent_dim=256, input_height=32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.transform_train = augmentation('Train')\n",
        "        self.transform_valid = augmentation('Test')\n",
        "        self.n_labels = n_labels\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # encoder, decoder\n",
        "        self.encoder = resnet18_encoder(False, False)\n",
        "        self.decoder = resnet18_decoder(\n",
        "            latent_dim=latent_dim,\n",
        "            input_height=input_height,\n",
        "            first_conv=False,\n",
        "            maxpool1=False\n",
        "        )\n",
        "\n",
        "        # distribution parameters\n",
        "        self.fc_mu = nn.Linear(enc_out_dim+self.n_labels, latent_dim)\n",
        "        self.fc_var = nn.Linear(enc_out_dim+self.n_labels, latent_dim)\n",
        "\n",
        "        # for the gaussian likelihood\n",
        "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "\n",
        "    def gaussian_likelihood(self, mean, logscale, sample):\n",
        "        scale = torch.exp(logscale)\n",
        "        dist = torch.distributions.Normal(mean, scale)\n",
        "        log_pxz = dist.log_prob(sample)\n",
        "        return log_pxz.sum(dim=(1, 2, 3))\n",
        "\n",
        "    def kl_divergence(self, z, mu, std):\n",
        "        # --------------------------\n",
        "        # Monte carlo KL divergence\n",
        "        # --------------------------\n",
        "        # 1. define the first two probabilities (in this case Normal for both)\n",
        "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
        "        q = torch.distributions.Normal(mu, std)\n",
        "\n",
        "        # 2. get the probabilities from the equation\n",
        "        log_qzx = q.log_prob(z)\n",
        "        log_pz = p.log_prob(z)\n",
        "\n",
        "        # kl\n",
        "        kl = (log_qzx - log_pz)\n",
        "        kl = kl.sum(-1)\n",
        "        return kl\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, label = batch\n",
        "\n",
        "        # encode x to get the mu and variance parameters\n",
        "        x_encoded = self.encoder(x)\n",
        "\n",
        "        mu, log_var = self.fc_mu(torch.cat((x_encoded, label), dim=1)), self.fc_var(torch.cat((x_encoded, label), dim=1))\n",
        "\n",
        "        # sample z from q\n",
        "        std = torch.exp(log_var / 2)\n",
        "        q = torch.distributions.Normal(mu, std)\n",
        "        z = q.rsample()\n",
        "\n",
        "        # decoded\n",
        "        x_hat = self.decoder(z)\n",
        "\n",
        "        # reconstruction loss\n",
        "        recon_loss = self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
        "\n",
        "        # kl\n",
        "        kl = self.kl_divergence(z, mu, std)\n",
        "\n",
        "        # elbo\n",
        "        elbo = (kl - recon_loss)\n",
        "        elbo = elbo.mean()\n",
        "\n",
        "        self.log_dict({\n",
        "            'elbo': elbo,\n",
        "            'kl': kl.mean(),\n",
        "            'recon_loss': recon_loss.mean(),\n",
        "            'reconstruction': recon_loss.mean(),\n",
        "            'kl': kl.mean(),\n",
        "        })\n",
        "\n",
        "        return elbo\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        Cifar10Dataset(self.data_dir, train=True, download=True, transform=None)\n",
        "        Cifar10Dataset(self.data_dir, train=False, download=True, transform=None)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.cifar_train = cifar10_data(Cifar10Dataset(self.data_dir, train=True, download=False, transform=self.transform_train))\n",
        "            self.cifar_valid = cifar10_data(Cifar10Dataset(self.data_dir, train=False, download=False, transform=self.transform_valid))\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.cifar_test = cifar10_data(Cifar10Dataset(self.data_dir, train=False, download=False, transform=self.transform_valid))\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.cifar_train, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.cifar_valid, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.cifar_test, batch_size=BATCH_SIZE, num_workers=os.cpu_count())"
      ],
      "metadata": {
        "id": "FhK3EL0y8bRX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ILDo2XXa8iO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "num_classes=10\n",
        "learning_rate=1e-4\n",
        "\n",
        "model = VAE('.')\n",
        "trainer = Trainer(\n",
        "    callbacks=[ModelSummary(max_depth=-1)],\n",
        "    gpus=[0],\n",
        "    max_epochs = epochs,\n",
        "    )\n",
        "trainer.fit(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "60e06fa0e11d4134afb68ca55d6fd2bf",
            "28f61acb3ed1417aa2116601fd6cd2f8",
            "6ac03709d82443e899cd1109bd503622",
            "b4ae299330334f9ab0350d8f2bdeb74e",
            "4a63943801324a4393e47ddb4e71610c",
            "5cafc5bc93e541f0aad51062cd7db359",
            "81cf928589b345628fc78fd74769dbe4",
            "d06f8441af8e4b88995e0aea17b194b3",
            "2cac432f3bd14eb7811fa863ab55bcc6",
            "b24500526a5b453b9117e9f12fa659be",
            "deb31457fd01480e840f91cfaa0f0bc9"
          ]
        },
        "id": "6h2AG5CO8ka8",
        "outputId": "ce8dfaf6-cf8f-4257-e571-aa22675894ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-34ebd0d3ac5e>:14: UnderReviewWarning: The feature resnet18_encoder is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.encoder = resnet18_encoder(False, False)\n",
            "<ipython-input-11-34ebd0d3ac5e>:15: UnderReviewWarning: The feature resnet18_decoder is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.decoder = resnet18_decoder(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
            "  rank_zero_deprecation(\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:106: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
            "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 76400988.66it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "    | Name                          | Type              | Params\n",
            "----------------------------------------------------------------------\n",
            "0   | encoder                       | ResNetEncoder     | 11.2 M\n",
            "1   | encoder.conv1                 | Conv2d            | 1.7 K \n",
            "2   | encoder.bn1                   | BatchNorm2d       | 128   \n",
            "3   | encoder.relu                  | ReLU              | 0     \n",
            "4   | encoder.maxpool               | MaxPool2d         | 0     \n",
            "5   | encoder.layer1                | Sequential        | 147 K \n",
            "6   | encoder.layer1.0              | EncoderBlock      | 74.0 K\n",
            "7   | encoder.layer1.0.conv1        | Conv2d            | 36.9 K\n",
            "8   | encoder.layer1.0.bn1          | BatchNorm2d       | 128   \n",
            "9   | encoder.layer1.0.relu         | ReLU              | 0     \n",
            "10  | encoder.layer1.0.conv2        | Conv2d            | 36.9 K\n",
            "11  | encoder.layer1.0.bn2          | BatchNorm2d       | 128   \n",
            "12  | encoder.layer1.1              | EncoderBlock      | 74.0 K\n",
            "13  | encoder.layer1.1.conv1        | Conv2d            | 36.9 K\n",
            "14  | encoder.layer1.1.bn1          | BatchNorm2d       | 128   \n",
            "15  | encoder.layer1.1.relu         | ReLU              | 0     \n",
            "16  | encoder.layer1.1.conv2        | Conv2d            | 36.9 K\n",
            "17  | encoder.layer1.1.bn2          | BatchNorm2d       | 128   \n",
            "18  | encoder.layer2                | Sequential        | 525 K \n",
            "19  | encoder.layer2.0              | EncoderBlock      | 230 K \n",
            "20  | encoder.layer2.0.conv1        | Conv2d            | 73.7 K\n",
            "21  | encoder.layer2.0.bn1          | BatchNorm2d       | 256   \n",
            "22  | encoder.layer2.0.relu         | ReLU              | 0     \n",
            "23  | encoder.layer2.0.conv2        | Conv2d            | 147 K \n",
            "24  | encoder.layer2.0.bn2          | BatchNorm2d       | 256   \n",
            "25  | encoder.layer2.0.downsample   | Sequential        | 8.4 K \n",
            "26  | encoder.layer2.0.downsample.0 | Conv2d            | 8.2 K \n",
            "27  | encoder.layer2.0.downsample.1 | BatchNorm2d       | 256   \n",
            "28  | encoder.layer2.1              | EncoderBlock      | 295 K \n",
            "29  | encoder.layer2.1.conv1        | Conv2d            | 147 K \n",
            "30  | encoder.layer2.1.bn1          | BatchNorm2d       | 256   \n",
            "31  | encoder.layer2.1.relu         | ReLU              | 0     \n",
            "32  | encoder.layer2.1.conv2        | Conv2d            | 147 K \n",
            "33  | encoder.layer2.1.bn2          | BatchNorm2d       | 256   \n",
            "34  | encoder.layer3                | Sequential        | 2.1 M \n",
            "35  | encoder.layer3.0              | EncoderBlock      | 919 K \n",
            "36  | encoder.layer3.0.conv1        | Conv2d            | 294 K \n",
            "37  | encoder.layer3.0.bn1          | BatchNorm2d       | 512   \n",
            "38  | encoder.layer3.0.relu         | ReLU              | 0     \n",
            "39  | encoder.layer3.0.conv2        | Conv2d            | 589 K \n",
            "40  | encoder.layer3.0.bn2          | BatchNorm2d       | 512   \n",
            "41  | encoder.layer3.0.downsample   | Sequential        | 33.3 K\n",
            "42  | encoder.layer3.0.downsample.0 | Conv2d            | 32.8 K\n",
            "43  | encoder.layer3.0.downsample.1 | BatchNorm2d       | 512   \n",
            "44  | encoder.layer3.1              | EncoderBlock      | 1.2 M \n",
            "45  | encoder.layer3.1.conv1        | Conv2d            | 589 K \n",
            "46  | encoder.layer3.1.bn1          | BatchNorm2d       | 512   \n",
            "47  | encoder.layer3.1.relu         | ReLU              | 0     \n",
            "48  | encoder.layer3.1.conv2        | Conv2d            | 589 K \n",
            "49  | encoder.layer3.1.bn2          | BatchNorm2d       | 512   \n",
            "50  | encoder.layer4                | Sequential        | 8.4 M \n",
            "51  | encoder.layer4.0              | EncoderBlock      | 3.7 M \n",
            "52  | encoder.layer4.0.conv1        | Conv2d            | 1.2 M \n",
            "53  | encoder.layer4.0.bn1          | BatchNorm2d       | 1.0 K \n",
            "54  | encoder.layer4.0.relu         | ReLU              | 0     \n",
            "55  | encoder.layer4.0.conv2        | Conv2d            | 2.4 M \n",
            "56  | encoder.layer4.0.bn2          | BatchNorm2d       | 1.0 K \n",
            "57  | encoder.layer4.0.downsample   | Sequential        | 132 K \n",
            "58  | encoder.layer4.0.downsample.0 | Conv2d            | 131 K \n",
            "59  | encoder.layer4.0.downsample.1 | BatchNorm2d       | 1.0 K \n",
            "60  | encoder.layer4.1              | EncoderBlock      | 4.7 M \n",
            "61  | encoder.layer4.1.conv1        | Conv2d            | 2.4 M \n",
            "62  | encoder.layer4.1.bn1          | BatchNorm2d       | 1.0 K \n",
            "63  | encoder.layer4.1.relu         | ReLU              | 0     \n",
            "64  | encoder.layer4.1.conv2        | Conv2d            | 2.4 M \n",
            "65  | encoder.layer4.1.bn2          | BatchNorm2d       | 1.0 K \n",
            "66  | encoder.avgpool               | AdaptiveAvgPool2d | 0     \n",
            "67  | decoder                       | ResNetDecoder     | 8.6 M \n",
            "68  | decoder.linear                | Linear            | 2.1 M \n",
            "69  | decoder.layer1                | Sequential        | 4.9 M \n",
            "70  | decoder.layer1.0              | DecoderBlock      | 3.7 M \n",
            "71  | decoder.layer1.0.conv1        | Conv2d            | 2.4 M \n",
            "72  | decoder.layer1.0.bn1          | BatchNorm2d       | 1.0 K \n",
            "73  | decoder.layer1.0.relu         | ReLU              | 0     \n",
            "74  | decoder.layer1.0.conv2        | Sequential        | 1.2 M \n",
            "75  | decoder.layer1.0.conv2.0      | Interpolate       | 0     \n",
            "76  | decoder.layer1.0.conv2.1      | Conv2d            | 1.2 M \n",
            "77  | decoder.layer1.0.bn2          | BatchNorm2d       | 512   \n",
            "78  | decoder.layer1.0.upsample     | Sequential        | 131 K \n",
            "79  | decoder.layer1.0.upsample.0   | Sequential        | 131 K \n",
            "80  | decoder.layer1.0.upsample.0.0 | Interpolate       | 0     \n",
            "81  | decoder.layer1.0.upsample.0.1 | Conv2d            | 131 K \n",
            "82  | decoder.layer1.0.upsample.1   | BatchNorm2d       | 512   \n",
            "83  | decoder.layer1.1              | DecoderBlock      | 1.2 M \n",
            "84  | decoder.layer1.1.conv1        | Conv2d            | 589 K \n",
            "85  | decoder.layer1.1.bn1          | BatchNorm2d       | 512   \n",
            "86  | decoder.layer1.1.relu         | ReLU              | 0     \n",
            "87  | decoder.layer1.1.conv2        | Conv2d            | 589 K \n",
            "88  | decoder.layer1.1.bn2          | BatchNorm2d       | 512   \n",
            "89  | decoder.layer2                | Sequential        | 1.2 M \n",
            "90  | decoder.layer2.0              | DecoderBlock      | 918 K \n",
            "91  | decoder.layer2.0.conv1        | Conv2d            | 589 K \n",
            "92  | decoder.layer2.0.bn1          | BatchNorm2d       | 512   \n",
            "93  | decoder.layer2.0.relu         | ReLU              | 0     \n",
            "94  | decoder.layer2.0.conv2        | Sequential        | 294 K \n",
            "95  | decoder.layer2.0.conv2.0      | Interpolate       | 0     \n",
            "96  | decoder.layer2.0.conv2.1      | Conv2d            | 294 K \n",
            "97  | decoder.layer2.0.bn2          | BatchNorm2d       | 256   \n",
            "98  | decoder.layer2.0.upsample     | Sequential        | 33.0 K\n",
            "99  | decoder.layer2.0.upsample.0   | Sequential        | 32.8 K\n",
            "100 | decoder.layer2.0.upsample.0.0 | Interpolate       | 0     \n",
            "101 | decoder.layer2.0.upsample.0.1 | Conv2d            | 32.8 K\n",
            "102 | decoder.layer2.0.upsample.1   | BatchNorm2d       | 256   \n",
            "103 | decoder.layer2.1              | DecoderBlock      | 295 K \n",
            "104 | decoder.layer2.1.conv1        | Conv2d            | 147 K \n",
            "105 | decoder.layer2.1.bn1          | BatchNorm2d       | 256   \n",
            "106 | decoder.layer2.1.relu         | ReLU              | 0     \n",
            "107 | decoder.layer2.1.conv2        | Conv2d            | 147 K \n",
            "108 | decoder.layer2.1.bn2          | BatchNorm2d       | 256   \n",
            "109 | decoder.layer3                | Sequential        | 303 K \n",
            "110 | decoder.layer3.0              | DecoderBlock      | 229 K \n",
            "111 | decoder.layer3.0.conv1        | Conv2d            | 147 K \n",
            "112 | decoder.layer3.0.bn1          | BatchNorm2d       | 256   \n",
            "113 | decoder.layer3.0.relu         | ReLU              | 0     \n",
            "114 | decoder.layer3.0.conv2        | Sequential        | 73.7 K\n",
            "115 | decoder.layer3.0.conv2.0      | Interpolate       | 0     \n",
            "116 | decoder.layer3.0.conv2.1      | Conv2d            | 73.7 K\n",
            "117 | decoder.layer3.0.bn2          | BatchNorm2d       | 128   \n",
            "118 | decoder.layer3.0.upsample     | Sequential        | 8.3 K \n",
            "119 | decoder.layer3.0.upsample.0   | Sequential        | 8.2 K \n",
            "120 | decoder.layer3.0.upsample.0.0 | Interpolate       | 0     \n",
            "121 | decoder.layer3.0.upsample.0.1 | Conv2d            | 8.2 K \n",
            "122 | decoder.layer3.0.upsample.1   | BatchNorm2d       | 128   \n",
            "123 | decoder.layer3.1              | DecoderBlock      | 74.0 K\n",
            "124 | decoder.layer3.1.conv1        | Conv2d            | 36.9 K\n",
            "125 | decoder.layer3.1.bn1          | BatchNorm2d       | 128   \n",
            "126 | decoder.layer3.1.relu         | ReLU              | 0     \n",
            "127 | decoder.layer3.1.conv2        | Conv2d            | 36.9 K\n",
            "128 | decoder.layer3.1.bn2          | BatchNorm2d       | 128   \n",
            "129 | decoder.layer4                | Sequential        | 147 K \n",
            "130 | decoder.layer4.0              | DecoderBlock      | 74.0 K\n",
            "131 | decoder.layer4.0.conv1        | Conv2d            | 36.9 K\n",
            "132 | decoder.layer4.0.bn1          | BatchNorm2d       | 128   \n",
            "133 | decoder.layer4.0.relu         | ReLU              | 0     \n",
            "134 | decoder.layer4.0.conv2        | Conv2d            | 36.9 K\n",
            "135 | decoder.layer4.0.bn2          | BatchNorm2d       | 128   \n",
            "136 | decoder.layer4.1              | DecoderBlock      | 74.0 K\n",
            "137 | decoder.layer4.1.conv1        | Conv2d            | 36.9 K\n",
            "138 | decoder.layer4.1.bn1          | BatchNorm2d       | 128   \n",
            "139 | decoder.layer4.1.relu         | ReLU              | 0     \n",
            "140 | decoder.layer4.1.conv2        | Conv2d            | 36.9 K\n",
            "141 | decoder.layer4.1.bn2          | BatchNorm2d       | 128   \n",
            "142 | decoder.upscale               | Interpolate       | 0     \n",
            "143 | decoder.upscale1              | Interpolate       | 0     \n",
            "144 | decoder.conv1                 | Conv2d            | 1.7 K \n",
            "145 | fc_mu                         | Linear            | 133 K \n",
            "146 | fc_var                        | Linear            | 133 K \n",
            "----------------------------------------------------------------------\n",
            "20.1 M    Trainable params\n",
            "0         Non-trainable params\n",
            "20.1 M    Total params\n",
            "80.249    Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60e06fa0e11d4134afb68ca55d6fd2bf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Define the model\n",
        "# model = YourModelClass()\n",
        "\n",
        "# Define the directory for saving the model\n",
        "save_dir = 'results'\n",
        "\n",
        "# Check if the directory exists, and if not, create it\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# Save the model's state dictionary to a file\n",
        "torch.save(model.state_dict(), os.path.join(save_dir, 'CIFAR10.pth'))\n"
      ],
      "metadata": {
        "id": "W-rTppv88ooM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:1\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "IhH1Jpts8vsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = Cifar10Dataset(train=False, download=True, transform=augmentation('Test'))\n",
        "test_set_custom = cifar10_data(test_set)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set_custom,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    **kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "aZvCnbtq8yQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_call(model, input1, input2):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        encoded_image = model.encoder(input1)\n",
        "        linear_layer = torch.cat((encoded_image, input2), dim=1)\n",
        "        mu, log_var = model.fc_mu(linear_layer), model.fc_var(linear_layer)\n",
        "\n",
        "        std = torch.exp(log_var / 2)\n",
        "        q = torch.distributions.Normal(mu, std)\n",
        "        z = q.rsample()\n",
        "\n",
        "        decoded_image = model.decoder(z).to('cpu')\n",
        "\n",
        "        return decoded_image"
      ],
      "metadata": {
        "id": "BMl9-d6-80r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "all_predictions_wrong = []\n",
        "all_predictions_correct = []\n",
        "all_labels = []\n",
        "\n",
        "for data, label in test_loader:\n",
        "\n",
        "    r_idx = random.randrange(0, 256)\n",
        "\n",
        "    input = data[r_idx]\n",
        "    gt = label[r_idx]\n",
        "\n",
        "    correct_label = categorical_to_num(np.array(gt))\n",
        "\n",
        "    while count < 25:\n",
        "        r_label = random.randrange(0, 10)\n",
        "        if r_label != correct_label:\n",
        "            wrong_label = r_label\n",
        "\n",
        "            all_labels.append([correct_label, wrong_label])\n",
        "\n",
        "            wrong_label_vector = num_to_categorical(wrong_label, 10)\n",
        "            gt_mod = gt.clone()\n",
        "            gt_mod = torch.tensor(wrong_label_vector)\n",
        "\n",
        "            ## unqueeze to get batch dim\n",
        "            input1 = input.unsqueeze(0)\n",
        "            input2 = gt.unsqueeze(0)\n",
        "            input2_mod = gt_mod.unsqueeze(0)\n",
        "\n",
        "            # print(f'gt label {correct_label}, wrong label added {wrong_label}')\n",
        "\n",
        "            prediction_correct = model_call(model.to(device), input1.to(device), input2.to(device)).detach().cpu().squeeze(0).squeeze(0)\n",
        "            prediction_wrong = model_call(model.to(device), input1.to(device), input2_mod.to(device)).detach().cpu().squeeze(0).squeeze(0)\n",
        "\n",
        "            all_predictions_correct.append(prediction_correct)\n",
        "            all_predictions_wrong.append(prediction_wrong)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "ukqLMBX785oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2classes = {classes_dict[key]:key for key in classes_dict.keys()}"
      ],
      "metadata": {
        "id": "lcFS1PL99Las"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu_cifar = (0.49139968, 0.48215827, 0.44653124)\n",
        "sigma_cifar = (0.24703233, 0.24348505, 0.26158768)"
      ],
      "metadata": {
        "id": "HVUooRDX9PiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grid(image, labels, noise=False):\n",
        "\n",
        "    nrows = 5\n",
        "    ncols = 5\n",
        "\n",
        "    fig, ax = plt.subplots(nrows, ncols, figsize=(10, 10))\n",
        "\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            index = i * ncols + j\n",
        "            ax[i, j].axis(\"off\")\n",
        "            img = torch.tensor(image[index]).permute(1, 2, 0).numpy()*sigma_cifar + mu_cifar\n",
        "\n",
        "            ax[i, j].imshow(img)\n",
        "\n",
        "            correct, wrong = labels[index]\n",
        "            if noise:\n",
        "                s = f'True: {id2classes[correct]}\\nNoise: {id2classes[wrong]}'\n",
        "                ax[i, j].text(0, 5, s, color='Black')\n",
        "            else:\n",
        "                s = f'True: {id2classes[correct]}'\n",
        "                ax[i, j].text(0, 2, s, color='Black')\n",
        "\n",
        "    plt.subplots_adjust(wspace=0, hspace=0.01)"
      ],
      "metadata": {
        "id": "pKBTU4Cz9TnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_grid(all_predictions_wrong, all_labels, True)"
      ],
      "metadata": {
        "id": "K7LGcT2f9Ycb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}